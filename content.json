{"meta":{"title":"韬光养月巴的技术博客","subtitle":"","description":"","author":"韬光养月巴","url":"http://blog.tgyf.com","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-02-22T13:20:16.531Z","updated":"2020-02-22T12:01:21.685Z","comments":false,"path":"/404.html","permalink":"http://blog.tgyf.com/404.html","excerpt":"","text":""},{"title":"书单","date":"2020-02-22T13:20:16.520Z","updated":"2020-02-22T12:01:21.688Z","comments":false,"path":"books/index.html","permalink":"http://blog.tgyf.com/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2020-03-04T03:34:15.353Z","updated":"2020-03-04T03:34:15.289Z","comments":false,"path":"about/index.html","permalink":"http://blog.tgyf.com/about/index.html","excerpt":"","text":"欢迎访问我的博客，我会保持不定期更新。 123456789101112131415161718192021222324252627282930313233343536 _______ ______ __ __ _____ /\\_______)\\/_/\\___\\/\\ /\\ /\\ /\\_____\\ \\(___ __\\/) ) ___/\\ \\ \\/ / /( ( ___/ / / / /_/ / ___\\ \\__/ / \\ \\ \\_ ( ( ( \\ \\ \\_/\\__\\\\__/ / / / /_\\ \\ \\ \\ )_) \\/ _// / / / /____/ /_/_/ \\_\\____/ \\/_/ \\/_/ &#123; \"name\": \"韬光养月巴\", \"age\": 30, \"gender\": \"男\", \"profession\": \"Java Developer &amp; Designer\", \"experience\": \"5年\", \"address\": \"四川省成都市\", \"education\": \"本科\", \"github\": \"https://github.com/tgyf\", \"blog\": \"http://blog.tgyf.com\", \"email\": \"back_up[a]foxmail.com\", \"skills\":[ [\"SpringFramework\", \"Netty\", \"Dubbo\", \"Mybatis\"], [\"Html\", \"Javascript\", \"jQuery\", \"CSS\"], [\"Mysql\",\"Orcal\",\"Redis\",\"MongoDB\"], [\"Docker\", \"Swarm\",\"K8s\"], [\"Git\", \"SVN\"], ], \"devTools\":[ [\"IntelliJ IDEA \", \"VisualStudioCode\", \"Notepad++\"], [\"SourceTree\", \"TortoiseSVN\"], [\"Navicat\", \"RedisDesktopManager\",\"Robo3t\"], [\"MobaXterm\",\"Xshell\"] ] &#125;"},{"title":"分类","date":"2020-02-22T13:20:16.515Z","updated":"2020-02-22T13:07:14.116Z","comments":false,"path":"categories/index.html","permalink":"http://blog.tgyf.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-02-22T13:20:16.505Z","updated":"2020-02-22T12:01:21.688Z","comments":true,"path":"links/index.html","permalink":"http://blog.tgyf.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-02-22T13:20:16.499Z","updated":"2020-02-22T12:01:21.689Z","comments":false,"path":"repository/index.html","permalink":"http://blog.tgyf.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-02-22T13:20:16.510Z","updated":"2020-02-22T12:01:21.689Z","comments":false,"path":"tags/index.html","permalink":"http://blog.tgyf.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MyBatis 源码分析 - 插件机制","slug":"Java框架/Mybatis/MyBatis 源码分析 - 插件机制","date":"2020-05-20T13:13:16.494Z","updated":"2020-05-20T13:13:16.494Z","comments":true,"path":"2020/05/20/Java框架/Mybatis/MyBatis 源码分析 - 插件机制/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Java%E6%A1%86%E6%9E%B6/Mybatis/MyBatis%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%20-%20%E6%8F%92%E4%BB%B6%E6%9C%BA%E5%88%B6/","excerpt":"","text":"简介 一般情况下，开源框架都会提供插件或其他形式的拓展点，供开发者自行拓展。这样的好处是显而易见的，一是增加了框架的灵活性。二是开发者可以结合实际需求，对框架进行拓展，使其能够更好的工作。以 MyBatis 为例，我们可基于 MyBatis 插件机制实现分页、分表，监控等功能。由于插件和业务无关，业务也无法感知插件的存在。因此可以无感植入插件，在无形中增强功能。 开发 MyBatis 插件需要对 MyBatis 比较深了解才行，一般来说最好能够掌握 MyBatis 的源码，门槛相对较高。本篇文章在分析完 MyBatis 插件机制后，会手写一个简单的分页插件，以帮助大家更好的掌握 MyBatis 插件的编写。 插件机制原理 我们在编写插件时，除了需要让插件类实现 Interceptor 接口，还需要通过注解标注该插件的拦截点。所谓拦截点指的是插件所能拦截的方法，MyBatis 所允许拦截的方法如下： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 如果我们想要拦截 Executor 的 query 方法，那么可以这样定义插件。 1234567891011@Intercepts(&#123; @Signature( type = Executor.class, method = \"query\", args =&#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class&#125; )&#125;)public class ExamplePlugin implements Interceptor &#123; // 省略逻辑&#125; 除此之外，我们还需将插件配置到相关文件中。这样 MyBatis 在启动时可以加载插件，并保存插件实例到相关对象（InterceptorChain，拦截器链）中。待准备工作做完后，MyBatis 处于就绪状态。我们在执行 SQL 时，需要先通过 DefaultSqlSessionFactory 创建 SqlSession 。Executor 实例会在创建 SqlSession 的过程中被创建，Executor 实例创建完毕后，MyBatis 会通过 JDK 动态代理为实例生成代理类。这样，插件逻辑即可在 Executor 相关方法被调用前执行。 以上就是 MyBatis 插件机制的基本原理。接下来，我们来看一下原理背后对应的源码是怎样的。 源码分析 植入插件逻辑 本节，我将以 Executor 为例，分析 MyBatis 是如何为 Executor 实例植入插件逻辑的。Executor 实例是在开启 SqlSession 时被创建的，因此，下面我们从源头进行分析。先来看一下 SqlSession 开启的过程。 123456789101112131415161718// -☆- DefaultSqlSessionFactorypublic SqlSession openSession() &#123; return openSessionFromDataSource(configuration.getDefaultExecutorType(), null, false);&#125;private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; // 省略部分逻辑 // 创建 Executor final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123;...&#125; finally &#123;...&#125;&#125; Executor 的创建过程封装在 Configuration 中，我们跟进去看看看。 123456789101112131415161718192021// -☆- Configurationpublic Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; // 根据 executorType 创建相应的 Executor 实例 if (ExecutorType.BATCH == executorType) &#123;...&#125; else if (ExecutorType.REUSE == executorType) &#123;...&#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; // 植入插件 executor = (Executor) interceptorChain.pluginAll(executor); return executor;&#125; 如上，newExecutor 方法在创建好 Executor 实例后，紧接着通过拦截器链 interceptorChain 为 Executor 实例植入代理逻辑。那下面我们看一下 InterceptorChain 的代码是怎样的。 123456789101112131415161718192021222324public class InterceptorChain &#123; private final List&lt;Interceptor&gt; interceptors = new ArrayList&lt;Interceptor&gt;(); public Object pluginAll(Object target) &#123; // 遍历拦截器集合 for (Interceptor interceptor : interceptors) &#123; // 调用拦截器的 plugin 方法植入相应的插件逻辑 target = interceptor.plugin(target); &#125; return target; &#125; /** 添加插件实例到 interceptors 集合中 */ public void addInterceptor(Interceptor interceptor) &#123; interceptors.add(interceptor); &#125; /** 获取插件列表 */ public List&lt;Interceptor&gt; getInterceptors() &#123; return Collections.unmodifiableList(interceptors); &#125;&#125; 以上是 InterceptorChain 的全部代码，比较简单。它的 pluginAll 方法会调用具体插件的 plugin 方法植入相应的插件逻辑。如果有多个插件，则会多次调用 plugin 方法，最终生成一个层层嵌套的代理类。形如下面： 当 Executor 的某个方法被调用的时候，插件逻辑会先行执行。执行顺序由外而内，比如上图的执行顺序为 plugin3 → plugin2 → Plugin1 → Executor。 plugin 方法是由具体的插件类实现，不过该方法代码一般比较固定，所以下面找个示例分析一下。 12345678910111213141516171819202122232425262728// -☆- ExamplePluginpublic Object plugin(Object target) &#123; return Plugin.wrap(target, this);&#125;// -☆- Pluginpublic static Object wrap(Object target, Interceptor interceptor) &#123; /* * 获取插件类 @Signature 注解内容，并生成相应的映射结构。形如下面： * &#123; * Executor.class : [query, update, commit], * ParameterHandler.class : [getParameterObject, setParameters] * &#125; */ Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = getSignatureMap(interceptor); Class&lt;?&gt; type = target.getClass(); // 获取目标类实现的接口 Class&lt;?&gt;[] interfaces = getAllInterfaces(type, signatureMap); if (interfaces.length &gt; 0) &#123; // 通过 JDK 动态代理为目标类生成代理类 return Proxy.newProxyInstance( type.getClassLoader(), interfaces, new Plugin(target, interceptor, signatureMap)); &#125; return target;&#125; 如上，plugin 方法在内部调用了 Plugin 类的 wrap 方法，用于为目标对象生成代理。Plugin 类实现了 InvocationHandler 接口，因此它可以作为参数传给 Proxy 的 newProxyInstance 方法。 到这里，关于插件植入的逻辑就分析完了。接下来，我们来看看插件逻辑是怎样执行的。 执行插件逻辑 Plugin 实现了 InvocationHandler 接口，因此它的 invoke 方法会拦截所有的方法调用。invoke 方法会对所拦截的方法进行检测，以决定是否执行插件逻辑。该方法的逻辑如下： 1234567891011121314151617181920// -☆- Pluginpublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; /* * 获取被拦截方法列表，比如： * signatureMap.get(Executor.class)，可能返回 [query, update, commit] */ Set&lt;Method&gt; methods = signatureMap.get(method.getDeclaringClass()); // 检测方法列表是否包含被拦截的方法 if (methods != null &amp;&amp; methods.contains(method)) &#123; // 执行插件逻辑 return interceptor.intercept(new Invocation(target, method, args)); &#125; // 执行被拦截的方法 return method.invoke(target, args); &#125; catch (Exception e) &#123; throw ExceptionUtil.unwrapThrowable(e); &#125;&#125; invoke 方法的代码比较少，逻辑不难理解。首先，invoke 方法会检测被拦截方法是否配置在插件的 @Signature 注解中，若是，则执行插件逻辑，否则执行被拦截方法。插件逻辑封装在 intercept 中，该方法的参数类型为 Invocation。Invocation 主要用于存储目标类，方法以及方法参数列表。下面简单看一下该类的定义。 1234567891011121314151617181920public class Invocation &#123; private final Object target; private final Method method; private final Object[] args; public Invocation(Object target, Method method, Object[] args) &#123; this.target = target; this.method = method; this.args = args; &#125; // 省略部分代码 public Object proceed() throws InvocationTargetException, IllegalAccessException &#123; // 调用被拦截的方法 return method.invoke(target, args); &#125;&#125; 关于插件的执行逻辑就分析到这，整个过程不难理解，大家简单看看即可。 实现一个分页插件 为了更好的向大家介绍 MyBatis 的插件机制，下面我将手写一个针对 MySQL 的分页插件。Talk is cheap. Show the code。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Intercepts(&#123; @Signature( type = Executor.class, // 目标类 method = \"query\", // 目标方法 args =&#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class&#125; )&#125;)public class MySqlPagingPlugin implements Interceptor &#123; private static final Integer MAPPED_STATEMENT_INDEX = 0; private static final Integer PARAMETER_INDEX = 1; private static final Integer ROW_BOUNDS_INDEX = 2; @Override public Object intercept(Invocation invocation) throws Throwable &#123; Object[] args = invocation.getArgs(); RowBounds rb = (RowBounds) args[ROW_BOUNDS_INDEX]; // 无需分页 if (rb == RowBounds.DEFAULT) &#123; return invocation.proceed(); &#125; // 将原 RowBounds 参数设为 RowBounds.DEFAULT，关闭 MyBatis 内置的分页机制 args[ROW_BOUNDS_INDEX] = RowBounds.DEFAULT; MappedStatement ms = (MappedStatement) args[MAPPED_STATEMENT_INDEX]; BoundSql boundSql = ms.getBoundSql(args[PARAMETER_INDEX]); // 获取 SQL 语句，拼接 limit 语句 String sql = boundSql.getSql(); String limit = String.format(\"LIMIT %d,%d\", rb.getOffset(), rb.getLimit()); sql = sql + \" \" + limit; // 创建一个 StaticSqlSource，并将拼接好的 sql 传入 SqlSource sqlSource = new StaticSqlSource(ms.getConfiguration(), sql, boundSql.getParameterMappings()); // 通过反射获取并设置 MappedStatement 的 sqlSource 字段 Field field = MappedStatement.class.getDeclaredField(\"sqlSource\"); field.setAccessible(true); field.set(ms, sqlSource); // 执行被拦截方法 return invocation.proceed(); &#125; @Override public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; @Override public void setProperties(Properties properties) &#123; &#125;&#125; 上面的分页插件通过 RowBounds 参数获取分页信息，并生成相应的 limit 语句。之后拼接 sql，并使用该 sql 作为参数创建 StaticSqlSource。最后通过反射替换 MappedStatement 对象中的 sqlSource 字段。以上代码中出现了一些大家不太熟悉的类，比如 BoundSql，MappedStatement 以及 StaticSqlSource，这里简单解释一下吧。BoundSql 包含了经过解析后的 sql 语句，以及使用者运行时传入的参数，这些参数最终会被设置到 sql 中。MappedStatement 与映射文件中的 ， 等节点对应，包含了节点的配置信息，比如 id，fetchSize 以及 SqlSource。StaticSqlSource 是 SqlSource 实现类之一，包含完全解析后的 sql 语句。所谓完全解析是指 sql 语句中不包含 ${xxx} 或 #{xxx} 等占位符，以及其他一些未解析的动态节点，比如 ， 等。关于这些类就介绍这么多，如果大家还是不怎么理解的话，可以看看我之前写的文章。接下里，写点测试代码验证一下插件是否可以正常运行。先来看一下 Dao 接口与映射文件的定义： 1234public interface StudentDao &#123; List&lt;Student&gt; findByPaging(@Param(\"id\") Integer id, RowBounds rb);&#125; 1234567891011&lt;mapper namespace=\"xyz.coolblog.dao6.StudentDao\"&gt; &lt;select id=\"findByPaging\" resultType=\"xyz.coolblog.model5.Student\"&gt; SELECT `id`, `name`, `age` FROM student WHERE id &gt; #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 测试代码如下： 123456789101112131415161718192021222324public class PluginTest &#123; private SqlSessionFactory sqlSessionFactory; @Before public void prepare() throws IOException &#123; String resource = \"mybatis-plugin-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); inputStream.close(); &#125; @Test public void testPlugin() &#123; SqlSession session = sqlSessionFactory.openSession(); try &#123; StudentDao studentDao = session.getMapper(StudentDao.class); studentDao.findByPaging(1, new RowBounds(20, 10)); &#125; finally &#123; session.close(); &#125; &#125;&#125; 上面代码运行之后，会打印如下日志。 在上面的输出中，SQL 语句中包含了 LIMIT 字样，这说明插件生效了。","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://blog.tgyf.com/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://blog.tgyf.com/tags/MyBatis/"}]},{"title":"Java线程池原理分析","slug":"Java并发编程/Java线程池原理分析","date":"2020-05-20T12:54:06.955Z","updated":"2020-05-20T12:54:06.955Z","comments":true,"path":"2020/05/20/Java并发编程/Java线程池原理分析/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/","excerpt":"","text":"简介 线程池可以简单看做是一组线程的集合，通过使用线程池，我们可以方便的复用线程，避免了频繁创建和销毁线程所带来的开销。在应用上，线程池可应用在后端相关服务中。比如 Web 服务器，数据库服务器等。以 Web 服务器为例，假如 Web 服务器会收到大量短时的 HTTP 请求，如果此时我们简单的为每个 HTTP 请求创建一个处理线程，那么服务器的资源将会很快被耗尽。当然我们也可以自己去管理并复用已创建的线程，以限制资源的消耗量，但这样会使用程序的逻辑变复杂。好在，幸运的是，我们不必那样做。在 JDK 1.5 中，官方已经提供了强大的线程池工具类。通过使用这些工具类，我们可以用低廉的代价使用多线程技术。 线程池作为 Java 并发重要的工具类，在会用的基础上，我觉得很有必要去学习一下线程池的相关原理。毕竟线程池除了要管理线程，还要管理任务，同时还要具备统计功能。所以多了解一点，还是可以扩充眼界的，同时也可以更为熟悉线程池技术。 继承体系 线程池所涉及到的接口和类并不是很多，其继承体系也相对简单。相关继承关系如下： 如上图，最顶层的接口 Executor 仅声明了一个方法execute。ExecutorService 接口在其父类接口基础上，声明了包含但不限于shutdown、submit、invokeAll、invokeAny 等方法。至于 ScheduledExecutorService 接口，则是声明了一些和定时任务相关的方法，比如 schedule和scheduleAtFixedRate。线程池的核心实现是在 ThreadPoolExecutor 类中，我们使用 Executors 调用newFixedThreadPool、newSingleThreadExecutor和newCachedThreadPool等方法创建线程池均是 ThreadPoolExecutor 类型。 以上是对线程池继承体系的简单介绍，这里先让大家对线程池大致轮廓有一定的了解。接下来我会介绍一下线程池的实现原理，继续往下看吧。 原理分析 核心参数分析 核心参数简介 如上节所说，线程池的核心实现即 ThreadPoolExecutor 类。该类包含了几个核心属性，这些属性在可在构造方法进行初始化。在介绍核心属性前，我们先来看看 ThreadPoolExecutor 的构造方法，如下： 12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 如上所示，构造方法的参数即核心参数，这里我用一个表格来简要说明一下各个参数的意义。如下： 参数 说明 corePoolSize 核心线程数。当线程数小于该值时，线程池会优先创建新线程来执行新任务 maximumPoolSize 线程池所能维护的最大线程数 keepAliveTime 空闲线程的存活时间 workQueue 任务队列，用于缓存未执行的任务 threadFactory 线程工厂。可通过工厂为新建的线程设置更有意义的名字 handler 拒绝策略。当线程池和任务队列均处于饱和状态时，使用拒绝策略处理新任务。默认是 AbortPolicy，即直接抛出异常 以上是各个参数的简介，下面我将会针对部分参数进行详细说明，继续往下看。 线程创建规则 在 Java 线程池实现中，线程池所能创建的线程数量受限于 corePoolSize 和 maximumPoolSize 两个参数值。线程的创建时机则和 corePoolSize 以及 workQueue 两个参数有关。下面列举一下线程创建的4个规则（线程池中无空闲线程），如下： 线程数量小于 corePoolSize，直接创建新线程处理新的任务 线程数量大于等于 corePoolSize，workQueue 未满，则缓存新任务 线程数量大于等于 corePoolSize，但小于 maximumPoolSize，且 workQueue 已满。则创建新线程处理新任务 线程数量大于等于 maximumPoolSize，且 workQueue 已满，则使用拒绝策略处理新任务 简化一下上面的规则： 序号 条件 动作 1 线程数 &lt; corePoolSize 创建新线程 2 线程数 ≥ corePoolSize，且 workQueue 未满 缓存新任务 3 corePoolSize ≤ 线程数 ＜ maximumPoolSize，且 workQueue 已满 创建新线程 4 线程数 ≥ maximumPoolSize，且 workQueue 已满 使用拒绝策略处理 资源回收 考虑到系统资源是有限的，对于线程池超出 corePoolSize 数量的空闲线程应进行回收操作。进行此操作存在一个问题，即回收时机。目前的实现方式是当线程空闲时间超过 keepAliveTime 后，进行回收。除了核心线程数之外的线程可以进行回收，核心线程内的空闲线程也可以进行回收。回收的前提是allowCoreThreadTimeOut属性被设置为 true，通过public void allowCoreThreadTimeOut(boolean) 方法可以设置属性值。 排队策略 如3.1.2 线程创建规则一节中规则2所说，当线程数量大于等于 corePoolSize，workQueue 未满时，则缓存新任务。这里要考虑使用什么类型的容器缓存新任务，通过 JDK 文档介绍，我们可知道有3中类型的容器可供使用，分别是同步队列，有界队列和无界队列。对于有优先级的任务，这里还可以增加优先级队列。以上所介绍的4中类型的队列，对应的实现类如下： 实现类 类型 说明 SynchronousQueue 同步队列 该队列不存储元素，每个插入操作必须等待另一个线程调用移除操作，否则插入操作会一直阻塞 ArrayBlockingQueue 有界队列 基于数组的阻塞队列，按照 FIFO 原则对元素进行排序 LinkedBlockingQueue 无界队列 基于链表的阻塞队列，按照 FIFO 原则对元素进行排序 PriorityBlockingQueue 优先级队列 具有优先级的阻塞队列 拒绝策略 如3.1.2 线程创建规则一节中规则4所说，线程数量大于等于 maximumPoolSize，且 workQueue 已满，则使用拒绝策略处理新任务。Java 线程池提供了4中拒绝策略实现类，如下： 实现 说明 AbortPolicy 丢弃新任务，并抛出 RejectedExecutionException DiscardPolicy 不做任何操作，直接丢弃新任务 DiscardOldestPolicy 丢弃队列队首的元素，并执行新任务 CallerRunsPolicy 由调用线程执行新任务 以上4个拒绝策略中，AbortPolicy 是线程池实现类所使用的策略。我们也可以通过方法public void setRejectedExecutionHandler(RejectedExecutionHandler)修改线程池决绝策略。 重要操作 线程的创建与复用 在线程池的实现上，线程的创建是通过线程工厂接口ThreadFactory的实现类来完成的。默认情况下，线程池使用Executors.defaultThreadFactory()方法返回的线程工厂实现类。当然，我们也可以通过 public void setThreadFactory(ThreadFactory)方法进行动态修改。具体细节这里就不多说了，并不复杂，大家可以自己去看下源码。 在线程池中，线程的复用是线程池的关键所在。这就要求线程在执行完一个任务后，不能立即退出。对应到具体实现上，工作线程在执行完一个任务后，会再次到任务队列获取新的任务。如果任务队列中没有任务，且 keepAliveTime 也未被设置，工作线程则会被一致阻塞下去。通过这种方式即可实现线程复用。 说完原理，再来看看线程的创建和复用的相关代码（基于 JDK 1.8），如下： ThreadPoolExecutor.Worker.java 123456789101112Worker(Runnable firstTask) &#123; setState(-1); this.firstTask = firstTask; // 调用线程工厂创建线程 this.thread = getThreadFactory().newThread(this);&#125;// Worker 实现了 Runnable 接口public void run() &#123; runWorker(this);&#125; ThreadPoolExecutor.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); boolean completedAbruptly = true; try &#123; // 循环从任务队列中获取新任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; // 执行新任务 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; // 线程退出后，进行后续处理 processWorkerExit(w, completedAbruptly); &#125;&#125; 提交任务 通常情况下，我们可以通过线程池的submit方法提交任务。被提交的任务可能会立即执行，也可能会被缓存或者被拒绝。任务的处理流程如下图所示： 上面的流程图不是很复杂，下面再来看看流程图对应的代码，如下： AbstractExecutorService.java 12345678public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) throw new NullPointerException(); // 创建任务 RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); // 提交任务 execute(ftask); return ftask;&#125; ThreadPoolExecutor.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); // 如果工作线程数量 &lt; 核心线程数，则创建新线程 if (workerCountOf(c) &lt; corePoolSize) &#123; // 添加工作者对象 if (addWorker(command, true)) return; c = ctl.get(); &#125; // 缓存任务，如果队列已满，则 offer 方法返回 false。否则，offer 返回 true if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; // 添加工作者对象，并在 addWorker 方法中检测线程数是否小于最大线程数 else if (!addWorker(command, false)) // 线程数 &gt;= 最大线程数，使用拒绝策略处理任务 reject(command);&#125;private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); // 检测工作线程数与核心线程数或最大线程数的关系 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; // 创建工作者对象，细节参考上一节所贴代码 w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // 将 worker 对象添加到 workers 集合中 workers.add(w); int s = workers.size(); // 更新 largestPoolSize 属性 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 开始执行任务 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 上面的代码略多，不过结合上面的流程图，和我所写的注释，理解主逻辑应该不难。 关闭线程池 我们可以通过shutdown和shutdownNow两个方法关闭线程池。两个方法的区别在于，shutdown 会将线程池的状态设置为SHUTDOWN，同时该方法还会中断空闲线程。shutdownNow 则会将线程池状态设置为STOP，并尝试中断所有的线程。中断线程使用的是Thread.interrupt方法，未响应中断方法的任务是无法被中断的。最后，shutdownNow 方法会将未执行的任务全部返回。 调用 shutdown 和 shutdownNow 方法关闭线程池后，就不能再向线程池提交新任务了。对于处于关闭状态的线程池，会使用拒绝策略处理新提交的任务。 几种线程池 一般情况下，我们并不直接使用 ThreadPoolExecutor 类创建线程池，而是通过 Executors 工具类去构建线程池。通过 Executors 工具类，我们可以构造5中不同的线程池。下面通过一个表格简单介绍一下几种线程池，如下： 静态构造方法 说明 newFixedThreadPool(int nThreads) 构建包含固定线程数的线程池，默认情况下，空闲线程不会被回收 newCachedThreadPool() 构建线程数不定的线程池，线程数量随任务量变动，空闲线程存活时间超过60秒后会被回收 newSingleThreadExecutor() 构建线程数为1的线程池，等价于 newFixedThreadPool(1) 所构造出的线程池 newScheduledThreadPool(int corePoolSize) 构建核心线程数为 corePoolSize，可执行定时任务的线程池 newSingleThreadScheduledExecutor() 等价于 newScheduledThreadPool(1)","categories":[],"tags":[]},{"title":"LinkedList源码分析","slug":"Java基础/LinkedList源码分析","date":"2020-05-20T10:00:31.210Z","updated":"2020-05-20T10:00:31.210Z","comments":true,"path":"2020/05/20/Java基础/LinkedList源码分析/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Java%E5%9F%BA%E7%A1%80/LinkedList%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"LinkedList介绍 LinkedList 是 Java 集合框架中一个重要的实现，其底层采用的双向链表结构.和 ArrayList 一样，LinkedList 也支持空值和重复值.由于 LinkedList 基于链表实现，存储元素过程中，无需像 ArrayList 那样进行扩容.但有得必有失，LinkedList 存储元素的节点需要额外的空间存储前驱和后继的引用.另一方面，LinkedList 在链表头部和尾部插入效率比较高，但在指定位置进行插入时，效率一般.原因是，在指定位置插入需要定位到该位置处的节点，此操作的时间复杂度为O(N).最后，LinkedList 是非线程安全的集合类，并发环境下，多个线程同时操作 LinkedList，会引发不可预知的错误. 继承体系 LinkedList 的继承体系较为复杂，继承自 AbstractSequentialList，同时又实现了 List 和 Deque 接口.继承体系图如下: 它的主要操作有三个： 在尾部添加元素 (add, offer) 查看头部元素 (element, peek)，返回头部元素，但不改变队列 删除头部元素 (remove, poll)，返回头部元素，并且从队列中删除 每个操作对应的两个方法的区别在于，对于特殊情况的处理不同。特殊情况是指，队列为空或者队列为满，为空容易理解，为满是指队列有长度大小限制，而且已经占满了。LinkedList的实现中，队列长度没有限制，但别的Queue的实现可能有。 LinkedList 继承自 AbstractSequentialList，AbstractSequentialList 又是什么呢？从实现上，AbstractSequentialList 提供了一套基于顺序访问的接口.通过继承此类，子类仅需实现部分代码即可拥有完整的一套访问某种序列表（比如链表）的接口.深入源码，AbstractSequentialList 提供的方法基本上都是通过 ListIterator 实现的，比如： 123456789101112131415161718public E get(int index) &#123; try &#123; return listIterator(index).next(); &#125; catch (NoSuchElementException exc) &#123; throw new IndexOutOfBoundsException(\"Index: \"+index); &#125;&#125;public void add(int index, E element) &#123; try &#123; listIterator(index).add(element); &#125; catch (NoSuchElementException exc) &#123; throw new IndexOutOfBoundsException(\"Index: \"+index); &#125;&#125;// 留给子类实现public abstract ListIterator&lt;E&gt; listIterator(int index); 所以只要继承类实现了 listIterator 方法，它不需要再额外实现什么即可使用.对于随机访问集合类一般建议继承 AbstractList 而不是 AbstractSequentialList.LinkedList 和其父类一样，也是基于顺序访问.所以 LinkedList 继承了 AbstractSequentialList，但 LinkedList 并没有直接使用父类的方法，而是重新实现了一套的方法. 另外，LinkedList 还实现了 Deque (double ended queue)，Deque 又继承自 Queue 接口.这样 LinkedList 就具备了队列的功能.比如，我们可以这样使用： 1Queue&lt;T&gt; queue = new LinkedList&lt;&gt;(); 除此之外，我们基于 LinkedList 还可以实现一些其他的数据结构，比如栈，以此来替换 Java 集合框架中的 Stack 类（该类实现的不好，《Java 编程思想》一书的作者也对此类进行了吐槽）. 关于 LinkedList 继承体系先说到这，下面进入源码分析部分. 源码分析 查找 LinkedList 底层基于链表结构，无法向 ArrayList 那样随机访问指定位置的元素.LinkedList 查找过程要稍麻烦一些，需要从链表头结点（或尾节点）向后查找，时间复杂度为 O(N).相关源码如下： 1234567891011121314151617181920212223public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;Node&lt;E&gt; node(int index) &#123; /* * 则从头节点开始查找，否则从尾节点查找 * 查找位置 index 如果小于节点数量的一半， */ if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; // 循环向后查找，直至 i == index for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 上面的代码比较简单，主要是通过遍历的方式定位目标位置的节点.获取到节点后，取出节点存储的值返回即可.这里面有个小优化，即通过比较 index 与节点数量 size/2 的大小，决定从头结点还是尾节点进行查找.查找操作的代码没什么复杂的地方，这里先讲到这里. 遍历 链表的遍历过程也很简单，和上面查找过程类似，我们从头节点往后遍历就行了.但对于 LinkedList 的遍历还是需要注意一些，不然可能会导致代码效率低下.通常情况下，我们会使用 foreach 遍历 LinkedList，而 foreach 最终转换成迭代器形式.所以分析 LinkedList 的遍历的核心就是它的迭代器实现，相关代码如下： 1234567891011121314151617181920212223242526272829303132333435public ListIterator&lt;E&gt; listIterator(int index) &#123; checkPositionIndex(index); return new ListItr(index);&#125;private class ListItr implements ListIterator&lt;E&gt; &#123; private Node&lt;E&gt; lastReturned; private Node&lt;E&gt; next; private int nextIndex; private int expectedModCount = modCount; /** 构造方法将 next 引用指向指定位置的节点 */ ListItr(int index) &#123; // assert isPositionIndex(index); next = (index == size) ? null : node(index); nextIndex = index; &#125; public boolean hasNext() &#123; return nextIndex &lt; size; &#125; public E next() &#123; checkForComodification(); if (!hasNext()) throw new NoSuchElementException(); lastReturned = next; next = next.next; // 调用 next 方法后，next 引用都会指向他的后继节点 nextIndex++; return lastReturned.item; &#125; // 省略部分方法&#125; 上面的方法很简单，大家应该都能很快看懂，这里就不多说了.下面来说说遍历 LinkedList 需要注意的一个点. 我们都知道 LinkedList 不擅长随机位置访问，如果大家用随机访问的方式遍历 LinkedList，效率会很差.比如下面的代码： 12345678List&lt;Integet&gt; list = new LinkedList&lt;&gt;();list.add(1)list.add(2)......for (int i = 0; i &lt; list.size(); i++) &#123; Integet item = list.get(i); // do something&#125; 当链表中存储的元素很多时，上面的遍历方式对于效率来说就是灾难.原因在于，通过上面的方式每获取一个元素，LinkedList 都需要从头节点（或尾节点）进行遍历，效率不可谓不低.在我的电脑（MacBook Pro Early 2015, 2.7 GHz Intel Core i5）实测10万级的数据量，耗时约7秒钟.20万级的数据量耗时达到了约34秒的时间.50万级的数据量耗时约250秒.从测试结果上来看，上面的遍历方式在大数据量情况下，效率很差.大家在日常开发中应该尽量避免这种用法. 插入 LinkedList 除了实现了 List 接口相关方法，还实现了 Deque 接口的很多方法，所以我们有很多种方式插入元素.但这里，我只打算分析 List 接口中相关的插入方法，其他的方法大家自己看吧.LinkedList 插入元素的过程实际上就是链表链入节点的过程，学过数据结构的同学对此应该都很熟悉了.这里简单分析一下，先看源码吧： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** 在链表尾部插入元素 */public boolean add(E e) &#123; linkLast(e); return true;&#125;/** 在链表指定位置插入元素 */public void add(int index, E element) &#123; checkPositionIndex(index); // 判断 index 是不是链表尾部位置，如果是，直接将元素节点插入链表尾部即可 if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;/** 将元素节点插入到链表尾部 */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; // 创建节点，并指定节点前驱为链表尾节点 last，后继引用为空 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); // 将 last 引用指向新节点 last = newNode; // 判断尾节点是否为空，为空表示当前链表还没有节点 if (l == null) first = newNode; else l.next = newNode; // 让原尾节点后继引用 next 指向新的尾节点 size++; modCount++;&#125;/** 将元素节点插入到 succ 之前的位置 */void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; // 1. 初始化节点，并指明前驱和后继节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); // 2. 将 succ 节点前驱引用 prev 指向新节点 succ.prev = newNode; // 判断尾节点是否为空，为空表示当前链表还没有节点 if (pred == null) first = newNode; else pred.next = newNode; // 3. succ 节点前驱的后继引用指向新节点 size++; modCount++;&#125; 上面是插入过程的源码，我对源码进行了比较详细的注释，应该不难看懂.上面两个 add 方法只是对操作链表的方法做了一层包装，核心逻辑在 linkBefore 和 linkLast 中.这里以 linkBefore 为例，它的逻辑流程如下： 创建新节点，并指明新节点的前驱和后继 将 succ 的前驱引用指向新节点 如果 succ 的前驱不为空，则将 succ 前驱的后继引用指向新节点 删除 如果大家看懂了上面的插入源码分析，那么再看删除操作实际上也很简单了.删除操作通过解除待删除节点与前后节点的链接，即可完成任务.过程比较简单，看源码吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public boolean remove(Object o) &#123; if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; // 遍历链表，找到要删除的节点 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); // 将节点从链表中移除 return true; &#125; &#125; &#125; return false;&#125;public E remove(int index) &#123; checkElementIndex(index); // 通过 node 方法定位节点，并调用 unlink 将节点从链表中移除 return unlink(node(index));&#125;/** 将某个节点从链表中移除 */E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; // prev 为空，表明删除的是头节点 if (prev == null) &#123; first = next; &#125; else &#123; // 将 x 的前驱的后继指向 x 的后继 prev.next = next; // 将 x 的前驱引用置空，断开与前驱的链接 x.prev = null; &#125; // next 为空，表明删除的是尾节点 if (next == null) &#123; last = prev; &#125; else &#123; // 将 x 的后继的前驱指向 x 的前驱 next.prev = prev; // 将 x 的后继引用置空，断开与后继的链接 x.next = null; &#125; // 将 item 置空，方便 GC 回收 x.item = null; size--; modCount++; return element;&#125; 和插入操作一样，删除操作方法也是对底层方法的一层保证，核心逻辑在底层 unlink 方法中.所以长驱直入，直接分析 unlink 方法吧.unlink 方法的逻辑如下（假设删除的节点既不是头节点，也不是尾节点）： 将待删除节点 x 的前驱的后继指向 x 的后继 将待删除节点 x 的前驱引用置空，断开与前驱的链接 将待删除节点 x 的后继的前驱指向 x 的前驱 将待删除节点 x 的后继引用置空，断开与后继的链接 总结 通过上面的分析，大家对 LinkedList 的底层实现应该很清楚了.总体来看 LinkedList 的源码并不复杂，大家耐心看一下，一般都能看懂.","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://blog.tgyf.com/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"Java集合框架","slug":"Java基础/Java集合框架","permalink":"http://blog.tgyf.com/categories/Java%E5%9F%BA%E7%A1%80/Java%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"LinkedList","slug":"LinkedList","permalink":"http://blog.tgyf.com/tags/LinkedList/"},{"name":"Java集合框架","slug":"Java集合框架","permalink":"http://blog.tgyf.com/tags/Java%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/"}]},{"title":"并发机制底层实现原理","slug":"Java并发编程/并发机制底层实现原理","date":"2020-05-20T09:36:42.129Z","updated":"2020-05-20T09:36:42.129Z","comments":true,"path":"2020/05/20/Java并发编程/并发机制底层实现原理/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/%E5%B9%B6%E5%8F%91%E6%9C%BA%E5%88%B6%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"一、本地内存和线程安全问题 1、缓存行 CPU不会直接和内存(主存)交互，而是通过总线将数据读到自己的缓存行中。 缓存行: 缓存是分段（line）的，一个段对应一块存储空间，我们称之为缓存行； 它是CPU缓存中可分配的最小存储单元，通常来说是64字节； 当CPU看到一条读取内存的指令时，它会把内存地址传递给一级数据缓存，一级数据缓存会检查它是否有这个内存地址对应的缓存段，如果没有就把整个缓存段从内存（或更高一级的缓存）中加载进来； 2、写缓冲区 CPU不会直接和内存(主存)交互，会将要读取的数据先写入到自己的写缓冲区，随后才会刷新到内存。 3、线程安全问题 实际中都是由很多CPU来执行并发程序，不同处理器同时执行不同的线程（每个线程都有一个仅对执行自己的处理器可见的本地内存)； 所以就会出现主内存中i = 1，线程A读取到自己的本地内存i++，于此同时线程B也读取到主内存i= 1到自己的本地内存执行i++，待两个线程的本地内存刷新到主内存时i = 2。于是引发了线程安全问题。 线程安全问题总结: 线程都是在自己的本地内存中操作共享变量的，仅对执行自己的处理器可见而对其他处理器不可见。 而它们在自己的本地内存对共享变量的更新何时会刷新到主内存、会按照什么顺序刷新到主内存是不可预见的。 二、volatile实现原理 总结: 对于声明了volatile的变量进行写操作的时候，JVM会向处理器发送一条LOCK前缀的指令。会把这个变量所在缓存行的数据写回到主内存中； 在多处理器的情况下，保证各个处理器缓存一致性的特点，实现缓存一致性协议； 通过加入内存屏障和禁止重排序优化实现。 对volatile变量写操作时，会在写操作后面加入一条store屏障指令，将本地内存中的共享变量值刷新到主内存； 对volatile变量读操作时，会在读操作前加入一条load屏障指令，从主内存中读取共享变量； 1、volatile语义 使用volatile关键字可以保证共享变量之间的可见性，被volatile修饰的变量在线程之间就是可见的，能保证变量被一致性的更新。 volatile做的两件事: 1、锁定缓存行； 在某个处理器将共享数据写入自己的缓冲区 (对应线程对本地内存中的共享记量做修改) 时，会使用缓存锁定其他也读取了该共享记量的缓存行，使其他处理器不能访问该共享专量。 早期使用的是总线锁定，即一经锁定，其他处理器就不能访问所有共享记量，但是这会影响处理器读写其他共享变量，影响效率。 2、刷新内存，保证数据一致性； 该处理器将自己写缓冲区中的所有数据刷新到主内存 (包括非volatile变量) 。 由缓存一致性协议来保证其他CPU重新读数据 (其他处理器会通过总线嗅探其他处理器写组冲区中的更改，一经发现就会将自己的缓存行置为无效状态(看自己的是不是过期了)，下次访问数据时需要到主内存中重新读) 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 1、保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 2、禁止进行指令重排序。 由于缓存行为32字节宽或者64字节宽，因此避免缓存锁定多个共享资源，可以采用字节填充的方式来提高对象并发性能。 2、Lock前缀指令 加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令 lock前缀指令实际上相当于一个内存屏障（也称内存栅栏），内存屏障会提供3个功能： 1、它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 2、它会强制将对缓存的修改操作立即写入主存； 3、如果是写操作，它会导致其他CPU(处理器)中对应的缓存行无效。 在JVM底层volatile是采用“内存屏障”来实现的。 可以得出lock指令的几个作用： 1、锁总线，其它CPU对内存的读写请求都会被阻塞，直到锁释放，不过实际后来的处理器都采用锁缓存替代锁总线，因为锁总线的开销比较大，锁总线期间其他CPU没法访问内存 2、lock后的写操作会回写已修改的数据，同时让其它CPU相关缓存行失效，从而重新从主存中加载最新的数据 3、不是内存屏障却能完成类似内存屏障的功能，阻止屏障两遍的指令重排序 这种场景下多缓存的数据一致是通过缓存一致性协议来保证的，我们来看一下什么是缓存一致性协议。 3、缓存一致性 LOCK#会锁总线，实际上这不现实，因为锁总线效率太低了。因此最好能做到：使用多组缓存，但是它们的行为看起来只有一组缓存那样。缓存一致性协议就是为了做到这一点而设计的，就像名称所暗示的那样，这类协议就是要使多组缓存的内容保持一致。 缓存一致性协议有多种，但是日常处理的大多数计算机设备都属于&quot;嗅探（snooping）&quot;协议，基本思想如下: 所有内存的传输都发生在一条共享的总线上，而所有的处理器都能看到这条总线：缓存本身是独立的，但是内存是共享资源，所有的内存访问都要经过仲裁（同一个指令周期中，只有一个CPU缓存可以读写内存）。 CPU缓存不仅仅在做内存传输的时候才与总线打交道，而是不停在嗅探总线上发生的数据交换，跟踪其他缓存在做什么。所以当一个缓存代表它所属的处理器去读写内存时，其它处理器都会得到通知，它们以此来使自己的缓存保持同步。只要某个处理器一写内存，其它处理器马上知道这块内存在它们的缓存段中已失效。 4、回看volatile原理 工作内存Work Memory其实就是对CPU寄存器和高速缓存的抽象，或者说每个线程的工作内存也可以简单理解为CPU寄存器和高速缓存。 那么当写两条线程Thread-A与Threab-B同时操作主存中的一个volatile变量i时，Thread-A写了变量i，那么： Thread-A发出LOCK#指令 发出的LOCK#指令锁总线（或锁缓存行），同时让Thread-B高速缓存中的缓存行内容失效 Thread-A向主存回写最新修改的i Thread-B读取变量i，那么： Thread-B发现对应地址的缓存行被锁了，等待锁的释放，缓存一致性协议会保证它读取到最新的值 由此可以看出，volatile关键字的读和普通变量的读取相比基本没差别，差别主要还是在变量的写操作上。 三、sychronized实现原理 1、Monitor 利用synchronized实现同步的基础，Java中的每一个对象都可以作为锁，有以下3种形式 对于普通同步方法，锁是当前实例对象； 对于静态同步方法，锁是当前类的Class对象； 对于同步方法块，锁住的是synchonized括号里配置的对象； Java 虚拟机中的同步(Synchronization)是基于进入和退出Monitor对象实现， 无论是显式同步(有明确的 monitorenter 和 monitorexit 指令，即同步代码块)还是隐式同步都是如此。 monitorenter指令实在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束和异常处，每个monitorenter必须要有对应的额monitorexit与之配对，任何对象都有monitor与之关联。 线程只有持有到monitor，才会属于锁定状态，线程会尝试获取对象对应的monitor的所有权，即尝试获得对象的锁。 在 Java 语言中，同步用的最多的地方可能是被 synchronized 修饰的同步方法。同步方法并不是由monitorenter 和 monitorexit 指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的 ACC_SYNCHRONIZED 标志来隐式实现的。 123456789101112131415161718192021222324252627282930313233341 public void add(Object obj)&#123;2 synchronized (obj)&#123;3 //do something4 &#125;5 &#125;反编译后： 1public class com.zxin.thread.SynchronizedDemo &#123; 2 public com.wuzy.thread.SynchronizedDemo(); 3 Code: 4 0: aload_0 5 1: invokespecial #1 6 4: return 7 8 public void add(java.lang.Object); 9 Code:10 0: aload_111 1: dup12 2: astore_213 3: monitorenter //进入同步方法14 4: aload_215 5: monitorexit //退出同步方法16 6: goto 1417 9: astore_318 10: aload_219 11: monitorexit //退出同步方法20 12: aload_321 13: athrow22 14: return23 Exception table:24 from to target type25 4 6 9 any26 9 12 9 any27&#125; 看下第13行~15行代码**，发现同步代码块是使用monitorenter和monitorexit指令来进行代码同步的,注意看第19行代码，为什么会多出一个monitorexit指令，主要是JVM为了防止代码出现异常**，也能正确退出同步方法。 同步方法并不是用monitorenter和monitorexit指令来进行同步的，实际上同步方法会被翻译成普通的方法调用和返回指令如:invokevirtual、areturn指令，在VM字节码层面并没有任何特别的指令来实现被synchronized修饰的方法，而是在Class文件的方法表中将该方法的access_flags字段中的synchronized标志位置设为1，表示该方法是同步方法并使用调用该方法的对象或该方法所属的Class在JVM的内部对象表示做为锁对象。 总结: 同步代码块: 使用monitorenter和monitorexit； 同步方法: 使用方法修饰符ACC_ASYNCHRONIZED。 2、Java对象头 在JVM内存中，对象在内存中的布局分为3块: 对象头、实例数据和对其填充。 其中对象头包括 : Mark Word、Class Meta Data、Array Length。 Mark Word: 锁标志位等跟锁有关的信息，2个字节存储(数组则3个)； Class Meta Data: 对象所属类的类元数据信息； Array Length : 对象为数组类型时才有，记录了数组长度； Mark Word的状态变化: 锁标志的意义: 锁标识lock == 00标识轻量级锁； 锁标识lock = 10标识重量级锁； 偏向锁标识biased_lock = 1表示偏向锁； 偏向锁标识biased_lock = 0且锁标识=01表示无锁状态； 3、锁的升级 锁的状态: 无锁、偏向锁、轻量级锁、重量级锁。 锁升级: JVM检测到不同的竞争状态时，自动切换到合适的锁实现。 无锁: 初始情况(没有任何线程访问过同步块)； 偏向锁: 大多数场景不会出现多线程并发访问共享资源的情况，针对并发强度小的情况，引入了偏向锁，在一个线程访问同步块时有如下操作: 1、判断对象头的Mark Word中的线程ID是否指的就是当前线程，如果是，直接进入同步块，如果不是，进入步骤2； 2、如果对象头的Mark Word中的线程ID不是指向当前线程，那么查看Mark Word中&quot;是否是偏向锁&quot;这一标志位。如果是1，指向步骤3；否则表示是无锁状态，CAS将Mard Word 中的线程ID指向当前线程，进入同步块； 3、如果是1就说明是偏向锁，而且出现了锁争用的情况，偏向锁升级为轻量级锁； 偏向锁撤销 由于偏向锁是&quot;偏向&quot;某一个线程的，如果线程&quot;挂了&quot;怎么办？这就需要偏向锁撤销机制； 即在一个安全点（没有字节码执行），首先暂停锁&quot;偏向&quot;的线程，然后检查线程状态，如果线程&quot;挂了&quot;那么将锁置为无锁状态； 轻量级锁: 获取锁 1、首先将同步对象(synchronize内的对象)的Mark Word复制一份到当前线程栈桢的一块空间中，并使用CAS将同步对象的Mark Word更新为指向该空间的指针，如果更新成功那么成功获取锁；否则CAS自旋； 2、获取锁的线程在执行完同步块释放锁时，CAS将同步对象的Mark Word替换回占中原先保存的Mark Word，如果成功，则表示成功释放锁，没有竞争发生。否则表明当前锁存在竞争，升级为重量级锁； 重量级锁(排他锁): 想进入同步快需要获取对象的monitor，退出时释放monitor。 获取对象monitor时如果monitor已被持有，则该线程将进入monitor的阻塞队列，直到monitor被释放，monitor阻塞队列上的线程将开启一轮新的竞争。 四、原子操作实现原理 1、处理器实现原子操作 1、使用总线锁保证原子性 所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占使用共享内存。 2、使用缓存锁保证原子性 所谓“缓存锁定”就是如果缓存在处理器缓存行中内存区域在LOCK操作期间被锁定，当它执行锁操作回写内存时，处理器不在总线上声言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性； 2、Java实现原子操作 两种方式：锁和循环CAS CAS全称Compare-and-Swap（比较并交换），JVM中的CAS操作是依赖处理器提供的cmpxchg指令完成的，CAS指令中有3个操作数，分别是内存位置V、旧的预期值A和新值B。 当CAS指令执行时，当且仅当内存位置V符合旧预期值时A时，处理器才会用新值B去更新V的值，否则就不执行更新，但是无论是否更新V，都会返回V的旧值，该操作过程就是一个原子操作 JDK1.5之后才可以使用CAS，由sun.misc.Unsafe类里面的compareAndSwapInt()和compareAndSwapLong()等方法包装实现，虚拟机在即时编译时，对这些方法做了特殊处理，会编译出一条相关的处理器CAS指令 CAS就是Compare And Swap，涉及到两个术语: 预期值、更新值。在对内存中的值进行更新时，拿A和B两线程同时对i变量进行i++举例: 首先A线程读到i的值为1，执行i++操作并刷新到主内存时，比较一下主内存中的还是不是1 (预期值) ，如果是就将共享记量替换为2 (更新值) 。 后来B也准备将i= 2刷新到主内存时，发现主内存中的不等于1 (预期值) ，于是更新失败，重新读取i=2，进行CAS更新，这个不断尝试CAS更新的过程称为自旋。 CAS实现原子操作的三大问题 1、ABA问题：初次读取内存旧值时是A，再次检查之前这段期间，如果内存位置的值发生过从A变成B再变回A的过程，我们就会错误的检查到旧值还是A，认为没有发生变化，其实已经发生过A-B-A得变化，这就是CAS操作的ABA问题 解决方法：使用版本号，即1A-2B-3A，这样就会发现1A到3A的变化，不存在ABA变化无感知问题，JDK的atomic包中提供一个带有标记的原子引用类AtomicStampedReference来解决ABA问题 2、循环时间长开销大：自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销 3、只能保证一个共享变量的原子操作：当对一个共享变量执行操作时，可以使用循环CAS来保证原子操作，但是多个共享变量操作时，就无法保证了 解决方法： 将多个变量组合成一个共享变量，jdk提供了AtomicReference类来保证引用对象之间的原子性，那么就可以把多个变量放在一个对象里来进行CAS操作 使用锁 除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。","categories":[{"name":"Java并发编程","slug":"Java并发编程","permalink":"http://blog.tgyf.com/categories/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Java并发编程","slug":"Java并发编程","permalink":"http://blog.tgyf.com/tags/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Mysql日志系统 - 一条SQL更新语句是如何执行的","slug":"Mysql/Mysql日志系统 - 一条SQL更新语句是如何执行的","date":"2020-05-20T09:06:27.075Z","updated":"2020-05-20T09:06:27.076Z","comments":true,"path":"2020/05/20/Mysql/Mysql日志系统 - 一条SQL更新语句是如何执行的/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%20-%20%E4%B8%80%E6%9D%A1SQL%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/","excerpt":"","text":"一、引入 一条更新语句的执行流程又是怎样的呢？ 之前你可能经常听DBA同事说，MySQL可以恢复到半个月内任意一秒的状态，这是怎样做到的呢？ 我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键ID和一个整型字段c： 1mysql&gt; create table T(ID int primary key, c int); 如果要将ID=2这一行的值加1，SQL语句就会这么写： 1mysql&gt; update T set c&#x3D;c+1 where ID&#x3D;2; 在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。如果接触MySQL，那这两个词肯定是绕不过的。 二、重做日志(redo log) 不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。 如果有人要赊账或者还账的话，掌柜一般有两种做法： 一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉； 另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算。 在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。 这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的帮助，每次记账都得翻账本，效率是不是低得让人难以忍受？ 同样，在MySQL里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。为了解决这个问题，MySQL的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘(账本)里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 与此类似，InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示: write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 三、归档日志(binlog) MySQL整体来看，其实就有两块： 一块是Server层，它主要做的是MySQL功能层面的事情； 还有一块是引擎层，负责存储相关的具体事宜。 上面我们聊到的粉板redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。 你可能会问，为什么会有两份日志呢？ 因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。 这两种日志有以下三点不同。 redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程。 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。 这里我给出这个update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。 你可能注意到了，最后三步看上去有点“绕”，将redo log的写入拆成了两个步骤：prepare和commit，这就是&quot;两阶段提交&quot;。 两阶段提交 为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？ 前面我们说过了，binlog会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的DBA承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做： 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。 好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。 由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？ 先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。 但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。 然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。 先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。 你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？ 其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用binlog来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 四、总结 物理日志redo log和逻辑日志binlog。 redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数我建议你设置成1，这样可以保证MySQL异常重启之后数据不丢失。 sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数我也建议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。 学习后的收获: redo是物理的，binlog是逻辑的； 现在由于redo是属于InnoDB引擎，所以必须要有binlog，因为你可以使用别的引擎 保证数据库的一致性，必须要保证2份日志一致，使用的2阶段式提交；其实感觉像事务，不是成功就是失败，不能让中间环节出现，也就是一个成功，一个失败 如果有一天mysql只有InnoDB引擎了，有redo来实现复制，那么感觉oracle的DG就诞生了，物理的速度也将远超逻辑的，毕竟只记录了改动向量 binlog几大模式，一般采用row，因为遇到时间，从库可能会出现不一致的情况，但是row更新前后都有，会导致日志变大 最后2个参数，保证事务成功，日志必须落盘，这样，数据库crash后，就不会丢失某个事务的数据了。 其次说一下，对问题的理解 备份时间周期的长短，感觉有2个方便 首先，是恢复数据丢失的时间，既然需要恢复，肯定是数据丢失了。如果一天一备份的话，只要找到这天的全备，加入这天某段时间的binlog来恢复，如果一周一备份，假设是周一，而你要恢复的数据是周日某个时间点，那就，需要全备+周一到周日某个时间点的全部binlog用来恢复，时间相比前者需要增加很多；看业务能忍受的程度 其次，是数据库丢失，如果一周一备份的话，需要确保整个一周的binlog都完好无损，否则将无法恢复；而一天一备，只要保证这天的binlog都完好无损；当然这个可以通过校验，或者冗余等技术来实现，相比之下，上面那点更重要","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"Mysql架构 - 一条SQL查询语句是如何执行的","slug":"Mysql/Mysql架构 - 一条SQL查询语句是如何执行的","date":"2020-05-20T08:59:12.810Z","updated":"2020-05-20T08:59:12.811Z","comments":true,"path":"2020/05/20/Mysql/Mysql架构 - 一条SQL查询语句是如何执行的/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E6%9E%B6%E6%9E%84%20-%20%E4%B8%80%E6%9D%A1SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/","excerpt":"","text":"一、引入以及基础架构 你知道下面这条语句在MYSQL内部的执行过程吗? 1mysql&gt; select * from T where ID&#x3D;10； MySQL的基本架构示意图: 大体来说，MySQL可以分为Server层和存储引擎层两部分。 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。 二、连接器 第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的： 1mysql -h$ip -P$port -u$user -p show processlist命令可以看到链接的客户端。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时。 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，你可能会发现，有些时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。 怎么解决这个问题呢？你可以考虑以下两种方案。 1、定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 2、如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 三、查询缓存 连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步：查询缓存。 MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 好在MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样： 1mysql&gt; select SQL_CACHE * from T where ID&#x3D;10； 需要注意的是，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。 四、分析器 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。 MySQL从你输入的&quot;select&quot;这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。 五、优化器 经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的join： 1mysql&gt; select * from t1 join t2 using(ID) where t1.c&#x3D;10 and t2.d&#x3D;20; 既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。 也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。 六、执行器 MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。 开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示(在工程实现上，如果命中查询缓存，会在查询缓存放回结果的时候，做权限验证。查询也会在优化器之前调用precheck验证权限)。 123mysql&gt; select * from T where ID&#x3D;10;ERROR 1142 (42000): SELECT command denied to user &#39;b&#39;@&#39;localhost&#39; for table &#39;T&#39; 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的： 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个rows_examined的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟rows_examined并不是完全相同的。","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"Mysql索引-深入浅出索引(二)","slug":"Mysql/Mysql索引-深入浅出索引(二)","date":"2020-05-20T08:55:25.491Z","updated":"2020-05-20T08:55:25.492Z","comments":true,"path":"2020/05/20/Mysql/Mysql索引-深入浅出索引(二)/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E7%B4%A2%E5%BC%95-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95(%E4%BA%8C)/","excerpt":"","text":"在下面这个表T中，如果我执行select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？ 下面是这个表的初始化语句。 12345678mysql&gt; create table T (ID int primary key,k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT &#39;&#39;,index k(k))engine&#x3D;InnoDB;insert into T values(100,1, &#39;aa&#39;),(200,2,&#39;bb&#39;),(300,3,&#39;cc&#39;),(500,5,&#39;ee&#39;),(600,6,&#39;ff&#39;),(700,7,&#39;gg&#39;); 现在，我们一起来看看这条SQL查询语句的执行流程： 在k索引树上找到k=3的记录，取得 ID = 300； 再到ID索引树查到ID=300对应的R3； 在k索引树取下一个值k=5，取得ID=500； 再回到ID索引树查到ID=500对应的R4； 在k索引树取下一个值k=6，不满足条件，循环结束。 在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了k索引树的3条记录（步骤1、3和5），回表了两次（步骤2和4）。 在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？ 一、覆盖索引 如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 基于上面覆盖索引的说明，我们来讨论一个问题：在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？ 假设这个市民表的定义是这样的： 12345678910CREATE TABLE &#96;tuser&#96; ( &#96;id&#96; int(11) NOT NULL, &#96;id_card&#96; varchar(32) DEFAULT NULL, &#96;name&#96; varchar(32) DEFAULT NULL, &#96;age&#96; int(11) DEFAULT NULL, &#96;ismale&#96; tinyint(1) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;), KEY &#96;id_card&#96; (&#96;id_card&#96;), KEY &#96;name_age&#96; (&#96;name&#96;,&#96;age&#96;)) ENGINE&#x3D;InnoDB 我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？ 如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。 当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。 二、最左前缀原则 看到这里你一定有一个疑问，如果为每一种查询都设计一个索引，索引是不是太多了。如果我现在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？ 这里，我先和你说结论吧。B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。 为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。 可以看到，索引项是按照索引定义里面出现的字段顺序排序的。 当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到ID4，然后向后遍历得到所有需要的结果。 如果你要查的是所有名字第一个字是“张”的人，你的SQL语句的条件是&quot;where name like ‘张%’&quot;。这时，你也能够用上这个索引，查找到第一个符合条件的记录是ID3，然后向后遍历，直到不满足条件为止。 可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。 基于上面对最左前缀索引的说明，我们来讨论一个问题：在建立联合索引的时候，如何安排索引内的字段顺序。 这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 所以现在你知道了，这段开头的问题里，我们要为高频请求创建(身份证号，姓名）这个联合索引，并用这个索引支持“根据身份证号查询地址”的需求。 那么，如果既有联合查询，又有基于a、b各自的查询呢？查询条件里面只有b的语句，是无法使用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护(a,b)、(b) 这两个索引。 这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name字段是比age字段大的 ，那我就建议你创建一个（name,age)的联合索引和一个(age)的单字段索引。 三、索引下推 上一段我们说到满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。这时，你可能要问，那些不符合最左前缀的部分，会怎么样呢？ 我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是10岁的所有男孩”。那么，SQL语句是这么写的： 1mysql&gt; select * from tuser where name like &#39;张%&#39; and age&#x3D;10 and ismale&#x3D;1; 你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录ID3。当然，这还不错，总比全表扫描要好。 然后呢？ 当然是判断其他条件是否满足。 在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。 而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 图3和图4，是这两个过程的执行流程图。 在图3和4这两个图里面，每一个虚线箭头表示回表一次。 图3中，在(name,age)索引里面我特意去掉了age的值，这个过程InnoDB并不会去看age的值，只是按顺序把“name第一个字是’张’”的记录一条条取出来回表。因此，需要回表4次。 图4跟图3的区别是，InnoDB在(name,age)索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过。在我们的这个例子中，只需要对ID4、ID5这两条记录回表取数据判断，就只需要回表2次。 四、总结 回表：回到主键索引树搜索的过程，称为回表 覆盖索引：某索引已经覆盖了查询需求，称为覆盖索引，例如：select ID from T where k between 3 and 5 在引擎内部使用覆盖索引在索引K上其实读了三个记录，R3~R5(对应的索引k上的记录项)，但对于MySQL的Server层来说，它就是找引擎拿到了两条记录，因此MySQL认为扫描行数是2 最左前缀原则：B+Tree这种索引结构，可以利用索引的&quot;最左前缀&quot;来定位记录 只要满足最左前缀，就可以利用索引来加速检索。 最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符 第一原则是：如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 索引下推：在MySQL5.6之前，只能从根据最左前缀查询到ID开始一个个回表。到主键索引上找出数据行，再对比字段值。 MySQL5.6引入的索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"Mysql索引-深入浅出索引(一)","slug":"Mysql/Mysql索引-深入浅出索引(一)","date":"2020-05-20T08:53:38.171Z","updated":"2020-05-20T08:53:38.171Z","comments":true,"path":"2020/05/20/Mysql/Mysql索引-深入浅出索引(一)/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E7%B4%A2%E5%BC%95-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95(%E4%B8%80)/","excerpt":"","text":"一句话简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。 一、索引常见的模型 索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了索引模型的概念。可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是哈希表、有序数组和搜索树。 下面介绍三种模型区别: 哈希表是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的值即key，就可以找到其对应的值即Value，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。 不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。 假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示： 图中，User2和User4根据身份证号算出来的值都是N，但没关系，后面还跟了一个链表。假设，这时候你要查ID_card_n2对应的名字是什么，处理步骤就是：首先，将ID_card_n2通过哈希函数算出N；然后，按顺序遍历，找到User2。 需要注意的是，图中四个ID_card_n的值并不是递增的，这样做的好处是增加新的User时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。 你可以设想下，如果你现在要找身份证号在[ID_card_X, ID_card_Y]这个区间的所有用户，就必须全部扫描一遍了。 所以，哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。 而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示： 这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查ID_card_n2对应的名字，用二分法就可以快速得到，这个时间复杂度是O(log(N))。 同时很显然，这个索引结构支持范围查询。你要查身份证号在[ID_card_X, ID_card_Y]区间的User，可以先用二分法找到ID_card_X（如果不存在ID_card_X，就找到大于ID_card_X的第一个User），然后向右遍历，直到查到第一个大于ID_card_Y的身份证号，退出循环。 如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。 所以，有序数组索引只适用于静态存储引擎，比如你要保存的是2017年某个城市的所有人口信息，这类不会再修改的数据。 二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示： 二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果你要查ID_card_n2的话，按照图中的搜索顺序就是按照UserA -&gt; UserC -&gt; UserF -&gt; User2这个路径得到。这个时间复杂度是O(log(N))。(更新时间复杂度也是O(log(N))) 树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。 你可以想象一下一棵100万节点的平衡二叉树，树高20。一次查询可能需要访问20个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的。 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小。 以InnoDB的一个整数字段索引为例，这个N差不多是1200。这棵树高是4的时候，就可以存1200的3次方个值，这已经17亿了。考虑到树根的数据块总是在内存中的，一个10亿行的表上一个整数字段的索引，查找一个值最多只需要访问3次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。 N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于InnoDB存储引擎在MySQL数据库中使用最为广泛，所以下面我就以InnoDB为例，和你分析一下其中的索引模型。 二、InnoDB索引模型 在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。 每一个索引在InnoDB里面对应一棵B+树。 假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。 这个表的建表语句是： 12345mysql&gt; create table T(id int primary key, k int not null, name varchar(16),index (k))engine&#x3D;InnoDB; 表中R1~R5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意图如下。 从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。 根据上面的索引结构说明，我们来讨论一个问题：基于主键索引和普通索引的查询有什么区别？ 如果语句是select * from T where ID=500，即主键查询方式，则只需要搜索ID这棵B+树； 如果语句是select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为回表。 三、索引维护 B+树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行ID值为700，则只需要在R5的记录后面插入一个新记录。如果新插入的ID值为400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果R5所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约50%。 当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。 基于上面的索引维护过程说明，我们来讨论一个案例： 你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。当然事无绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。 插入新记录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。 也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的： 1、只有一个索引； 2、该索引必须是唯一索引。 你一定看出来了，这就是典型的KV场景。 由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。 这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"Mysql乐观锁和悲观锁","slug":"Mysql/Mysql乐观锁和悲观锁","date":"2020-05-20T08:46:14.028Z","updated":"2020-05-20T08:46:14.028Z","comments":true,"path":"2020/05/20/Mysql/Mysql乐观锁和悲观锁/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81/","excerpt":"","text":"转载: https://www.cnblogs.com/zhiqian-ali/p/6200874.html 悲观锁（Pessimistic Lock） 悲观锁的特点是先获取锁，再进行业务操作，即“悲观”的认为获取锁是非常有可能失败的，因此要先确保获取锁成功再进行业务操作。通常所说的“一锁二查三更新”即指的是使用悲观锁。通常来讲在数据库上的悲观锁需要数据库本身提供支持，即通过常用的select … for update操作来实现悲观锁。当数据库执行select for update时会获取被select中的数据行的行锁，因此其他并发执行的select for update如果试图选中同一行则会发生排斥（需要等待行锁被释放），因此达到锁的效果。select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用。 这里需要注意的一点是不同的数据库对select for update的实现和支持都是有所区别的，例如oracle支持select for update no wait，表示如果拿不到锁立刻报错，而不是等待，mysql就没有no wait这个选项。另外mysql还有个问题是select for update语句执行中所有扫描过的行都会被锁上，这一点很容易造成问题。因此如果在mysql中用悲观锁务必要确定走了索引(Index)，而不是全表扫描(ALL)。 乐观锁（Optimistic Lock） 乐观锁的特点先进行业务操作，不到万不得已不去拿锁。即“乐观”的认为拿锁多半是会成功的，因此在进行完业务操作需要实际更新数据的最后一步再去拿一下锁就好。 乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持。一般的做法是在需要锁的数据上增加一个版本号，或者时间戳，然后按照如下方式实现： 123456781. SELECT data AS old_data, version AS old_version FROM …;2. 根据获取的数据进行业务操作，得到new_data和new_version3. UPDATE SET data &#x3D; new_data, version &#x3D; new_version WHERE version &#x3D; old_versionif (updated row &gt; 0) &#123; &#x2F;&#x2F; 乐观锁获取成功，操作完成&#125; else &#123; &#x2F;&#x2F; 乐观锁获取失败，回滚并重试&#125; 乐观锁是否在事务中其实都是无所谓的，其底层机制是这样：在数据库内部update同一行的时候是不允许并发的，即数据库每次执行一条update语句时会获取被update行的写锁，直到这一行被成功更新后才释放。因此在业务操作进行前获取需要锁的数据的当前版本号，然后实际更新数据时再次对比版本号确认与之前获取的相同，并更新版本号，即可确认这之间没有发生并发的修改。如果更新失败即可认为老版本的数据已经被并发修改掉而不存在了，此时认为获取锁失败，需要回滚整个业务操作并可根据需要重试整个过程。 总结 乐观锁在不发生取锁失败的情况下开销比悲观锁小，但是一旦发生失败回滚开销则比较大，因此适合用在取锁失败概率比较小的场景，可以提升系统并发性能 乐观锁还适用于一些比较特殊的场景，例如在业务操作过程中无法和数据库保持连接等悲观锁无法适用的地方","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"Mysql锁机制","slug":"Mysql/Mysql锁机制","date":"2020-05-20T08:41:54.574Z","updated":"2020-05-20T08:41:54.574Z","comments":true,"path":"2020/05/20/Mysql/Mysql锁机制/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E9%94%81%E6%9C%BA%E5%88%B6/","excerpt":"","text":"一、锁概述和分类 二、表锁 三、行锁 四、优化建议 一、锁概述和分类 二、表锁 偏向MyISAM存储引擎，开销小，加锁快；无死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。 【手动增加表锁】 1lock table 表名字1 read(write)，表名字2 read(write)，其它; 【查看表上加过的锁】 1show open tables; 【释放表锁】 1unlock tables; 演示: 1234567891011121314151617181920212223242526272829303132mysql&gt; select * from mylock;+----+------+| id | name |+----+------+| 1 | a || 2 | b || 3 | c || 4 | d || 5 | e |+----+------+5 rows in set (0.00 sec)# 给mylock表加读锁,给t1表加写锁mysql&gt; lock table mylock read, t1 write;Query OK, 0 rows affected (0.02 sec)# 查看已经加锁的表, 下面的结果省略了很多行mysql&gt; show open tables;+--------------------+------------------------------------------------------+--------+-------------+| Database | Table | In_use | Name_locked |+--------------------+------------------------------------------------------+--------+-------------+| mysqlad | t1 | 1 | 0 || performance_schema | events_transactions_current | 0 | 0 || performance_schema | events_statements_summary_by_program | 0 | 0 || performance_schema | events_waits_summary_by_host_by_event_name | 0 | 0 || mysqlad | mylock | 1 | 0 || performance_schema | file_sum121 rows in set (0.00 sec)# 释放表锁mysql&gt; unlock tables;Query OK, 0 rows affected (0.00 sec) 读锁案例:下面通过两个会话窗口来演示对mylock表加读锁之后的效果: session_1 session_2 获得表mylock的READ锁定 连接终端 当前session_1可以查询该表记录 其他session(session_2)也可以查询该表 当前session_1不能查询其它没有锁定的表。 其他session_2可以查询或者更新未锁定的表 当前session_1中插入或者更新锁定的表都会提示错误： 其他session_2插入或者更新锁定表会一直等待获得锁：(阻塞) 释放锁。mysql&gt; unlock tables; session_2立即释放阻塞，马上获得锁。 演示对mylock加写锁: seession_1 session_2 获得表mylock的WRITE锁定， 当前session对锁定表的查询+更新+插入操作都可以执行： 其他session对锁定表的查询被阻塞，需要等待锁被释放： 在锁表前，如果session2有数据缓存，锁表以后，在锁住的表不发生改变的情况下session2可以读出缓存数据，一旦数据发生改变，缓存将失效，操作将被阻塞住。 释放锁mysql&gt; unlock tables; session_2立即被释放得到锁 通过上面的实验，可以发现： MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行增删改操作前，会自动给涉及的表加写锁。 MySQL的表级锁有两种模式： 表共享读锁（Table Read Lock） 表独占写锁（Table Write Lock） 锁类型 他人可读 他人可写 读锁 是 否 写锁 否 否 结合上表，所以对MyISAM表进行操作，会有以下情况： 1、对MyISAM表的读操作（加读锁），不会阻塞其他进程对同一表的读请求，但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其它进程的写操作。 2、对MyISAM表的写操作（加写锁），会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其它进程的读写操作。 简而言之，就是读锁会阻塞写，但是不会堵塞读。而写锁则会把读和写都堵塞。 总结: 可以通过show open tables来查看哪些表被枷锁了； 如何分析表锁定，可以通过检查table_locks_waited和table_locks_immediate状态变量来分析系统上的表锁定； 这里有两个状态变手记录MySQL内部表级锁定的情况，两个变量说明如下: Table_locks_immediate: 产生表级锁定的次数，表示可以立即获取锁的查询次数，每立即获取锁值加1 ; Table_locks_waited: 出现表级锁定争用而发生等待的次数(不能立即获取锁的次数，每等待一次锁值加1)，此值高则说明存在着较严重的表级锁争用情况; 总结: MyISAM的读写锁调度是写优先，这也是MyISAM不适合做写为主表的引擎。 因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。 三、行锁 特点: 偏向InnoDB存储引擎，开销大，加锁慢； 会出现死锁； 锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 InnoDB与MyISAM的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁。 Innodb存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一些，但是在整体并发处理能力方面要远远优于MyISAM的表级锁定的。当系统并发量较高的时候，Innodb的整体性能和MyISAM相比就会有比较明显的优势了。 但是，Innodb的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让Innodb的整体性能表现不仅不能比MyISAM高，甚至可能会更差。 演示案例，建表SQL: 1234567891011121314151617181920212223242526272829303132create table test_innodb_lock (a int(11),b varchar(16))engine&#x3D;innodb;insert into test_innodb_lock values(1,&#39;b2&#39;);insert into test_innodb_lock values(3,&#39;3&#39;);insert into test_innodb_lock values(4,&#39;4000&#39;);insert into test_innodb_lock values(5,&#39;5000&#39;);insert into test_innodb_lock values(6,&#39;6000&#39;);insert into test_innodb_lock values(7,&#39;7000&#39;);insert into test_innodb_lock values(8,&#39;8000&#39;);insert into test_innodb_lock values(9,&#39;9000&#39;);insert into test_innodb_lock values(1,&#39;b1&#39;);# 创建两个索引create index test_innodb_a_ind on test_innodb_lock(a);create index test_innodb_lock_b_ind on test_innodb_lock(b);# 查询结果mysql&gt; select * from test_innodb_lock;+------+------+| a | b |+------+------+| 1 | b2 || 3 | 3 || 4 | 4000 || 5 | 5000 || 6 | 6000 || 7 | 7000 || 8 | 8000 || 9 | 9000 || 1 | b1 |+------+------+9 rows in set (0.00 sec) 测试: 读己之所写。 然后看session_1和session_2同时更新a = 4的情况: 更新但是不提交，即没有commit session_2被阻塞，只能等待 提交更新mysql&gt; commit; 解除阻塞 但是如果两个会话不是更新同一行呢? 如果不是更新同一行，则就算在session_1没有commit的时候，session_2也不会阻塞。 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁。 举个例子: 因为我们的b是varchar类型的，更新的时候我故意将b的单引号去掉，此时MYSQL底层自动类型转换，但是此时就会导致索引失效，然后我们看下面，就会导致我们的行锁变成了表锁，从而导致阻塞等待。 间隙锁带来的插入问题: 【什么是间隙锁】 当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”， InnoDB也会对这个“间隙”加锁(不放过一个)，这种锁机制就是所谓的间隙锁（GAP Lock）。 【危害】 因为Query执行过程中通过过范围查找的话，他会锁定整个范围内所有的索引键值，即使这个键值并不存在。 间隙锁有一个比较致命的弱点，就是当锁定一个范围键值之后，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下这可能会对性能造成很大的危害。 面试题：常考如何锁定一行。 使用for update。 【如何分析行锁定】 通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况 mysql&gt;show status like 'innodb_row_lock%'; 对各个状态量的说明如下： Innodb_row_lock_current_waits：当前正在等待锁定的数量； Innodb_row_lock_time：从系统启动到现在锁定总时间长度； Innodb_row_lock_time_avg：每次等待所花平均时间； Innodb_row_lock_time_max：从系统启动到现在等待最常的一次所花的时间； Innodb_row_lock_waits：系统启动后到现在总共等待的次数； 对于这5个状态变量，比较重要的主要是 Innodb_row_lock_time_avg（等待平均时长）， Innodb_row_lock_waits（等待总次数） Innodb_row_lock_time（等待总时长）这三项。 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。 最后可以通过SELECT * FROM information_schema.INNODB_TRX\\G;来查询正在被锁阻塞的sql语句。 四、优化建议 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁； 尽可能较少检索条件，避免间隙锁； 尽量控制事务大小，减少锁定资源量和时间长度； 锁住某行后，尽量不要去调别的行或表，赶紧处理被锁住的行然后释放掉锁； 涉及相同表的事务，对于调用表的顺序尽量保持一致； 在业务环境允许的情况下,尽可能低级别事务隔离；","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"Mysql索引","slug":"Mysql/Mysql索引","date":"2020-05-20T08:25:42.887Z","updated":"2020-05-20T08:25:42.887Z","comments":true,"path":"2020/05/20/Mysql/Mysql索引/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E7%B4%A2%E5%BC%95/","excerpt":"","text":"1、性能下降分析 需要优化的原因：性能低、执行时间太长、等待时间太长、SQL语句欠佳（连接查询）、索引失效、服务器参数设置不合理（缓冲、线程数） 。 先看SQL执行的顺序: 12345编写过程：select dinstinct ..from ..join ..on ..where ..group by ...having ..order by ..limit ..解析过程： from .. on.. join ..where ..group by ....having ...select dinstinct ..order by limit ... 解析图: 详细参考这篇博客: https://www.cnblogs.com/annsshadow/p/5037667.html SQL优化， 主要就是在优化索引 相当于书的目录； index是帮助MYSQL高效获取数据的数据结构。索引是数据结构（树：B树(默认)、Hash树…）； 2、索引优缺点 索引的弊端： 索引本身很大， 实际上索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录，所以索引列也是要占用空间的； 索引不是所有情况均适用： a. 少量数据，b.频繁更新的字段，c.很少使用的字段 索引会降低增删改的效率；MySQL不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段， 都会调整因为更新所带来的键值变化后的索引信息。 优势： 提高查询效率（降低IO使用率） 降低CPU使用率 （…order by age desc，因为 B树索引 本身就是一个 好排序的结构，因此在排序时 可以直接使用） 3、索引分类 主键索引： 不能重复。id 不能是null (设定为主键后数据库会自动建立索引，innodb为聚簇索引)； 唯一索引 ：不能重复。id 可以是null 单值索引 ： 单列， 一个表可以多个单值索引。 复合索引 ：多个列构成的索引 （相当于二级目录 ： z: zhao） (name,age) (a,b,c,d,…,n) 创建索引的两种方式: 123456789101112131415161718192021222324创建索引： 方式一(创建)： create 索引类型 索引名 on 表(字段) 单值(普通索引)： create index dept_index on tb(dept); 唯一： create unique index name_index on tb(name) ; 复合索引 create index dept_name_index on tb(dept,name); 方式二(添加)： alter table 表名 索引类型 索引名（字段） 主键索引: ALTER TABLE &#96;table_name&#96; ADD PRIMARY KEY ( &#96;column&#96; ) 单值： alter table tb add index dept_index(dept) ; 唯一： alter table tb add unique index name_index(name); 复合索引 alter table tb add index dept_name_index(dept,name); 全文索引 ALTER TABLE &#96;table_name&#96; ADD FULLTEXT ( &#96;column&#96;) 注意：如果一个字段是primary key，则改字段默认就是 主键索引 删除索引 123删除索引：drop index 索引名 on 表名 ;drop index name_index on tb ; 查询索引 123查询索引：show index from 表名 ;show index from 表名 \\G 4、哪些情况需要建立索引，哪些不需要 需要建立索引的情况: 主键自动建立唯一索引(primary key)； 频繁作为查询条件的字段应该创建索引(where 后面的语句)； 查询中与其它表关联的字段，外键关系建立索引； 单键/组合索引的选择问题，who？(在高并发下倾向创建组合索引)； 查询中排序的字段，排序字段若通过索引去访问将大大提高排序速度； 查询中统计或者分组字段；(group by....) 不需要建立索引的情况: 表记录太少； 经常增删改的表； Where条件里用不到的字段不创建索引； 数据重复且分布平均的表字段，因此应该只为最经常查询和最经常排序的数据列建立索引。 注意，如果某个数据列包含许多重复的内容，为它建立索引就没有太大的实际效果(有一个比值，不同的个数和总个数的比值越大越好)； 5、Explain 具体可以参考这篇博客: https://blog.csdn.net/drdongshiye/article/details/84546264。 1)、概念和作用 概念: 使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是 如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈； 作用: 表的读取顺序； 哪些索引可以使用； 哪些索引被实际使用； 数据读取操作的操作类型； 表之间的引用； 每张表有多少行被优化器查询； 2)、id 表示：select查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序。 分为三种情况: a)、第一种情况: id相同，执行顺序由上至下。 此例中 先执行where 后的第一条语句 t1.id = t2.id 通过 t1.id 关联 t2.id 。 而 t2.id 的结果建立在 t2.id=t3.id 的基础之上。 b)、id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行。 c)、id相同不同，同时存在。 id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行。 衍生表 = derived2 --&gt; derived + 2 （2 表示由 id =2 的查询衍生出来的表。type 肯定是 all ，因为衍生的表没有建立索引） 3)、select_type 4)、type 5)、possible_keys和key possible_keys : 显示可能应用在这张表中的索引，一个或多个。 查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用。 key: 实际使用的索引。如果为NULL，则没有使用索引； 查询中若使用了覆盖索引，则该索引和查询的select字段重叠； 覆盖索引: 如果一个索引包含 (或者说覆盖) 所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在InnoDB存情引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要&quot;回表&quot;，也就是要通过主键再查找一次。这样就会比较慢。 覆盖索引就是把要查询出的列和索引是对应的，不做回表操作! 现在我创建了索引(username,age)，在查询数据的时候: select username , age fromuser where username = Java' and age = 22。要查词出的列在叶子节点都存在! 所以就不要回表。 6)、key_len、ref、rows key_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。 在不损失精确性的情况下，长度越短越好。 key_len字段能够帮你检查是否充分的利用上了索引。 具体使用到了多少个列的索引，这里就会计算进去，没有使用到的列，这里不会计算进去。留意下这个列的值，算一下你的多列索引总长度就知道有没有使用到所有的列了。 ref: 显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值； rows: rows列显示MySQL认为它执行查询时必须检查的行数。 越少越好； 7)、Extra 8)、检测 答案: 6、SQL优化实战 1)、实战一-单表 建表SQL: 123456789101112131415CREATE TABLE IF NOT EXISTS &#96;article&#96;(&#96;id&#96; INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT,&#96;author_id&#96; INT(10) UNSIGNED NOT NULL,&#96;category_id&#96; INT(10) UNSIGNED NOT NULL,&#96;views&#96; INT(10) UNSIGNED NOT NULL,&#96;comments&#96; INT(10) UNSIGNED NOT NULL,&#96;title&#96; VARBINARY(255) NOT NULL,&#96;content&#96; TEXT NOT NULL);INSERT INTO &#96;article&#96; (author_id,category_id,views,comments,title,content) VALUES(1,1,1,1,1,1),(2,2,2,2,2,2),(1,1,3,3,3,3); 表中内容: 实战一: 查询 categoryid 为1 且 comments 大于 1 的情况下，views 最多的文章。 完整代码: 1234567891011121314151617181920212223mysql&gt; select id, author_id from article where category_id &#x3D; 1 AND comments &gt; 1 ORDER BY views DESC LIMIT 1;+----+-----------+| id | author_id |+----+-----------+| 3 | 1 |+----+-----------+1 row in set (0.01 sec)mysql&gt; explain select id, author_id from article where category_id &#x3D; 1 AND comments &gt; 1 ORDER BY views DESC LIMIT 1\\G^[[A*************************** 1. row *************************** id: 1 select_type: SIMPLE table: article partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 3 filtered: 33.33 Extra: Using where; Using filesort1 row in set, 1 warning (0.00 sec) 第一版优化，建立索引: 代码: 12345678910111213141516171819mysql&gt; create index idx_article_ccv on article(category_id, comments, views);Query OK, 0 rows affected (0.20 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; explain select id, author_id from article where category_id &#x3D; 1 AND comments &gt; 1 ORDER BY views DESC LIMIT 1\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: article partitions: NULL type: rangepossible_keys: idx_article_ccv key: idx_article_ccv key_len: 8 ref: NULL rows: 1 filtered: 100.00 Extra: Using index condition; Using filesort1 row in set, 1 warning (0.02 sec) 结论: type 变成了 range,这是可以忍受的。但是 extra 里使用 Using filesort 仍是无法接受的。 但是我们已经建立了索引为啥没用呢? 这是因为按照 BTree 索引的工作原理: 先排序 category_id， 如果遇到相同的 category_id 则再排序 comments,如果遇到相同的 comments 则再排序 views。当 comments 字段在联合素引里处于中间位置时，因comments &gt; 1 条件是一个范围值(所谓 range)， MySQL 无法利用索引再对后面的 views 部分进行检索,即 range 类型查询字段后面的索引无效。 第二版: 先删除上面那个不是很好的索引，然后只建立(category_id, views)之间的索引，而没有comments: 1234567891011121314151617181920212223mysql&gt; drop index idx_article_ccv on article;Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; create index article_cv on article(category_id, views);Query OK, 0 rows affected (0.08 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; explain select id, author_id from article where category_id &#x3D; 1 AND comments &gt; 1 ORDER BY views DESC LIMIT 1\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: article partitions: NULL type: refpossible_keys: article_cv key: article_cv key_len: 4 ref: const rows: 2 filtered: 33.33 Extra: Using where1 row in set, 1 warning (0.00 sec) 结论: 可以看到type变成了ref，Extra中的Using fileSort也消失了，结果非常理想。 2)、实战二-双表 两个表: 使用 12345678mysql&gt; EXPLAIN SELECT * FROM class LEFT JOIN book ON class.card &#x3D; book.card;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+| 1 | SIMPLE | class | NULL | ALL | NULL | NULL | NULL | NULL | 20 | 100.00 | NULL || 1 | SIMPLE | book | NULL | ALL | NULL | NULL | NULL | NULL | 20 | 100.00 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+2 rows in set, 1 warning (0.04 sec) 结论：type 有All，不是很好。 可以看到第二行的 type 变为了 ref,rows 也变成了优化比较明显。 这是由左连接特性决定的。LEFT JOIN 条件用于确定如何从右表搜索行,左边一定都有,所以右边是我们的关键点,一定需要建立索引。(如果将索引建立在左边，不会有这么好)。 123456789101112mysql&gt; ALTER TABLE &#96;book&#96; ADD INDEX Y ( &#96;card&#96;);Query OK, 0 rows affected (0.12 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; EXPLAIN SELECT * FROM class LEFT JOIN book ON class.card &#x3D; book.card;+----+-------------+-------+------------+------+---------------+------+---------+--------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+--------------------+------+----------+-------------+| 1 | SIMPLE | class | NULL | ALL | NULL | NULL | NULL | NULL | 20 | 100.00 | NULL || 1 | SIMPLE | book | NULL | ref | Y | Y | 4 | mysqlad.class.card | 1 | 100.00 | Using index |+----+-------------+-------+------------+------+---------------+------+---------+--------------------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec) 上面的索引建立在右边的表(book)。下面如果我们建立在class表，并使用左连接，就不会有这么好的效果，如下: 1234567891011121314151617mysql&gt; DROP INDEX Y ON book;Query OK, 0 rows affected (0.05 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; ALTER TABLE class ADD INDEX X (card);Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; mysql&gt; EXPLAIN SELECT * FROM class LEFT JOIN book ON class.card &#x3D; book.card;+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+----------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+----------------------------------------------------+| 1 | SIMPLE | class | NULL | index | NULL | X | 4 | NULL | 20 | 100.00 | Using index || 1 | SIMPLE | book | NULL | ALL | NULL | NULL | NULL | NULL | 20 | 100.00 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+----------------------------------------------------+2 rows in set, 1 warning (0.00 sec) 所以总结: 1、保证被驱动表的join字段已经被索引。被驱动表 join 后的表为被驱动表 (需要被查询)； 2、left join 时，选择小表作为驱动表，大表作为被驱动表(建立索引的表)。但是 left join 时一定是左边是驱动表，右边是被驱动表。 3、inner join 时，mysql会自己帮你把小结果集的表选为驱动表。 4、子查询尽量不要放在被驱动表，有可能使用不到索引。 3)、实战三-三表 建立索引后的查询: 1234567891011121314151617mysql&gt; alter table phone add index z(card);Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; alter table book add index y(card);Query OK, 0 rows affected (0.06 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; explain select * from class left join book on class.card&#x3D;book.card left join phone on book.card&#x3D;phone.card;+----+-------------+-------+------------+------+---------------+------+---------+--------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+--------------------+------+----------+-------------+| 1 | SIMPLE | class | NULL | ALL | NULL | NULL | NULL | NULL | 20 | 100.00 | NULL || 1 | SIMPLE | book | NULL | ref | y | y | 4 | mysqlad.class.card | 1 | 100.00 | Using index || 1 | SIMPLE | phone | NULL | ref | z | z | 4 | mysqlad.book.card | 1 | 100.00 | Using index |+----+-------------+-------+------------+------+---------------+------+---------+--------------------+------+----------+-------------+3 rows in set, 1 warning (0.00 sec) 结论: 后2行的type都是ref且总rows优化很好，效果不错，因此索引最好设置在需要经常查询的字段中。 相关建索引建议: 1、保证被驱动表的join字段已经被索引； 2、left join 时，选择小表作为驱动表，大表作为被驱动表； 3、inner join 时，mysql会自己帮你把小结果集的表选为驱动表； 4、子查询尽量不要放在被驱动表，有可能使用不到索引； 7、索引失效(应该避免) 表: 建表语句: 1234567891011121314CREATE TABLE staffs ( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR (24) NULL DEFAULT &#39;&#39; COMMENT &#39;姓名&#39;, age INT NOT NULL DEFAULT 0 COMMENT &#39;年龄&#39;, pos VARCHAR (20) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;职位&#39;, add_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;入职时间&#39;) CHARSET utf8 COMMENT &#39;员工记录表&#39; ;INSERT INTO staffs(NAME,age,pos,add_time) VALUES(&#39;z3&#39;,22,&#39;manager&#39;,NOW());INSERT INTO staffs(NAME,age,pos,add_time) VALUES(&#39;July&#39;,23,&#39;dev&#39;,NOW());INSERT INTO staffs(NAME,age,pos,add_time) VALUES(&#39;2000&#39;,23,&#39;dev&#39;,NOW());INSERT INTO staffs(NAME,age,pos,add_time) VALUES(null,23,&#39;dev&#39;,NOW());ALTER TABLE staffs ADD INDEX idx_staffs_nameAgePos(name, age, pos); 1)、全值匹配我以及最佳前缀匹配 索引 idx_staffs_nameAgePos 建立索引时 以 name ， age ，pos 的顺序建立的。全值匹配表示 按顺序匹配的 12345EXPLAIN SELECT * FROM staffs WHERE NAME &#x3D; &#39;July&#39;;EXPLAIN SELECT * FROM staffs WHERE NAME &#x3D; &#39;July&#39; AND age &#x3D; 25;EXPLAIN SELECT * FROM staffs WHERE NAME &#x3D; &#39;July&#39; AND age &#x3D; 25 AND pos &#x3D; &#39;dev&#39;; 结果: 但是如果我们只有age和pos或者只有pos， 查询结果就会很差，所以这就是最佳左前缀法则。 123456789101112131415mysql&gt; explain select * from staffs where age&#x3D;25 and pos&#x3D;&#39;dev&#39;;+----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | staffs | NULL | ALL | NULL | NULL | NULL | NULL | 4 | 25.00 | Using where |+----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select * from staffs where pos&#x3D;&#39;dev&#39;;+----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | staffs | NULL | ALL | NULL | NULL | NULL | NULL | 4 | 25.00 | Using where |+----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。 再看中间断的情况: 1234567mysql&gt; explain select * from staffs where name&#x3D;&#39;July&#39; and pos&#x3D;&#39;dev&#39;;+----+-------------+--------+------------+------+-----------------------+-----------------------+---------+-------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+-----------------------+-----------------------+---------+-------+------+----------+-----------------------+| 1 | SIMPLE | staffs | NULL | ref | idx_staffs_nameAgePos | idx_staffs_nameAgePos | 75 | const | 1 | 25.00 | Using index condition |+----+-------------+--------+------------+------+-----------------------+-----------------------+---------+-------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec) 结论: 只用到了第一个。中间断了。 2)、不在索引列上做任何操作 不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描。 下面在name使用了left()函数，就会失效。 3)、存储引擎不能使用索引中范围条件右边的列 如果中间出现了范围的，就会变成range。后面就会失效: 4)、尽量使用覆盖索引(只访问索引的查询(索引列和查询列一致))，减少select * 不用select *，而是select具体的字段。 5)、使用不等于(!=或者&lt;&gt;)的时候无法使用索引 但是如果业务需要必须要写的话，那也没办法。 6)、like以通配符开头(’%abc…’)mysql索引失效会变成全表扫描(最好在右边写%) 问题：解决like '%字符串%'时索引不被使用的方法？？ 答: 使用覆盖索引。 1234567891011121314151617181920#before index# 第一批 (建最下面的索引后可以被优化)EXPLAIN SELECT NAME,age FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;EXPLAIN SELECT id FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;EXPLAIN SELECT NAME FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;EXPLAIN SELECT age FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;EXPLAIN SELECT id,NAME FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;EXPLAIN SELECT id,NAME,age FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;EXPLAIN SELECT NAME,age FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;# 第二批: 搅屎棍EXPLAIN SELECT * FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;EXPLAIN SELECT id,NAME,age,email FROM tbl_user WHERE NAME LIKE &#39;%aa%&#39;;#create index (上面的字符第一批在键了下面的索引后会优化，但是第二批搅屎棍不会,因为覆盖不了)# 为啥第一批的id也能优化，因为Extra中的 Using Index (主键本身也是索引)CREATE INDEX idx_user_nameAge ON tbl_user(NAME,age); 7)、字符串不加单引号索引失效(发生了类型转换) 8)、总结和练习 索引建议总结: 1、对于单键索引，尽量选择针对当前query过滤性更好的索引； 2、在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。(避免索引过滤性好的索引失效)； 3、在选择组合索引的时候，尽量选择可以能够包含当前query中的where字句中更多字段的索引； 4、尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的； 再来一波练习: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138mysql&gt; select * from test03; # 表+----+------+------+------+------+------+| id | c1 | c2 | c3 | c4 | c5 |+----+------+------+------+------+------+| 1 | a1 | a2 | a3 | a4 | a5 || 2 | b1 | b2 | b3 | b4 | b5 || 3 | c1 | c2 | c3 | c4 | c5 || 4 | d1 | d2 | d3 | d4 | d5 || 5 | e1 | e2 | e3 | e4 | e5 |+----+------+------+------+------+------+5 rows in set (0.02 sec)mysql&gt; create index idx_test03_c1234 on test03(c1,c2,c3,c4); # 创建索引Query OK, 0 rows affected (0.12 sec)Records: 0 Duplicates: 0 Warnings: 0# 1、全值匹配我最爱mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; and c3&#x3D;&#39;a3&#39; and c4&#x3D;&#39;a4&#39;; +----+-------------+--------+------------+------+------------------+------------------+---------+-------------------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------------------+------+----------+-------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 124 | const,const,const,const | 1 | 100.00 | NULL |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------------------+------+----------+-------+1 row in set, 1 warning (0.31 sec)# 2、这种情况Mysql会底层会帮我们自动优化mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; and c4&#x3D;&#39;a4&#39; and c3&#x3D;&#39;a3&#39;; +----+-------------+--------+------------+------+------------------+------------------+---------+-------------------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------------------+------+----------+-------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 124 | const,const,const,const | 1 | 100.00 | NULL |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------------------+------+----------+-------+1 row in set, 1 warning (0.00 sec)# 3、 中间阶段 -&gt; range mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; and c3&gt;&#39;a3&#39; and c4&#x3D;&#39;a4&#39;;+----+-------------+--------+------------+-------+------------------+------------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+-------+------------------+------------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | test03 | NULL | range | idx_test03_c1234 | idx_test03_c1234 | 93 | NULL | 1 | 20.00 | Using index condition |+----+-------------+--------+------------+-------+------------------+------------------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec)# 4、这个比上面那个好，多用了一个（key_len会大一点）,因为Mysql底层会调优将c4&gt;&#39;a4&#39;放在后面mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; and c4&gt;&#39;a4&#39; and c3&#x3D;&#39;a3&#39;;+----+-------------+--------+------------+-------+------------------+------------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+-------+------------------+------------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | test03 | NULL | range | idx_test03_c1234 | idx_test03_c1234 | 124 | NULL | 1 | 100.00 | Using index condition |+----+-------------+--------+------------+-------+------------------+------------------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec)# 5、注意 : c3作用在排序而不是查找mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; and c4&#x3D;&#39;a4&#39; order by c3;+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 62 | const,const | 1 | 20.00 | Using index condition |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+1 row in set, 1 warning (0.01 sec)# 6、 和5一模一样mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; order by c3;+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 62 | const,const | 1 | 100.00 | Using index condition |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec)# 6、出现了Using filesortmysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; order by c4; +----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+---------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+---------------------------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 62 | const,const | 1 | 100.00 | Using index condition; Using filesort |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+---------------------------------------+1 row in set, 1 warning (0.00 sec)# 8.1、 只用c1一个字段索引，但是c2、c3用于排序,所有没有 filesortmysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c5&#x3D;&#39;a5&#39; order by c2,c3; +----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+------------------------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 31 | const | 1 | 20.00 | Using index condition; Using where |+----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+------------------------------------+1 row in set, 1 warning (0.00 sec)# 8.2、 出现了filesort，我们建的索引是1234，它没有按照顺序来，3,2 颠倒了mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c5&#x3D;&#39;a5&#39; order by c3,c2;+----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+----------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+----------------------------------------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 31 | const | 1 | 20.00 | Using index condition; Using where; Using filesort |+----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+----------------------------------------------------+1 row in set, 1 warning (0.00 sec)# 9、mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; order by c2,c3;+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 62 | const,const | 1 | 100.00 | Using index condition |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec)# 10.1、 和c5这个坑爹货没关系mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; and c5&#x3D;&#39;a5&#39; order by c2,c3;+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+------------------------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 62 | const,const | 1 | 20.00 | Using index condition; Using where |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+------------------------------------+1 row in set, 1 warning (0.00 sec)# 10.2、 这里排序字段已经是一个常量 和8.2不同 mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c2&#x3D;&#39;a2&#39; and c5&#x3D;&#39;a5&#39; order by c3,c2; +----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+------------------------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 62 | const,const | 1 | 20.00 | Using index condition; Using where |+----+-------------+--------+------------+------+------------------+------------------+---------+-------------+------+----------+------------------------------------+1 row in set, 1 warning (0.00 sec)# 本例有常量c2的情况，和8.2对比 filesort (下面是8.2的)mysql&gt; explain select * from test03 where c1&#x3D;&#39;a1&#39; and c5&#x3D;&#39;a5&#39; order by c3,c2; +----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+----------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+----------------------------------------------------+| 1 | SIMPLE | test03 | NULL | ref | idx_test03_c1234 | idx_test03_c1234 | 31 | const | 1 | 20.00 | Using index condition; Using where; Using filesort |+----+-------------+--------+------------+------+------------------+------------------+---------+-------+------+----------+----------------------------------------------------+1 row in set, 1 warning (0.00 sec)# 11、group 虽然是分组，但是分组之前必排序 (可能导致临时表) explain select * from test03 where c1&#x3D;&#39;a1&#39; and c4&#x3D;&#39;a4&#39; group by c2,c3; # # 12、灭绝师太!!! Using temporary; Using filesort explain select * from test03 where c1&#x3D;&#39;a1&#39; and c4&#x3D;&#39;a4&#39; group by c3,c2; 8、order by关键字排序优化 主要讨论order by会不会产生fileSort。 MySQL支持二种方式的排序，FileSort和Index，Index效率高。它指MySQL扫描索引本身完成排序。FileSort方式效率较低。 ORDER BY满足两情况，会使用Index方式排序: ORDER BY 语句使用索引最左前列； 使用Where子句与Order BY子句条件列组合满足索引最左前列； where子句中如果出现索引的范围查询(即explain中出现range)会导致order by 索引失效。 ORDER BY子句，尽量使用Index方式排序,避免使用FileSort方式排序 测试: 键表语句： 1234567891011121314151617181920212223CREATE TABLE tblA( id int primary key not null auto_increment, age INT, birth TIMESTAMP NOT NULL, name varchar(200));INSERT INTO tblA(age,birth,name) VALUES(22,NOW(),&#39;abc&#39;);INSERT INTO tblA(age,birth,name) VALUES(23,NOW(),&#39;bcd&#39;);INSERT INTO tblA(age,birth,name) VALUES(24,NOW(),&#39;def&#39;);CREATE INDEX idx_A_ageBirth ON tblA(age,birth,name);表的内容mysql&gt; select * from tblA;+----+------+---------------------+------+| id | age | birth | name |+----+------+---------------------+------+| 1 | 22 | 2019-03-21 19:10:29 | abc || 2 | 23 | 2019-03-21 19:10:29 | bcd || 3 | 24 | 2019-03-21 19:10:29 | def |+----+------+---------------------+------+3 rows in set (0.00 sec) 注意我们建立的索引是(age, birth, name)。 然后看下面的查询，当我们order by birth或者order by birth,age的时候，就会出现Using fileSort: 1234567891011121314151617181920212223242526272829303132333435# 1、没有产生Using FileSortmysql&gt; explain select * from tblA where age&gt;20 order by age;+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | tblA | NULL | index | idx_A_ageBirth | idx_A_ageBirth | 612 | NULL | 3 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec)# 2、没有产生Using FileSortmysql&gt; explain select * from tblA where age&gt;20 order by age, birth;+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | tblA | NULL | index | idx_A_ageBirth | idx_A_ageBirth | 612 | NULL | 3 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec)# 3、产生了Using FileSortmysql&gt; explain select * from tblA where age&gt;20 order by birth;+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------------+| 1 | SIMPLE | tblA | NULL | index | idx_A_ageBirth | idx_A_ageBirth | 612 | NULL | 3 | 100.00 | Using where; Using index; Using filesort |+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------------+1 row in set, 1 warning (0.00 sec)# 4、产生了Using FileSortmysql&gt; explain select * from tblA where age&gt;20 order by birth, age;+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------------+| 1 | SIMPLE | tblA | NULL | index | idx_A_ageBirth | idx_A_ageBirth | 612 | NULL | 3 | 100.00 | Using where; Using index; Using filesort |+----+-------------+-------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------------+1 row in set, 1 warning (0.00 sec) 还要注意一个ASC和DESC的问题: 提高Order by速度: 1)、Order by时select * 是一个大忌只Query需要的字段， 这点非常重要。在这里的影响是： 当Query的字段大小总和小于max_length_for_sort_data 而且排序字段不是 TEXT|BLOB 类型时，会用改进后的算法——单路排序， 否则用老算法——多路排序。 两种算法的数据都有可能超出sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次I/O，但是用单路排序算法的风险会更大一些,所以要提高sort_buffer_size。 2)、 尝试提高sort_buffer_size 不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程的 3)、尝试提高 max_length_for_sort_data 提高这个参数， 会增加用改进算法的概率。但是如果设的太高，数据总容量超出sort_buffer_size的概率就增大，明显症状是高的磁盘I/O活动和低的处理器使用率. 总结: 尽可能在索引列上完成排序操作，遵照索引建的最佳左前缀。 第二种中，where a = const and b &gt; const order by b , c 不会出现 using filesort b , c 两个衔接上了 但是：where a = const and b &gt; const order by c将会出现 using filesort 。因为 b 用了范围索引，断了。而上一个 order by 后的b 用到了索引，所以能衔接上 c 。 9、B+Tree与B-Tree 的区别 结论在内存有限的情况下，B+TREE 永远比 B-TREE好。无限内存则后者方便。 1)、B-树的关键字和记录是放在一起的，叶子节点可以看作外部节点，不包含任何信息；B+树叶子节点中只有关键字和指向下一个节点的索引，记录只放在叶子节点中。(一次查询可能进行两次i/o操作) 2)、在B-树中，越靠近根节点的记录查找时间越快，只要找到关键字即可确定记录的存在；而B+树中每个记录的查找时间基本是一样的，都需要从根节点走到叶子节点，而且在叶子节点中还要再比较关键字。从这个角度看B-树的性能好像要比B+树好，而在实际应用中却是B+树的性能要好些。因为B+树的非叶子节点不存放实际的数据，这样每个节点可容纳的元素个数比B-树多，树高比B-树小，这样带来的好处是减少磁盘访问次数。尽管B+树找到一个记录所需的比较次数要比B-树多，但是一次磁盘访问的时间相当于成百上千次内存比较的时间，因此实际中B+树的性能可能还会好些，而且B+树的叶子节点使用指针连接在一起，方便顺序遍历（例如查看一个目录下的所有文件，一个表中的所有记录等），这也是很多数据库和文件系统使用B+树的缘故。 思考：为什么说B+树比B-树更适合实际应用中操作系统的文件索引和数据库索引？ B+树的磁盘读写代价更低 B+树的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B 树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。 B+树的查询效率更加稳定 由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 索引建立成哪种索引类型？ 根据数据引擎类型自动选择的索引类型 除开 innodb 引擎主键默认为聚簇索引 外。 Innodb的索引都采用的 B+TREE。 MyIsam 则都采用的 B-TREE索引。 10、聚簇索引和非聚簇索引 聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。 术语‘聚簇’表示数据行和相邻的键值进错的存储在一起。 如下图，左侧的索引就是聚簇索引，因为数据行在磁盘的排列和索引排序保持一致。 聚簇索引优点 : 按照聚簇索引排列顺序，查询显示一定范围数据的时候，由于数据都是紧密相连，数据库不用从多个数据块中提取数据，所以节省了大量的io操作。 聚簇索引限制 : 对于mysql数据库目前只有innodb数据引擎支持聚簇索引，而MyIsam并不支持聚簇索引。 由于数据物理存储排序方式只能有一种，所以每个Mysql的表只能有一个聚簇索引。一般情况下就是该表的主键。 为了充分利用聚簇索引的聚簇的特性，所以innodb表的主键列尽量选用有序的顺序id，而不建议用无序的id，比如uuid这种。（参考聚簇索引优点。） 11、全文索引、Hash索引 全文索引 MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。 查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。 12345不同于like方式的的查询：SELECT * FROM article WHERE content LIKE ‘%查询字符串%’;全文索引用match+against方式查询：(明显的提高查询效率。)SELECT * FROM article WHERE MATCH(title,content) AGAINST (‘查询字符串’); Hash索引 哈希索引能以 O(1) 时间进行查找，但是失去了有序性： 无法用于排序与分组； 只支持精确查找，无法用于部分查找和范围查找。 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 12、查询截取分析 优化SQL步骤: 1)、观察，至少跑一天，看看生产的慢SQL情况； 2)、开启慢查询日志，设置阙值，比如超过5秒钟的就是慢SQL，并将它抓取出来； 3)、explain+ 慢SQL分析； 4)、show profile； 即： 1)、慢查询的开启并捕获； 2)、explain+慢SQL分析； 3)、show profile查询SQL在MYSQL服务器里面的执行细节和生命周期情况； 4)、SQL数据库服务器的参数调优； 原则: 小表驱动大表。 GROUP BY关键字优化: group by实质是先排序后进行分组，遵照索引建的最佳左前缀； 当无法使用索引列，增大max_length_for_sort_data参数的设置+增大sort_buffer_size参数的设置； where高于having，能写在where限定的条件就不要去having限定了；","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"Mysql事务和隔离级别","slug":"Mysql/Mysql事务和隔离级别","date":"2020-05-20T08:25:34.582Z","updated":"2020-05-20T08:25:34.582Z","comments":true,"path":"2020/05/20/Mysql/Mysql事务和隔离级别/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E4%BA%8B%E5%8A%A1%E5%92%8C%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","excerpt":"","text":"一、事务 事务是由一组SQL语句组成的逻辑处理单元，是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。事务具有以下4个属性，通常简称为事务的ACID属性: 原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。比如在同一个事务中的SQL语句，要么全部执行成功，要么全部执行失败。回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 以转账为例子，A向B转账，假设转账之前这两个用户的钱加起来总共是2000，那么A向B转账之后，不管这两个账户怎么转，A用户的钱和B用户的钱加起来的总额还是2000，这个就是事务的一致性。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。 MySQL 默认采用自动提交模式。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。 这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库奔溃的情况。 二、并发一致性问题 1、更新丢失(Lost Update) T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 例如，两个程序员修改同一java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。 如果在一个程序员完成并提交事务之前，另一个程序员不能访问同一文件，则可避免此问题。 2、脏读 一句话：事务B读取到了事务A已修改但尚未提交的的数据，还在这个数据基础上做了操作。此时，如果A事务回滚Rollback，B读取的数据无效，不符合一致性要求。 解决办法: 把数据库的事务隔离级别调整到 READ_COMMITTED T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 3、不可重复读(Non-Repeatable Reads) 在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 一句话：一个事务范围内两个相同的查询却返回了不同数据。 同时操作，事务1分别读取事务2操作时和提交后的数据，读取的记录内容不一致。不可重复读是指在同一个事务内，两个相同的查询返回了不同的结果。 解决办法: 如果只有在修改事务完全提交之后才可以读取数据，则可以避免该问题。把数据库的事务隔离级别调整到REPEATABLE_READ T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 4、幻读 一个事务T1按相同的查询条件重新读取以前检索过的数据，却发现其他事务T2插入了满足其查询条件的新数据，这种现象就称为“幻读”。（和可重复读类似，但是事务 T2 的数据操作仅仅是插入和删除，不是修改数据，读取的记录数量前后不一致） 一句话：事务A 读取到了事务B提交的新增数据，不符合隔离性。 解决办法: 如果在操作事务完成数据处理之前，任何其他事务都不可以添加新数据，则可避免该问题。把数据库的事务隔离级别调整到 SERIALIZABLE_READ。 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 三、事务隔离级别 “脏读”、“不可重复读&quot;和&quot;幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。 MYSQL常看当前数据库的事务隔离级别：show variables like 'tx_isolation'; 1、读未提交 (Read Uncommitted) 最低的隔离等级，允许其他事务看到没有提交的数据，会导致脏读。 2、读已提交 (Read Committed) 被读取的数据可以被其他事务修改，这样可能导致不可重复读。也就是说，事务读取的时候获取读锁，但是在读完之后立即释放(不需要等事务结束)，而写锁则是事务提交之后才释放，释放读锁之后，就可能被其他事务修改数据。该等级也是 SQL Server 默认的隔离等级。 3、可重复读(Repeatable Read) 所有被 Select 获取的数据都不能被修改，这样就可以避免一个事务前后读取数据不一致的情况。但是却没有办法控制幻读，因为这个时候其他事务不能更改所选的数据，但是可以增加数据，即前一个事务有读锁但是没有范围锁，为什么叫做可重复读等级呢？那是因为该等级解决了下面的不可重复读问题。(引申：现在主流数据库都使用 MVCC 并发控制，使用之后RR（可重复读）隔离级别下是不会出现幻读的现象。) MYSQL默认是REPEATABLE-READ。 4、串行化(Serializable) 所有事务一个接着一个的执行，这样可以避免幻读 (phantom read)，对于基于锁来实现并发控制的数据库来说，串行化要求在执行范围查询的时候，需要获取范围锁，如果不是基于锁实现并发控制的数据库，则检查到有违反串行操作的事务时，需回滚该事务。 5、总结 读未提交: 一个事务还没提交时，它做的变更就能被别的事务看到。 读提交: 一个事务提交之后，它做的变更才会被其他事务看到。 可重复读 : 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化: 顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 四个级别逐渐增强，每个级别解决一个问题，事务级别越高，性能越差，大多数环境(Read committed 就可以用了) 隔离级别 读数据一致性 脏读 不可重复读 幻影读 未提交读 最低级别 √ √ √ 提交读 语句级 × √ √ 可重复读 事务级 × × √ 可串行化 最高级别,事务级 × × ×","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"Mysql主从复制","slug":"Mysql/Mysql主从复制","date":"2020-05-20T08:25:25.735Z","updated":"2020-05-20T08:25:25.735Z","comments":true,"path":"2020/05/20/Mysql/Mysql主从复制/","link":"","permalink":"http://blog.tgyf.com/2020/05/20/Mysql/Mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","excerpt":"","text":"一、基本原理 MySQL复制过程分成三步： 1)、master将改变记录到二进制日志（binary log）。这些记录过程叫做二进制日志事件，binary log events； 2)、slave将master的binary log events拷贝到它的中继日志（relay log）； 3)、slave重做中继日志中的事件，将改变应用到自己的数据库中。 MySQL复制是异步的且串行化的。 简单来说: slave会从master读取binlog来进行数据同步 Mysql的复制（replication）是一个异步的复制。 实现整个复制操作主要由三个进程完成的，其中两个进程在Slave（Sql进程和IO进程），另外一个进程在 Master（IO进程）上。 要实施复制，首先必须打开Master端的binary log（bin-log）功能，否则无法实现。 因为整个复制过程实际上就是Slave从Master端获取该日志然后再在自己身上完全顺序的执行日志中所记录的各种操作。 复制的详细过程： （1）Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容； （2）Master接收到来自Slave的IO进程的请求后，通过负责复制的IO进程根据请求信息读取制定日志指定位置之后的日志信息，返回给Slave 的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置； （3）Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的 bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的高速Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”； （4）Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行。 原则: 每个slave只有一个master； 每个slave只能有一个唯一的服务器ID； 每个master可以有多个salve； 二、一主一从相关配置 演示主机为Windows (配置文件为my.ini文件)，从机为Linux（配置文件为my.cnf） 1、主机配置(windows的my.ini) 1)、[必须]主服务器唯一ID； ２)、[必须]启用二进制日志； log-bin=自己本地的路径/data/mysqlbin。 log-bin=D:/devSoft/MySQLServer5.5/data/mysqlbin。 3)、[可选]启用错误日志 log-err=自己本地的路径/data/mysqlerr。 log-err=D:/devSoft/MySQLServer5.5/data/mysqlerr。 4)、[可选]根目录 basedir=&quot;自己本地路径&quot;。 basedir=&quot;D:/devSoft/MySQLServer5.5/&quot;。 5)、[可选]临时目录 tmpdir=&quot;自己本地路径&quot;。 tmpdir=&quot;D:/devSoft/MySQLServer5.5/&quot;。 6)、[可选]数据目录 datadir=&quot;自己本地路径/Data/&quot;。 datadir=&quot;D:/devSoft/MySQLServer5.5/Data/&quot;。 7)、[可选]设置不要复制的数据库 binlog-ignore-db=mysql。 8)、[可选]设置需要复制的数据库 binlog-do-db=需要复制的主数据库名字。 2、从机配置(linux的my.cnf) [必须]从服务器唯一ID； [可选]启用二进制日志； 3、因修改过配置文件，请主机+从机都重启后台mysql服务 4、主从机都关闭linux防火墙 windows手动关闭； 关闭虚拟机linux防火墙 service iptables stop； 5、在Windows主机上建立帐户并授权slave GRANT REPLICATION SLAVE ON *.* TO 'zhangsan'@'从机器数据库IP' IDENTIFIED BY '123456';。 刷新一下配置flush privileges;。 查询master的状态。 show master status; 记录下File和Position的值； 执行完此步骤后不要再操作主服务器MYSQL，防止主服务器状态值变化。 6、在Linux从机上配置需要复制的主机 配置 1CHANGE MASTER TO MASTER_HOST&#x3D;&#39;主机IP&#39;,MASTER_USER&#x3D;&#39;zhangsan&#39;,MASTER_PASSWORD&#x3D;&#39;123456&#39;,MASTER_LOG_FILE&#x3D;&#39;File名字&#39;,MASTER_LOG_POS&#x3D;Position数字; 启动从服务器复制功能，start slave;。 查看配置 下面两个参数都是Yes，则说明主从配置成功！ Slave_IO_Running: Yes。 Slave_SQL_Running: Yes。 1234CHANGE MASTER TO MASTER_HOST&#x3D;&#39;192.168.124.3&#39;,MASTER_USER&#x3D;&#39;zhangsan&#39;,MASTER_PASSWORD&#x3D;&#39;123456&#39;,MASTER_LOG_FILE&#x3D;&#39;mysqlbin.具体数字&#39;,MASTER_LOG_POS&#x3D;具体值; 7、主机键表，看从机有没有 8、如何停止主从服务复制功能 在linux下面输入stop slave；。","categories":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://blog.tgyf.com/tags/Database/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.tgyf.com/tags/Mysql/"}]},{"title":"JVM类加载机制","slug":"Java虚拟机/JVM类加载机制","date":"2020-03-05T03:39:58.385Z","updated":"2020-03-05T03:39:58.385Z","comments":true,"path":"2020/03/05/Java虚拟机/JVM类加载机制/","link":"","permalink":"http://blog.tgyf.com/2020/03/05/Java%E8%99%9A%E6%8B%9F%E6%9C%BA/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/","excerpt":"","text":"一张总结图（来自牛客网） 一、类加载时机 类从被加载到虚拟机内存中开始，到出内存为止，它的整个生命周期包括 : 加载 (Loading)、验 证(Verification)、准 备 (Preparation)、解 析 (Resolution)、初始化(Initialization)、使用(Using) 和卸载 (Unloading) 7 个阶段。其中验证、准备、解析 3 个部分统称为连接 (Linking)，顺序如图 所示。 其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定。 这 7 个阶段中的：加载、验证、准备、初始化、卸载的顺序是固定的。但它们并不一定是严格同步串行执行，它们之间可能会有交叉，但总是以 “开始” 的顺序总是按部就班的。至于解析则有可能在初始化之后才开始，这是为了支持 Java 语言的运行时绑定（也称为动态绑定或晚期绑定）。 Java程序对类的使用方式可以分为两种: 主动引用、被动引用。 所有的Java虚拟机实现必须在每个类或接口被Java程序 &quot;首次主动使用&quot;时才初始化他们。 1、主动引用 虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列五种情况必须对类进行初始化（加载、验证、准备都会随之发生）： 遇到new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是： 使用 new 关键字实例化对象的时候； 读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候； 以及调用一个类的静态方法的时候； 通过反射(Class.forName())使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化； 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化； 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main()方法的那个类），虚拟机会先初始化这个主类； 当使用 JDK 1.7 的动态语言支持时，如果一个java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化； 2、被动引用 被动使用不会导致类的初始化，只有首次主动引用才会初始化。 以下三种情况是被动引用: 1、通过子类引用父类的静态字段，不会导致子类初始化。 测试: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 1、对于静态字段来说，只有直接定义了该字段的类才会被初始化 * 2、当一个类在初始化时，要求其父类全部已经初始化完毕了 * --X:+TraceClassLoading 用于追踪类的加载信息并打印出来 * jvm参数总结: * a、 -XX:+&lt;option&gt;， 表示开启option选项 * b、 -XX:-&lt;option&gt;， 表示关闭option选项 * c、 -XX:&lt;option&gt;=&lt;value&gt;，表示option选项的值设置为value */public class T1 &#123; public static void main(String[] args) &#123;// System.out.println(P1.p_str); //1、对于静态字段来说，只有直接定义了该字段的类才会被初始化 System.out.println(C1.c_str); //2、当一个类在初始化时，要求其父类全部已经初始化完毕了 &#125;&#125;class P1&#123; public static String p_str = \"parent str\"; static &#123; System.out.println(\"Parent static block~\"); &#125;&#125;class C1 extends P1&#123; public static String c_str = \"child str\"; static &#123; System.out.println(\"Child static block~\"); &#125;&#125;------------------------------------分析运行结果:第一种情况: System.out.println(P1.p_str);输出: Parent static block~parent str原因: 由于p_str是父类的，所以不会去加载C1(子类)第二种情况: System.out.println(C1.c_str); 输出:Parent static block~Child static block~child str原因:因为c_str是子类的，而子类加载完毕之前父类都要加载，所以会先输出父类static块 2、通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法。 12345678910111213141516171819202122232425262728293031public class T4 &#123; public static void main(String[] args) &#123; P4 p4_1 = new P4(); System.out.println(\"=================\"); P4 p4_2 = new P4(); //这里不会再 执行P4的初始化代码块，只会在 首次主动引用 的时候初始化 P4[] p4s = new P4[1]; // 数组也不会初始化 System.out.println(p4s.getClass()); // 对于数组实例来说，其类型是由JVM在运行期间动态生成的，表示为[Lcom.zxin...T4 P4[][] p4s1 = new P4[1][1]; System.out.println(p4s1.getClass()); int[] ints = new int[1]; System.out.println(ints.getClass()); &#125;&#125;class P4&#123; static &#123; System.out.println(\"P4 static block~\"); &#125;&#125;输出:(只会进行一次初始化)P4 static block~=================class [Lp1_classloader.P4;class [[Lp1_classloader.P4;class [I 3、常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 1234567891011121314151617181920212223242526/** * final (常量) 对类加载的影响 * 常量在编译阶段会存入到调用这个常量的方法所在的类的常量池中 * 本质上，调用类并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化 * 注意: 我们指的是 将常量存放到了T2的常量池中，之后T2与P就没有任何关系了(甚至可以将P.class删除) * * 助记符: * ldc表示将int,float或是String类型的常量值从常量池中推送至栈顶 * bipush表示将单字节(-128~127)的常量值推送至栈顶 * sipush表示将int型(-32768~32767)推送至栈顶 * iconst_1表示将int型的1推动到栈顶(只有-1~5有这种) */public class T2 &#123; public static void main(String[] args) &#123; System.out.println(P2.str); &#125;&#125;class P2&#123; public static final String str = \"P str\"; static &#123; System.out.println(\"P static block~\"); &#125;&#125; 输出: 1P str 但是要注意下面的情况: 当一个常量的值并非编译期间可以确定，那么其值就不会被放到调用类的常量池中，这时在程序运行时，会导致主动使用这个常量所在的类，显然会导致这个列被初始化。 123456789101112131415161718192021222324/** * 当一个常量的值并非编译期间可以确定，那么其值就不会被放到调用类的常量池中， * 这时在程序运行时，会导致主动使用这个常量所在的类，显然会导致这个列被初始化 */public class T3 &#123; public static void main(String[] args) &#123; System.out.println(P3.str); &#125;&#125;class P3&#123; public static final String str = UUID.randomUUID().toString(); static &#123; System.out.println(\"P static block~\"); &#125;&#125;输出(可以看到P3被加载(static块被输出)):P static block~b716f2dd-5a6e-4542-bc88-0cf833216de5 当一个接口在初始化时，并不要求其父接口都完成了初始化，只有在真正用到父接口的时候，如引用接口中定义的常量时，才会初始化。 123456789101112131415161718/** * 都可以删除 * 可以删除父接口PI5的.class文件 和 子接口CI5的.class文件 */public class T5&#123; public static void main(String[] args)&#123; System.out.println(CI5.b); &#125;&#125;interface PI5&#123; public static final int a = new Random().nextInt(4);&#125;interface CI5 extends PI5&#123; public static final int b = 5;&#125; 可以发现删除PI5.class文件之后，也可以运行（如果改成类class，且不加上final关键字，就会抛出异常，因为接口就算不加final关键字，默认也会加上(接口里面的变量默认是public static final)） 但是下面的动态引用还是会需要用到父类的初始化。 123456789101112131415161718/** * 这种情况两个都不能删除 */public class T5&#123; public static void main(String[] args)&#123; System.out.println(CI5.b); &#125;&#125;interface PI5&#123; public static final int a = new Random().nextInt(4);&#125;interface CI5 extends PI5&#123; //public static final int b = 5; public static final int b = new Random().nextInt(5);&#125; 删除父接口的.class之后，再次运行，也会抛出异常。（两个都不能删除） 3、类加载顺序 类的加载由上到下进行。 注意下列代码。 1234567891011121314151617181920212223242526272829303132public class T7 &#123; public static void main(String[] args) &#123; Singleton singleton = Singleton.getInstance(); System.out.println(\"counter1 : \" + Singleton.counter1); System.out.println(\"counter2 : \" + Singleton.counter2); &#125;&#125;/** * 初始化的顺序: 从上往下 */class Singleton&#123; public static int counter1; private static Singleton singleton = new Singleton(); private Singleton()&#123; counter1++; counter2++; //一开始是1，后面又变成了0 (准备阶段的意义)// System.out.println(\"类加载初始化的时候-------counter1 : \"+ counter1 + \", counter2 : \" + counter2); &#125; public static int counter2 = 0; //由1变成0 public static Singleton getInstance()&#123; return singleton; &#125;&#125; 上述代码输出： 12counter1 : 1counter2 : 0 因为类加载从上到下，虽然在私有构造方法中counter2被赋值成了1，但是初始化后面代码的时候，又被赋值成了0。 二、类加载过程 包含了加载、验证、准备、解析和初始化这 5 个阶段（也就是类的生命周期的前5个阶段）。 1、加载 加载是类加载的一个阶段，不要混淆。 加载过程完成以下三件事： 通过一个类的全限定名来获取定义此类的二进制字节流，即将类的.class文件中的二进制数据读入到内存中(这个就是类加载器做的事情)； 将这个字节流所代表的静态存储结构转化为方法区的运行时存储结构； 在内存中生成一个代表这个类的 Class 对象(java.lang.Class)，作为方法区这个类的各种数据的访问入口； 加载源（即二进制字节流可以从以下方式中获取）： 文件：从 ZIP 包读取，这很常见，最终成为日后 JAR、EAR、WAR 格式的基础。 网络：从网络中获取，这种场景最典型的应用是 Applet。 计算生成一个二进制流：运行时计算生成，这种场景使用得最多得就是动态代理技术，在 java.lang.reflect.Proxy 中，就是用了ProxyGenerator.generateProxyClass 的代理类的二进制字节流。 由其他文件生成：由其他文件生成，典型场景是 JSP 应用，即由 JSP 文件生成对应的 Class 类。 数据库：从数据库读取，这种场景相对少见，例如有些中间件服务器（如 SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群间的分发。 2、验证 目的：确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 文件格式验证：验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理。 是否以 0xCAFEBABE 开头，前四个字节为魔数； 版本号是否合理，如：JDK1.8（52.0）、JDK1.7（51.0）； 常量池中的常量是否有不被支持的类型； 元数据验证：对字节码描述的信息进行语义分析，以保证其描述的信息符合 Java 语言规范的要求。 是否有父类-(除了Object类之外，所有的类都应该有父类)； 继承了 final 类？; 非抽象类实现了所有的抽象方法； 字节码验证（很复杂）：通过数据流和控制流分析，确保程序语义是合法、符合逻辑的。 运行检查； 栈数据类型和操作码数据参数吻合； 跳转指令指定到合理的位置；（保证不会跳转到方法体以外的字节码上） 符号引用验证：发生在虚拟机将符号引用转换为直接引用的时候，对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验。 常量池中描述类是否存在； 访问的方法或字段是否存在且有足够的权限； 3、准备 准备阶段正式为类变量分配内存并设置变量的初始值，但在到达初始化之前，类变量都没有初始化为真正的初始值。这些变量使用的内存都将在方法区中进行分配。类变量是被 static 修饰的变量，准备阶段为类变量分配内存并设置初始值，使用的是方法区的内存。 实例变量不会在这阶段分配内存，它将会在对象实例化时随着对象一起分配在堆中。注意，实例化不是类加载的一个过程，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次。 初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123，在初始化的 &lt;clinit&gt; 中才会被设置为1。 1public static int value = 123; // 只有在初始化的&lt;clinit&gt;中才会是123，在准备阶段只是0 有一个特例就是final型的，即static final型的变量会在准备的阶段就附上正确的值: 1public static final int value = 123; 4、解析 解析阶段是虚拟机将常量池的符号引用替换为直接引用的过程: 类或接口的解析 字段解析 类方法解析 接口方法解析 什么是符号引用和直接引用？ 符号引用：符号引用是一组符号来描述所引用的目标对象，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标对象并不一定已经加载到内存中。 直接引用：直接引用可以是直接指向目标对象的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机内存布局实现相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同，如果有了直接引用，那引用的目标必定已经在内存中存在。 符号引用就是字符串，这个字符串包含足够的信息，以供实际使用时可以找到相应的位置。你比如说某个方法的符号引用，如：“java/io/PrintStream.println:(Ljava/lang/String;)”。里面有类的信息，方法名，方法参数等信息。 当第一次运行时，要根据字符串的内容，到该类的方法表中搜索这个方法。运行一次之后，符号引用会被替换为直接引用，下次就不用搜索了。直接引用就是偏移量，通过偏移量虚拟机可以直接在该类的内存区域中找到方法字节码的起始位置。 5、初始化 初始化阶段才真正开始执行类中定义的 Java 程序代码。初始化阶段即虚拟机执行类构造器 &lt;clinit&gt;() 方法的过程。 在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源，即为类的静态变量赋予正确的初始值。 &lt;clinit&gt;() 方法具有以下特点： 是由编译器自动收集类中所有类变量的赋值动作和静态语句块中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： 1234567public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; 与类的构造函数（或者说实例构造器&lt;init&gt;()）不同，不需要显式的调用父类的构造器。虚拟机会自动保证在子类的 &lt;clinit&gt;() 方法运行之前，父类的 &lt;clinit&gt;() 方法已经执行结束。因此虚拟机中第一个执行&lt;clinit&gt;()方法的类肯定为 java.lang.Object。 由于父类的 &lt;clinit&gt;() 方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作。例如以下代码： 12345678910111213141516class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;class Sub extends Parent &#123; public static int B = A;&#125;public class Test &#123; public static void main(String[] args) &#123; System.out.println(Sub.B); // 2 &#125;&#125; &lt;clinit&gt;() 方法对于类或接口不是必须的，如果一个类中不包含静态语句块，也没有对类变量的赋值操作，编译器可以不为该类生成&lt;clinit&gt;()方法。 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 &lt;clinit&gt;()方法。但接口与类不同的是，执行接口的 &lt;clinit&gt;() 方法不需要先执行父接口的 &lt;clinit&gt;() 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 &lt;clinit&gt;() 方法。 虚拟机会保证一个类的 &lt;clinit&gt;()方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的&lt;clinit&gt;()方法，其它线程都会阻塞等待，直到活动线程执行&lt;clinit&gt;() 方法完毕。如果在一个类的 &lt;clinit&gt;() 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。 三、类加载器 虚拟机设计团队把类加载阶段中的 “通过一个类的全限定名来获取描述此类的二进制字节流（即字节码）” 这个动作放到 Java 虚拟机外部去实现，以便让应用程序自己决定如何去获取所需要的类（通过一个类的全限之名获取描述此类的二进制字节流）。实现这个动作的代码模块称为 “类加载器”。 简而言之: 类加载器ClassLoader就是加载其他类的类，它负责将字节码文件加载到内存，创建Class对象。 运行Java程序，就是执行java这个命令，指定包含main方法的完整类名，以及一个classpath，即类路径。类路径可以有多个，对于直接的class文件，路径是class文件的根目录，对于jar包，路径是jar包的完整名称（包括路径和jar包名）。 Java运行时，会根据类的完全限定名寻找并加载类，寻找的方式基本就是在系统类和指定的类路径中寻找，如果是class文件的根目录，则直接查看是否有对应的子目录及文件，如果是jar文件，则首先在内存中解压文件，然后再查看是否有对应的类。 负责加载类的类就是类加载器，它的输入是完全限定的类名，输出是Class对象。 1、类与类加载器 两个类相等，需要类本身相等，并且使用同一个类加载器进行加载。这是因为每一个类加载器都拥有一个独立的类名称空间。 这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom()方法、isInstance()方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true。 2、类加载器分类 从 Java 虚拟机的角度来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），这个类加载器用 C++ 实现，是虚拟机自身的一部分； 所有其他类的加载器，这些类由 Java 实现，独立于虚拟机外部，并且全都继承自抽象类 java.lang.ClassLoader。 按照开发人员来说，类加载器不是只有一个，一般程序运行时，都会有三个： 启动类加载器(Bootstrap ClassLoader)：这个加载器是Java虚拟机实现的一部分，不是Java语言实现的，一般是C++实现的，它负责加载Java的基础类，主要是&lt;JAVA_HOME&gt;/lib/rt.jar，我们日常用的Java类库比如String，ArrayList等都位于该包内。 扩展类加载器(Extension ClassLoader)：这个加载器的实现类是sun.misc.Launcher$ExtClassLoader，它负责加载Java的一些扩展类，一般是&lt;JAVA_HOME&gt;/lib/ext目录中的jar包。 应用程序类加载器(Application ClassLoader)：这个加载器的实现类是sun.misc.Launcher$AppClassLoader，它负责加载应用程序的类，包括自己写的和引入的第三方法类库，即所有在类路径中指定的类。 3、双亲委派模型 上面的三种加载器有一定的关系，可以认为是父子关系，Application ClassLoader的父亲是Extension ClassLoader，Extension的父亲是Bootstrap ClassLoader，注意不是父子继承关系，而是父子委派关系，子ClassLoader有一个变量parent指向父ClassLoader，在子ClassLoader加载类时，一般会首先通过父ClassLoader加载，具体来说，在加载一个类时，基本过程是： 判断是否已经加载过了，加载过了，直接返回Class对象，一个类只会被一个ClassLoader加载一次。 如果没有被加载，先让父ClassLoader去加载，如果加载成功，返回得到的Class对象。 在父ClassLoader没有加载成功的前提下，自己尝试加载类。 这个过程一般被称为&quot;双亲委派&quot;模型，即优先让父ClassLoader去加载。为什么要先让父ClassLoader去加载呢？这样，可以避免Java类库被覆盖的问题，比如用户程序也定义了一个类java.lang.String，通过双亲委派，java.lang.String只会被Bootstrap ClassLoader加载，避免自定义的String覆盖Java类库的定义。需要了解的是，&quot;双亲委派&quot;虽然是一般模型，但也有一些例外，比如： 自定义的加载顺序：尽管不被建议，自定义的ClassLoader可以不遵从&quot;双亲委派&quot;这个约定，不过，即使不遵从，以&quot;java&quot;开头的类也不能被自定义类加载器加载，这是由Java的安全机制保证的，以避免混乱。 网状加载顺序：在OSGI框架中，类加载器之间的关系是一个网，每个OSGI模块有一个类加载器，不同模块之间可能有依赖关系，在一个模块加载一个类时，可能是从自己模块加载，也可能是委派给其他模块的类加载器加载。 父加载器委派给子加载器加载：典型的例子有JNDI服务(Java Naming and Directory Interface)，它是Java企业级应用中的一项服务。 一个程序运行时，会创建一个Application ClassLoader，在程序中用到ClassLoader的地方，如果没有指定，一般用的都是这个ClassLoader，所以，这个ClassLoader也被称为系统类加载器(System ClassLoader)。 如果加载同一个类，该使用哪一个类？父类的。 为什么要使用双亲委派模型？主要是为了避免重复加载的问题。 4、理解ClassLoader 类ClassLoader是一个抽象类，Application ClassLoader和Extension ClassLoader的具体实现类分别是sun.misc.Launcher$AppClassLoader和sun.misc.Launcher$ExtClassLoader，Bootstrap ClassLoader不是由Java实现的，没有对应的类。 每个Class对象都有一个方法，可以获取实际加载它的ClassLoader，方法是： 1public ClassLoader getClassLoader() ClassLoader有一个方法，可以获取它的父ClassLoader： 1public final ClassLoader getParent() 如果ClassLoader是Bootstrap ClassLoader，返回值为null。 测试: 12345678910111213public class ClassLoaderDemo &#123; public static void main(String[] args) &#123; ClassLoader cl = ClassLoaderDemo.class.getClassLoader(); while (cl != null) &#123; System.out.println(cl.getClass().getName()); cl = cl.getParent(); // 一直向上 &#125; System.out.println(String.class.getClassLoader()); // 最后 = null &#125;&#125; 输出: 123sun.misc.Launcher$AppClassLoadersun.misc.Launcher$ExtClassLoadernull ClassLoader有一个静态方法，可以获取默认的系统类加载器： 1public static ClassLoader getSystemClassLoader() ClassLoader中有一个主要方法，用于加载类： 1public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException 测试: 1234567891011121314151617/** * 由于委派机制，Class的getClassLoader()方法返回的不一定是调用loadClass的ClassLoader， * 比如，下面代码中，java.util.ArrayList实际由BootStrap ClassLoader加载，所以返回值就是null。 */public class Test &#123; public static void main(String[] args) &#123; ClassLoader cl = ClassLoader.getSystemClassLoader(); try &#123; Class&lt;?&gt; cls = cl.loadClass(\"java.util.ArrayList\"); // 加载这个 ClassLoader actualLoader = cls.getClassLoader(); System.out.println(actualLoader); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 由于委派机制，Class的getClassLoader()方法返回的不一定是调用loadClass的ClassLoader，比如，上面代码中，java.util.ArrayList实际由BootStrap ClassLoader加载，所以返回值就是null。 在反射中，有两个方法: 12public static Class&lt;?&gt; forName(String className)public static Class&lt;?&gt; forName(String name, boolean initialize, ClassLoader loader) 第一个方法使用系统类加载器加载。第二个指定ClassLoader，参数initialize表示，加载后，是否执行类的初始化代码(如static语句块)，没有指定默认为true。 ClassLoader的loadClass方法和上面forName方法都可以加载类，它们有什么不同呢？基本是一样的，不过，有一个不同，ClassLoader的loadClass不会执行类的初始化代码，看个例子： 12345678910111213141516171819public class CLInitDemo &#123; static class Hello &#123; static &#123; System.out.println(\"hello\"); &#125; &#125; public static void main(String[] args) &#123; ClassLoader cl = ClassLoader.getSystemClassLoader(); String className = CLInitDemo.class.getName() + \"$Hello\"; try &#123;// Class&lt;?&gt; cls = cl.loadClass(className); //没有输出 Class&lt;?&gt; cls = Class.forName(className); //输出 hello &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 使用ClassLoader加载静态内部类Hello，Hello有一个static语句块，输出&quot;hello&quot;，运行该程序，类被加载了，但没有任何输出，即static语句块没有被执行。如果将loadClass的语句换为：Class&lt;?&gt; cls = Class.forName(className);，则static语句块会被执行，屏幕将输出&quot;hello&quot;。 面试题: Java中Class.forName和classloader都可以用来对类进行加载，他们的区别?。 Class.forName()除了将类的.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块。 而classloader只干一件事情，就是将.class文件加载到jvm中，不会执行static中的内容，只有在newInstance才会去执行static块。 Class.forName(name,initialize,loader)带参数也可控制是否加载static块。并且只有调用了newInstance()方法采用调用构造函数，创建类的对象。 看一下ClassLoader的loadClass的源代码: 123public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false);&#125; 它调用了另一个loadClass方法，其主要代码为(省略了一些代码，加了注释，以便于理解)： 1234567891011121314151617181920212223242526272829protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // 首先，检查类是否已经被加载了 Class c = findLoadedClass(name); if (c == null) &#123; //没被加载，先委派父ClassLoader或BootStrap ClassLoader去加载 try &#123; if (parent != null) &#123; //委派父ClassLoader，resolve参数固定为false c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; //没找到，捕获异常，以便尝试自己加载 &#125; if (c == null) &#123; // 自己去加载，findClass才是当前ClassLoader的真正加载方法 c = findClass(name); &#125; &#125; if (resolve) &#123; // 链接，执行static语句块 resolveClass(c); &#125; return c; &#125;&#125; 参数resolve类似Class.forName中的参数initialize，可以看出，其默认值为false，即使通过自定义ClassLoader重写loadClass，设置resolve为true，它调用父ClassLoader的时候，传递的也是固定的false。 findClass是一个protected方法，类ClassLoader的默认实现就是抛出ClassNotFoundException，子类应该重写该方法，实现自己的加载逻辑，后文我们会看个具体例子。 5、自定义类加载器 载器步骤： 定义一个类，继承 ClassLoader； 重写 loadClass 方法； 实例化 Class 对象； 自定义类加载器的优势 类加载器是 Java 语言的一项创新，也是 Java 语言流行的重要原因之一，它最初的设计是为了满足 java applet 的需求而开发出来的； 高度的灵活性； 通过自定义类加载器可以实现热部署； 代码加密；","categories":[{"name":"JVM","slug":"JVM","permalink":"http://blog.tgyf.com/categories/JVM/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://blog.tgyf.com/tags/JVM/"}]},{"title":"高并发下的SpringCloud参数优化","slug":"Java框架/SpringCloud/高并发下的SpringCloud参数优化","date":"2020-03-04T15:53:24.232Z","updated":"2020-03-04T15:53:24.232Z","comments":true,"path":"2020/03/04/Java框架/SpringCloud/高并发下的SpringCloud参数优化/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Java%E6%A1%86%E6%9E%B6/SpringCloud/%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8B%E7%9A%84SpringCloud%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/","excerpt":"","text":"相信不少朋友都在自己公司使用Spring Cloud框架来构建微服务架构，如果只是用户量很少的传统IT系统，使用Spring Cloud可能还暴露不出什么问题. 如果是较多用户量，高峰每秒高达上万并发请求的互联网公司的系统，使用Spring Cloud技术就有一些问题需要注意了. 听说的一个业务场景 故事的开始 朋友A的公司做互联网类的创业，组建了一个小型研发团队，上来就用了Spring Cloud技术栈来构建微服务架构的系统. 一段时间没日没夜的加班，好不容易核心业务系统给做出来了，平时正常QA测试没发现什么大毛病，感觉性能还不错，一切都很完美 问题初现 然后系统就这么上线了，一开始用户规模很小，注册用户量小几十万，日活几千用户. 每天都有新的数据进入数据库的表中，就这么日积月累的，没想到数据规模居然慢慢吞吞增长到了单表几百万. 这个时候呢，看起来也没太大的毛病，就是有用户反映，系统有些操作，会感觉卡顿几秒钟，会刷不出来页面. 这是为啥呢？ 核心原因是单表数据量大了一些，达到了几百万. 有个别服务，跑的SQL比较复杂，一大堆的多表关联 并且还没有设计好索引，或者是设计了索引，但无奈一些小弟写了上百行的大SQL，SQL实在太复杂了，那么一个SQL跑出来好几秒肯定是正常的. 如果大家对微服务框架有点了解的话，应该知道，比如Feign + Ribbon组成的服务调用框架，是有接口调用超时这一说的，有一些参数可以设置接口调用的超时时间. 如果你调用一个接口，好几秒刷不出来，人家就超时异常返回，用户就刷不出来页面了. 饮鸩止渴 一般碰到这种事情，一大坨屎一样的SQL摆在那儿，写SQL的人过一个月自己都看不懂了，80%的工程师看着都不愿意去花时间重写和优化. 一是修改的人力成本太高，二是谁敢负担这责任呢？ 系统跑的好好的，就是慢了点而已，结果你硬是乱改一通，重构，把系统核心业务流程搞挂了怎么办？ 所以，那些兄弟第一反应是：增加超时时间啊！接口慢点可以，但是别超时不响应啊！ 咱们让接口执行个几秒把结果返回，用户不就可以刷出来页面了！不用重构系统了啊！轻松+愉快！ 如何增加呢？看下面的参数 12345678910ribbon: ConnectTimeout: 30000 ReadTimeout: 30000hystrix: command: default: execution: isolation: thread: timeoutInMillisconds: 60000 所以设置超时一般设置两个地方，feign和ribbon那块的超时，还有hystrix那块的超时.其中后者那块的超时一般必须大于前者. 好了，日子在继续… 优化了参数后，看上去效果不错，用户虽然觉得有的页面慢是慢点，但是起码过几秒能刷出来. 这个时候，日活几千的用户量，压根儿没什么并发可言，高峰期每秒最多一二十并发请求罢了. 问题爆发 随着时间的推移，公司业务高速发展…… 那位兄弟的公司，在系统打磨成熟，几万用户试点都ok之后，老板立马拿到一轮几千万的融资. 公司上上下下意气风发啊！紧接着就是组建运营团队，地推团队，全国大范围的推广. 用户量上来后，悲剧的事情就发生了. 高峰期每秒的并发请求居然达到了近万的程度，研发团队的兄弟们哪里敢怠慢！在这个过程中，先是紧张的各种扩容服务，一台变两台，两台变四台. 然后数据库主从架构挂上去，读写分离是必须的，否则单个数据库服务器哪能承载那么大的请求！多搞几个从库，扛一下大量的读请求，这样基本就扛住了. 正准备坐下来喝口茶、松口气，更加悲剧的事情就发生了. 在这个过程中，那些兄弟经常会发现高峰期，系统的某个功能页面，突然就整个hang死了，就是没法再响应任何请求！所有用户刷新这个页面全部都是无法响应！ 这是为什么呢？原因很简单啊！一个服务A的实例里，专门调用服务B的那个线程池里的线程，总共可能就几十个.每个线程调用服务B都会卡住5秒钟. 那如果每秒钟过来几百个请求这个服务实例呢？一下子那个线程池里的线程就全部hang死了，没法再响应任何请求了. 这个时候咋办？兄弟们只能祭出程序员最古老的法宝，重启机器！ 遇到页面刷不出来，只能重启机器，相当于短暂的初始化了一下机器内的资源. 然后接着运行一段时间，又卡死，再次重启！真是令人崩溃啊！用户们的体验是极差的，老板的心情是愤怒的！ 这里学到的经验 12明明应该去优化服务接口性能，结果硬是调大了超时时间.结果导致并发量高了，对那个服务的调用直接hang死，系统的核心页面刷不出来，影响用户体验了. 追本溯源 第一步 关键点，优化图中核心服务B的性能.互联网公司，核心业务逻辑，面向C端用户高并发的请求，不要用上百行的大SQL，多表关联，那样单表几百万行数据量的话，会导致一下执行好几秒. 其实最佳的方式，就是对数据库就执行简单的单表查询和更新，然后复杂的业务逻辑全部放在java系统中来执行，比如一些关联，或者是计算之类的工作. 这一步干完了之后，那个核心服务B的响应速度就已经优化成几十毫秒了，是不是很开心？从几秒变成了几十毫秒！ 第二步 那个超时的时间，也就是上面那段ribbon和hystrix的超时时间设置. 奉劝各位同学，不要因为系统接口的性能过差而懒惰，搞成几秒甚至几十秒的超时，一般超时定义在1秒以内，是比较通用以及合理的. 为什么这么说？ 因为一个接口，理论的最佳响应速度应该在200ms以内，或者慢点的接口就几百毫秒. 如果一个接口响应时间达到1秒+，建议考虑用缓存、索引、NoSQL等各种你能想到的技术手段，优化一下性能. 否则你要是胡乱设置超时时间是几秒，甚至几十秒，万一下游服务偶然出了点问题响应时间长了点呢？那你这个线程池里的线程立马全部卡死！ 具体hystrix的线程池以及超时时间的最佳生产实践，请见下一篇文章：《微服务架构如何保障双11狂欢下的99.99%高可用》 这两步解决之后，其实系统表现就正常了，核心服务B响应速度很快，而且超时时间也在1秒以内，不会出现hystrix线程池频繁卡死的情况了. 第三步 事儿还没完，你要真觉得两步就搞定了，那还是经验不足. 如果你要是超时时间设置成了1秒，如果就是因为偶然发生的网络抖动，导致接口某次调用就是在1.5秒呢？这个是经常发生的，因为网络的问题，接口调用偶然超时. 所以此时配合着超时时间，一般都会设置一个合理的重试，如下所示： 123456ribbon: ConnectTimeout: 10000 ReadTimeout: 10000 OKToRetryOnAllOperations: true MaxAutoRetries: 1 MaxAutoRetriesNextServer: 1 设置这段重试之后，Spring Cloud中的Feign + Ribbon的组合，在进行服务调用的时候，如果发现某台机器超时请求失败，会自动重试这台机器，如果还是不行会换另外一台机器重试. 这样由于偶尔的网络请求造成的超时，不也可以通过自动重试避免了？ 第四步 其实事儿还没完，如果把重试参数配置了，结果你居然就放手了，那还是没对人家负责任啊！ 你的系统架构中，只要涉及到了重试，那么必须上接口的幂等性保障机制. 否则的话，试想一下，你要是对一个接口重试了好几次，结果人家重复插入了多条数据，该怎么办呢？ 其实幂等性保证本身并不复杂，根据业务来，常见的方案： 可以在数据库里建一个唯一索引，插入数据的时候如果唯一索引冲突了就不会插入重复数据 或者是通过redis里放一个唯一id值，然后每次要插入数据，都通过redis判断一下，那个值如果已经存在了，那么就不要插入重复数据了. 类似这样的方案还有一些.总之，要保证一个接口被多次调用的时候，不能插入重复的数据. 优化后","categories":[{"name":"Java框架","slug":"Java框架","permalink":"http://blog.tgyf.com/categories/Java%E6%A1%86%E6%9E%B6/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/categories/SpringCloud/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/tags/SpringCloud/"}]},{"title":"SpringCloud微服务杂谈","slug":"Java框架/SpringCloud/SpringCloud微服务杂谈","date":"2020-03-04T15:17:25.591Z","updated":"2020-03-04T15:17:25.592Z","comments":true,"path":"2020/03/04/Java框架/SpringCloud/SpringCloud微服务杂谈/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Java%E6%A1%86%E6%9E%B6/SpringCloud/SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9D%82%E8%B0%88/","excerpt":"","text":"微服务的定义 微服务是一种架构风格，其理念是把一个系统定义成多个微服务组成，每个服务都只关心自己的业务，而且很好的完成一件事情，微服务与微服务之间相互独立，互相不影响。这种思想延续了java高类聚的原则，每个类只负责完成自己的业务。 微服务的演变 单体架构 传统的项目是以一个整体的方式呈现，整个系统打成一个jar包部署，这样的结构会毋庸置疑会存在一些问题。一是开发，所有的开发人员在一个项目上开发，一是增加了复杂度，二是加大了代码的管理成本。其次是部署，不管是多小的改动都需要整个项目上传更新，这无疑加大了更新的工作量，然后是稳定性，如果功能出现问题，很有可能影响到整个系统阻塞。 SOA服务治理 SOA(Service Oriented Architecture)面向服务的架构，这是一种架构的设计理念，其核心思想就是把整个系统的业务模块和功能中心拆分成独立的服务，比如常见的业务订单服务，功能消息推送功能。每个服务相互独立，服务与服务之间通过网络互相调用。其设计沿用的是低耦合，高类聚的思想。 RPC(Remote Procedure call)远程服务调用，既然设计到服务与服务之间的调用，他还有一个作用就是解决了让远程调用时像本地调用一样方便，让作者感知不到远程调用的逻辑。其核心理念是通过反向代理然后注入一个对象给调用着，这样调用着就这样直接使用服务提供方的方法了。 微服务 微服务是将复杂的系统拆分成很多个小的业务系统，这里我们称这些小的业务系统为组件，也就是微服务。组件，相比于传统系统，以代码库的形式调用，微服务把组件以服务的形式提供远程调用。微服务，强调的是更细粒度的拆分，更精细的拆分更加方便解耦和复用，是系统更清晰，更容易维护。微服务，强调的是更独立，独立的数据库，独立部署，独立运行，这使得微服务的应用更加灵活和快速。 REST是一种软件架构风格，可以提高系统的伸缩性，降低开发的复杂度。如果一个系统能满足REST的这几个条件，那么就是Restful 风格。它以资源为中心，名称是资源地址，动词则表示对资源的操作，所有Rest使用的是http协议。相比于RPC 关注于方法的调用，但是RPC支持多种协议，从效率上来说高于http。 解决什么问题 开发维护：把业务分成很小的服务，每个服务都相互独立，这样一是耦合低，结构清晰。不管是编码人员还是维护人员都只需要关注自己负责的服务。 负载均衡：服务最小化有个好处毋庸置疑就是负载，在现在的互联网项目，为了承受大的流量，会用负载的方式来增加服务器的配置和数量。有人说如果传统项目用集群的方式也能实现负载，但是如果以微服务的方式可以更有针对性，更精确的扩容，节约资源。比如订单模块的压力比较大，我就只需要为订单服务增加服务器，而不需要真个系统都增加。 版本迭代：微服务的设计让偶和最低化，实现分而治之。在版本迭代的时候，只需要更新有变动的服务，而不需要整个模块更新。这使得迭代工作更加快速，便捷。 稳定性：既然版本迭代可以分开来实现，这样在宏观上来说多整个系统的稳定性是有好处的。就算是测试不到位，发现bug，这样也只会影响更新的模块，而不会导致整个系统的阻塞，雪崩。比如购物车模块出问题不会影响用户下单功能。","categories":[{"name":"Java框架","slug":"Java框架","permalink":"http://blog.tgyf.com/categories/Java%E6%A1%86%E6%9E%B6/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/categories/SpringCloud/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/tags/SpringCloud/"}]},{"title":"SpringCloud - Gateway","slug":"Java框架/SpringCloud/SpringCloud - Gateway","date":"2020-03-04T15:08:29.343Z","updated":"2020-03-04T15:08:29.343Z","comments":true,"path":"2020/03/04/Java框架/SpringCloud/SpringCloud - Gateway/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Java%E6%A1%86%E6%9E%B6/SpringCloud/SpringCloud%20-%20Gateway/","excerpt":"","text":"Spring Cloud Gateway Gateway是spring cloud 推出的新的网关路由，它关注于安全性，网关路由，限流控制等，是第二代网关路由器。 Spring Cloud Gateway使用的是Spring Boot和Spring Webflux提供的Netty底层环境，不能和传统的Servlet容器一起使用，也不能打包成一个WAR包。 引入了依赖默认即开启gateway了，如果暂时不想使用这个功能，这可以配置spring.cloud.gateway.enabled=false即可。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; (一)工作原理 当客户端发送请求到Spring Cloud Gateway，Gateway Handler Mapping会匹配Route映射分发到Gateway Web Handler。handler会将请求经过一系列的filter处理，代理请求前，会执行左右的”pre” filter逻辑，代理请求后，会执行所有”post” filter逻辑。 (二)谓词工厂 建议仔细阅读: 官方文档1 官方文档2 2.1 Datetime 接受一个时间参数，满足时间条件后路由 技巧：时间可使用 System.out.println(ZonedDateTime.now()); 打印，然后即可看到时区。例如：2019-09-29T16:50:42.579+08:00[Asia/Shanghai] 时间格式的相关逻辑： 默认时间格式：org.springframework.format.support.DefaultFormattingConversionService#addDefaultFormatters 时间格式注册：org.springframework.format.datetime.standard.DateTimeFormatterRegistrar#regi Before Route Predicate Factory:使用时间作为匹配规则，只要当前时间小于设定时间，路由才会匹配请求。 id:表示路由名称 uri:路由转发的地址 predicates:断言规则 Before:表示当前时间小于设定时间 123456789101112spring: cloud: gateway: routes: - id: before_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当请求时的时间Before配置的时间时，才会转发到用户微服务 # 目前配置不会进该路由配置，所以返回404 # 将时间改成 &gt; now的时间，则访问localhost:8040&#x2F;** -&gt; user-center&#x2F;** # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center&#x2F;users&#x2F;1 - Before&#x3D;2019-09-30T17:42:47.789-07:00[America&#x2F;Denver] After Route Predicate Factory:使用的是时间作为匹配规则，只要当前时间大于设定时间，路由才会匹配请求 id:表示路由名称 uri:路由转发的地址 predicates:断言规则 After:表示大于设定时间 123456789101112spring: cloud: gateway: routes: - id: after_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当请求时的时间After配置的时间时，才会转发到用户微服务 # 目前配置不会进该路由配置，所以返回404 # 将时间改成 &lt; now的时间，则访问localhost:8040&#x2F;** -&gt; user-center&#x2F;** # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center&#x2F;users&#x2F;1 - After&#x3D;2030-01-20T17:42:47.789-07:00[America&#x2F;Denver] Between Route Predicate Factory:使用两个时间作为匹配规则，只要当前时间大于第一个设定时间，并小于第二个设定时间，路由才会匹配请求。 id:表示路由名称 uri:路由转发的地址 predicates:断言规则 Between:表示大于第一个设定时间并小于第二个设定时间 1234567891011spring: cloud: gateway: routes: - id: between_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当请求时的时间Between配置的时间时，才会转发到用户微服务 # 因此，访问localhost:8040&#x2F;** -&gt; user-center&#x2F;** # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center&#x2F;users&#x2F;1 - Between&#x3D;2019-01-20T17:42:47.789-07:00[America&#x2F;Denver], 2027-01-21T17:42:47.789-07:00[America&#x2F;Denver] 2.2 Cookie Cookie Route Predicate Factory:使用的是cookie名字和正则表达式的value作为两个输入参数，请求的cookie需要匹配cookie名和符合其中value的正则。 1234567891011spring: cloud: gateway: routes: - id: cookie_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当带有名为somecookie，并且值符合正则ch.p的Cookie时，才会转发到用户微服务 # 如Cookie满足条件，则访问http:&#x2F;&#x2F;localhost:8040&#x2F;** -&gt; user-center&#x2F;** # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center&#x2F;users&#x2F;1 - Cookie&#x3D;somecookie, ch.p 2.3 Header Header Route Predicate Factory:使用的是两个参数，一个header的name，一个是正则匹配的value。 请求头 header中带 X-Request-Id，且值为数字，允许路由。 1234567891011spring: cloud: gateway: routes: - id: header_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当带有名为X-Request-Id，并且值符合正则\\d+的Header时，才会转发到用户微服务 # 如Header满足条件，则访问http:&#x2F;&#x2F;localhost:8040&#x2F;** -&gt; user-center&#x2F;** # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center&#x2F;users&#x2F;1 - Header&#x3D;X-Request-Id, \\d+ 2.4 Host 1234567891011spring: cloud: gateway: routes: - id: host_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当名为Host的Header符合**.somehost.org或**.anotherhost.org时，才会转发用户微服务 # 如Host满足条件，则访问http:&#x2F;&#x2F;localhost:8040&#x2F;** -&gt; user-center&#x2F;** # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center&#x2F;users&#x2F;1 - Host&#x3D;**.somehost.org,**.anotherhost.org 2.5 Method 1234567891011spring: cloud: gateway: routes: - id: method_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当HTTP请求方法是GET时，才会转发用户微服务 # 如请求方法满足条件，访问http:&#x2F;&#x2F;localhost:8040&#x2F;** -&gt; user-center&#x2F;** # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center&#x2F;users&#x2F;1 - Method&#x3D;GET 2.6 Path 官方文档：segment小技巧 1234567891011spring: cloud: gateway: routes: - id: path_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当访问路径是&#x2F;users&#x2F;*或者&#x2F;some-path&#x2F;**，才会转发用户微服务 # segment是一个特殊的占位符，单层路径匹配 # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center&#x2F;users&#x2F;1 - Path&#x3D;&#x2F;users&#x2F;&#123;segment&#125;,&#x2F;some-path&#x2F;** 2.7 Query 示例1： 12345678910spring: cloud: gateway: routes: - id: query_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当请求带有baz的参数，才会转发到用户微服务 # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1?baz&#x3D;xx -&gt; user-center的&#x2F;users&#x2F;1 - Query&#x3D;baz 示例2： 12345678910spring: cloud: gateway: routes: - id: query_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当请求带有名为foo的参数，且参数值符合正则ba.，才会转发到用户微服务 # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1?baz&#x3D;baz -&gt; user-center的&#x2F;users&#x2F;1?baz&#x3D;baz - Query&#x3D;foo, ba. 2.8 RemoteAddr 官方文档：代理的ip小技巧 XForwardedRemoteAddressResolver::trustAll得到的RemoteAddressResolver总是获取X-Forwarded-For的第一个ip地址作为remote address，这种方式就比较容易被伪装的请求欺骗，模拟请求很容易通过设置初始的X-Forwarded-For头信息，就可以欺骗到gateway。 XForwardedRemoteAddressResolver::maxTrustedIndex得到的RemoteAddressResolver则会在X-Forwarded-For信息里面，从右到左选择信任最多maxTrustedIndex个ip，因为X-Forwarded-For是越往右是越接近gateway的代理机器ip，所以是越往右的ip，信任度是越高的。 那么如果前面只是挡了一层Nginx的话，如果只需要Nginx前面客户端的ip，则maxTrustedIndex取1，就可以比较安全地获取真实客户端ip。 12345678910spring: cloud: gateway: routes: - id: remoteaddr_route uri: lb:&#x2F;&#x2F;user-center predicates: # 当且仅当请求IP是192.168.1.1&#x2F;24网段，例如192.168.1.10，才会转发到用户微服务 # eg. 访问http:&#x2F;&#x2F;localhost:8040&#x2F;users&#x2F;1 -&gt; user-center的&#x2F;users&#x2F;1 - RemoteAddr&#x3D;192.168.1.1&#x2F;24 2.9 Weigth Weight Route Predicate Factory: 12 (三)过滤器工厂 Route Predicate 决定路由到哪个路径，那么过滤器就是允许修改HTTP请求的一些属性。spring cloud 内置了一部分过滤器，也可以自定义过滤器 3.1. AddRequestHeader GatewayFilter Factory 3.2. AddRequestParameter GatewayFilter Factory 3.3. AddResponseHeader GatewayFilter Factory 3.4. DedupeResponseHeader GatewayFilter Factory 3.5. Hystrix GatewayFilter Factory 3.6. FallbackHeaders GatewayFilter Factory 3.7. MapRequestHeader GatewayFilter Factory 3.8. PrefixPath GatewayFilter Factory 3.9. PreserveHostHeader GatewayFilter Factory 3.10. RequestRateLimiter GatewayFilter Factory 3.11. RedirectTo GatewayFilter Factory 3.12. RemoveHopByHopHeadersFilter GatewayFilter Factory 3.13. RemoveRequestHeader GatewayFilter Factory 3.14. RemoveResponseHeader GatewayFilter Factory 3.15. RemoveRequestParameter GatewayFilter Factory 3.16. RewritePath GatewayFilter Factory 3.17. RewriteLocationResponseHeader GatewayFilter Factory 3.18. RewriteResponseHeader GatewayFilter Factory 3.19. SaveSession GatewayFilter Factory 3.20. SecureHeaders GatewayFilter Factory 3.21. SetPath GatewayFilter Factory 3.22. SetRequestHeader GatewayFilter Factory 3.23. SetResponseHeader GatewayFilter Factory 3.24. SetStatus GatewayFilter Factory 3.25. StripPrefix GatewayFilter Factory 3.26. Retry GatewayFilter Factory 3.27. RequestSize GatewayFilter Factory 3.28. Modify Request Body GatewayFilter Factory 3.29. Modify Response Body GatewayFilter Factory 3.30. Default Filters","categories":[{"name":"Java框架","slug":"Java框架","permalink":"http://blog.tgyf.com/categories/Java%E6%A1%86%E6%9E%B6/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/categories/SpringCloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/tags/SpringCloud/"},{"name":"Gateway","slug":"Gateway","permalink":"http://blog.tgyf.com/tags/Gateway/"}]},{"title":"SpringCloud - Eureka","slug":"Java框架/SpringCloud/SpringCloud - Eureka","date":"2020-03-04T14:56:55.247Z","updated":"2020-03-04T14:56:55.248Z","comments":true,"path":"2020/03/04/Java框架/SpringCloud/SpringCloud - Eureka/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Java%E6%A1%86%E6%9E%B6/SpringCloud/SpringCloud%20-%20Eureka/","excerpt":"","text":"服务发现之Eureka/Ribbon 深度剖析服务发现组件~Netflix Eureka https://zhuanlan.zhihu.com/p/24829766 深入理解Ribbon之源码解析 https://blog.csdn.net/forezp/article/details/74820899 (一)Spring Cloud Eureka服务端主要配置项 配置参数(eureka.server.*) 默认值 说明 enableSelfPreservation true 启用注册中心的自保护机制，Eureka 如果统计到15分钟之内损失&gt;15%的微服务心跳，则将会触发自保护机制，不再剔除服务提供者。 waitTimeInMsWhenSyncEmpty 1000 * 60 * 5 在Eureka服务器获取不到集群里对等服务器上的实例时，需要等待的时间，单位为毫秒，默认为1000 * 60 * 5。单机开发模式建议设置为0。 (二)Spring Cloud Eureka客户端主要配置项 配置参数(eureka.client.*) 默认值 说明 serviceUrl 指定服务注册中心地址，类型为 HashMap，并设置有一组默认值，默认的Key为defaultZone；默认的Value为 http://localhost:8761/eureka ，如果服务注册中心为高可用集群时，多个注册中心地址以逗号分隔。如果服务注册中心加入了安全验证，这里配置的地址格式为：http://:@localhost:8761/eureka 其中 为安全校验的用户名； 为该用户的密码 fetchRegistry true 是否从Eureka服务端获取注册信息 registryFetchIntervalSeconds 30 从Eureka服务端获取注册信息的间隔时间，单位为秒 registerWithEureka true 是否要将自身的实例信息注册到Eureka服务端 (三)Spring Cloud Eureka实例主要配置项 配置参数(eureka.instance.*) 默认值 说明 leaseRenewalIntervalInSeconds 30 Eureka客户端向服务端发送心跳的时间间隔，单位为秒 leaseExpirationDurationInSeconds 90 Eureka服务端在收到最后一次心跳之后等待的过期时间上限，单位为秒。超过该时间没有收到心跳，则服务端会将该服务实例从服务清单中剔除，从而禁止服务调用请求被发送到该实例上 appname 服务名，默认取spring.application.name的配置值，如果没有则为unknown hostname 主机名，不配置的时候将根据操作系统的主机名来获取 instance-id 主机名 注册到eureka的实例id，推荐spring.cloud.client.ipaddress:{spring.cloud.client.ipaddress}:spring.cloud.client.ipaddress:{spring.application.name}😒{server.port} (四)Spring Cloud Ribbon主要配置项 配置参数({svc}.ribbon.*) 默认值 说明 ConnectionTimeout 1000ms 连接超时时间 ReadTimeout 1000ms 读取超时时间 ServerListRefreshInterval 30秒 刷新服务列表源的间隔时间 NFLoadBalancerClassName com.netflix.loadbalancer.ZoneAwareLoadBalancer 定制ILoadBalancer实现 NFLoadBalancerRuleClassName com.netflix.loadbalancer.ZoneAvoidanceRule 定制IRule实现 NFLoadBalancerPingClassName com.netflix.loadbalancer.DummyPing 定制IPing NIWSServerListClassName com.netflix.loadbalancer.ConfigurationBasedServerList 定制ServerList ServerListUpdateClassName com.netflix.loadbalancer.PollingServerListUpdater 定制ServerListUpdater NIWSServerListFilterClassName com.netflix.loadbalancer.ZonePreferenceServerListFilter 定制SeverListFilter (五)Spring Cloud Eureka 自保护模式 翻墙查阅：https://medium.com/@fahimfarookme/the-mystery-of-eureka-self-preservation-c7aa0ed1b799 (六)健康检查和蓝绿发布 状态API例子： PUT /eureka/apps/{appId}/{instanceId}?status=UP PUT /eureka/apps/ORDER-SERVICE/localhost:order-service:8886?status=UP PUT /eureka/apps/{app id}/{instance id}/status?value={status} DELETE /eureka/apps/{app id}/{instance id}/status PUT /eureka/apps/ORDER-SERVICE/localhost:order-service:8886/status?value=OUT_OF_SERVICE HealthCheckHandler： 定制注册 EurekaClient#registerHealthCheck Spring Cloud eureka.client.healthcheck.enabled=true EurekaHealthCheckHandler DiskSpaceHealthIndicator RefreshScopeHealthIndicator HystrixHealthIndicator Ribbon软负载实例信息更新延迟： 注册延时（30秒） Eureka服务器响应延迟（30秒） Eureka客户端更新延迟（30秒） Ribbon服务列表更新延迟（30秒） 最大可能有2分钟延迟 测试 配置服务启动端 第一次运行使用8081服务器端口 12server.port&#x3D;8081eureka.client.service-url.defaultZone&#x3D;http:&#x2F;&#x2F;localhost:8082&#x2F;eureka&#x2F; 第二次运行使用8082服务器端口 12server.port&#x3D;8082eureka.client.service-url.defaultZone&#x3D;http:&#x2F;&#x2F;localhost:8081&#x2F;eureka&#x2F; UI校验： 12http:&#x2F;&#x2F;localhost:8081&#x2F;http:&#x2F;&#x2F;localhost:8082&#x2F; API校验： 12http:&#x2F;&#x2F;localhost:8081&#x2F;eureka&#x2F;appshttp:&#x2F;&#x2F;localhost:8082&#x2F;eureka&#x2F;apps","categories":[{"name":"Java框架","slug":"Java框架","permalink":"http://blog.tgyf.com/categories/Java%E6%A1%86%E6%9E%B6/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/categories/SpringCloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/tags/SpringCloud/"},{"name":"Eureka","slug":"Eureka","permalink":"http://blog.tgyf.com/tags/Eureka/"}]},{"title":"JVM调优到底调什么怎么调?","slug":"Java虚拟机/JVM调优到底调什么怎么调","date":"2020-03-04T14:32:18.344Z","updated":"2020-03-04T14:32:18.344Z","comments":true,"path":"2020/03/04/Java虚拟机/JVM调优到底调什么怎么调/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Java%E8%99%9A%E6%8B%9F%E6%9C%BA/JVM%E8%B0%83%E4%BC%98%E5%88%B0%E5%BA%95%E8%B0%83%E4%BB%80%E4%B9%88%E6%80%8E%E4%B9%88%E8%B0%83/","excerpt":"","text":"调优三步曲(发现/分析/解决) 通过监控, 客户, 老板得知问题 分析定位问题 调优解决问题 发现 JVM性能指标 JVM性能指标简单说有三点：CPU、IO、内存 基本检测工具 CPU检测工具 12top, vmstat, ps, jpstop -Hp &lt;pid&gt; IO检测工具 1vmstat,netstat, iostat , pidstat 内存(JVM内存)检测工具 1234567891011jps -lvm 查看当前用户下java应用进程jinfo &lt;pid&gt; 查看进程信息及jvm垃圾回收参数jstat -gcutil &lt;pid&gt; period count jvm统计信息监控jmap -heap &lt;pid&gt; 内存映射工具jhat jstack -l &lt;pid&gt; 堆栈跟踪工具jcmdjvisualvm jconsolejmcgcview gclog图形化查看 分析/解决 情景一：内存泄漏 Java heap space java.lang.OutOfMemoryError:Java heap space Java heap space，Java应用程序创建的对象存放在这片区域，垃圾回收（Garbage Collection）也发生在这块区域。导致该异常通常有：创建大量的对象，层次比较深的递归操作等。 解决方案有两种: 一是优化应用，找到消耗大量内存的地方，然后优化代码或者算法。这种方式比较推荐，但是难度比较大，尤其是在生产环境中出现这种问题，开发人员不能很好的重现问题。 第二种方案是提升Java heap size，这种方式虽然感觉有点治标不治本，但是可行性非常高，操作简单。 对于一般的应用，采用如下方式即可（数字根据自己的需要调整）： -Xms -Set initial Java heap size -Xmx -Set maximum Java heap size 如: java -Xms512m -Xmx1024m JavaApp 如果是在tomcat中，出现的这种问题，解决办法是在{tomcat_dir}/bin/catalina.bat加上一行（内存设置大小根据自己的需要调整）： set CATALINA_OPTS=-Xms512m -Xmx512m 情景二：内存泄漏 PermGen space java.lang.OutOfMemoryError:PermGen space Perm Gen Size（Permanent Generation Size），用来存储被加载的类的定义（class definition）和元数据（metadata），比如：Class Object和Method Object等。这是内存中的一块永久保存区域，JVM的垃圾回收不会触及这块区域。通常在加载一个大项目的时候才会出现该异常。 对于一般的应用，采用如下方式即可（数字根据自己的需要调整）： -XX:PermSize -Set initial PermGen Size. -XX:MaxPermSize -Set the maximum PermGen Size. 如: java -XX:PermSize=64m -XX:MaxPermSize=128m JavaApp 如果是在tomcat中出现这个问题，解决办法是在{tomcat_dir}/bin/catalina.bat中添加如下一行： set CATALINA_OPTS=-server -Xms256m -Xmx1024m -XX:PermSize=512m -XX:MaxPermSize=512m 情景三：内存泄漏 Metaspace java.lang.OutOfMemoryError:Metaspace 在Java8中,将之前PermGen 中的所有内容, 都移到了Metaspace 空间。 例如: class 名称, 字段, 方法, 字节码, 常量池, JIT优化代码, 等等。 -XX:MaxMetaspaceSize=64m 情景三：内存泄漏 GC overhead limit exceeded java.lang.OutOfMemoryError:GC overhead limit exceeded 这个错误会出现在这个场景中：GC占用了多于98%（默认值）的CPU时间却只回收了少于2%（默认值）的堆空间。目的是为了让应用终止，给开发者机会去诊断问题。 一般是应用程序在有限的内存上创建了大量的临时对象或者弱引用对象，从而导致该异常。虽然加大内存可以暂时解决这个问题，但是还是强烈建议去优化代码，后者更加有效。 首先，你可以关闭JVM这个默认的策略：java -XX:-UseGCOverheadLimit JavaApp 其次，你也可以尝试去加大Heap Size：java -Xmx512m JavaApp JVM启动参数 标准参数 参数 参数说明 -version -Dkey=value -Dfile.encoding=UTF-8 用于指定文件编码格式 -Djava.awt.headless=true #Headless模式是系统的一种配置模式。在该模式下，系统缺少了显示设备、键盘或鼠标。 -Djava.library.path=/bin/native 指定非java类包的位置（如：dll，so） -Djava.security.egd=file:/dev/./urandom 使用伪随机数-verbose:class输出jvm载入类的相关信息，当jvm报告说找不到类或者类冲突时可此进行诊断。 -verbose:gc 输出每次GC的相关情况。 -verbose:jni 输出native方法调用的相关情况，一般用于诊断jni调用错误信息。 非标准参数 -X 参数 参数说明 -Xmsn 指定jvm堆的初始大小，默认为物理内存的1/64，最小为1M；可以指定单位，比如k、m，若不指定，则默认为字节。 -Xmxn 指定jvm堆的最大值，默认为物理内存的1/4或者1G，最小为2M；单位与-Xms一致。 -Xmnn 指定jvm堆中年轻代的大小 -Xssn 设置单个线程栈的大小，一般默认为512k。 -Xint 设置jvm以解释模式运行，所有的字节码将被直接执行，而不会编译成本地码。 -Xbatch 关闭后台代码编译，强制在前台编译，编译完成之后才能进行代码执行；默认情况下，jvm在后台进行编译，若没有编译完成，则前台运行代码时以解释模式运行。 -Xbootclasspath:bootclasspath 让jvm从指定路径（可以是分号分隔的目录、jar、或者zip）中加载bootclass，用来替换jdk的rt.jar；若非必要，一般不会用到； -Xbootclasspath/a:path 将指定路径的所有文件追加到默认bootstrap路径中； -Xbootclasspath/p:path 让jvm优先于bootstrap默认路径加载指定路径的所有文件； -Xcheck:jni 对JNI函数进行附加check；此时jvm将校验传递给JNI函数参数的合法性，在本地代码中遇到非法数据时，jmv将报一个致命错误而终止；使用该参数后将造成性能下降，请慎用。 -Xfuture 让jvm对类文件执行严格的格式检查（默认jvm不进行严格格式检查），以符合类文件格式规范，推荐开发人员使用该参数。 -Xnoclassgc 关闭针对class的gc功能；因为其阻止内存回收，所以可能会导致OutOfMemoryError错误，慎用； -Xincgc 开启增量gc（默认为关闭）；这有助于减少长时间GC时应用程序出现的停顿；但由于可能和应用程序并发执行，所以会降低CPU对应用的处理能力。 -Xloggc:file 与-verbose:gc功能类似，只是将每次GC事件的相关情况记录到一个文件中，文件的位置最好在本地，以避免网络的潜在问题。若与verbose命令同时出现在命令行中，则以-Xloggc为准。 扩展参数，非Stable -XX 行为参数 参数 参数说明 -XX:-DisableExplicitGC 禁止调用System.gc()；但jvm的gc仍然有效 -XX:+MaxFDLimit 最大化文件描述符的数量限制 -XX:+ScavengeBeforeFullGC 新生代GC优先于Full GC执行 -XX:+UseGCOverheadLimit 在抛出OOM之前限制jvm耗费在GC上的时间比例 -XX:-UseConcMarkSweepGC 对老生代采用并发标记交换算法进行GC -XX:-UseParallelGC 启用并行GC -XX:-UseParallelOldGC 对Full GC启用并行，当-XX:-UseParallelGC启用时该项自动启用 -XX:-UseSerialGC 启用串行GC -XX:+UseThreadPriorities 启用本地线程优先级 性能调优参数 参数 参数说明 -XX:LargePageSizeInBytes=4m 设置用于Java堆的大页面尺寸 -XX:MaxHeapFreeRatio=70 GC后java堆中空闲量占的最大比例 -XX:MaxNewSize=size 新生成对象能占用内存的最大值 -XX:MaxPermSize=64m 老生代对象能占用内存的最大值 -XX:MinHeapFreeRatio=40 GC后java堆中空闲量占的最小比例 -XX:NewRatio=2 老年代内存容量与新生代内存容量的比例，此处表示新生代为1/3，老年代为2/3。 -XX:SurvivorRatio=3 新生代中eden区和survivor区的比例，此处表示Eden:survivor:survivor=3:1:1，即Eden占新生代2/5。 -XX:NewSize=2.125m 新生代对象生成时占用内存的默认值-XX:PretenureSizeThreshold大于这个设置值(单位：byte)的对象直接在老年代分配。 -XX:MaxTenuringThreshold 对象晋升老年代的年龄阈值（默认为15岁）,如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。(动态对象年龄判定) -XX:ReservedCodeCacheSize=32m 保留代码占用的内存容量 -XX:ThreadStackSize=512 设置线程栈大小，若为0则使用系统默认值 -XX:+UseLargePages 使用大页面内存 调试参数 参数 参数说明 -XX:-CITime 打印消耗在JIT编译的时间 -XX:ErrorFile=./hs_err_pid.log 保存错误日志或者数据到文件中 -XX:-ExtendedDTraceProbes 开启solaris特有的dtrace探针 -XX:HeapDumpPath=./java_pid.hprof 指定导出堆信息时的路径或文件名 -XX:-HeapDumpOnOutOfMemoryError 当首次遭遇OOM时导出此时堆中相关信息 -XX:OnError=&quot;;&quot; 出现致命ERROR之后运行自定义命令 -XX:OnOutOfMemoryError=&quot;;&quot; 当首次遭遇OOM时执行自定义命令 -XX:-PrintClassHistogram 遇到Ctrl-Break后打印类实例的柱状信息，与jmap -histo功能相同 -XX:-PrintConcurrentLocks 遇到Ctrl-Break后打印并发锁的相关信息，与jstack -l功能相同 -XX:-PrintCommandLineFlags 打印在命令行中出现过的标记 -XX:-PrintCompilation 当一个方法被编译时打印相关信息 -XX:-PrintGC 每次GC时打印相关信息 -XX:-PrintGCDetails 每次GC时打印详细信息 -XX:-PrintGCTimeStamps 打印每次GC的时间戳 -XX:-TraceClassLoading 跟踪类的加载信息 -XX:-TraceClassLoadingPreorder 跟踪被引用到的所有类的加载信息 -XX:-TraceClassResolution 跟踪常量池 -XX:-TraceClassUnloading 跟踪类的卸载信息 -XX:-TraceLoaderConstraints 跟踪类加载器约束的相关信息 JVM GC日志查看 开启GC日志 12345678910111213141516-verbose:gc -Djava.awt.headless=true -DAPP_NAME=myapp-Dcom.sun.management.jmxremote-Dcom.sun.management.jmxremote.port=20189 -Dcom.sun.management.jmxremote.authenticate=false-Dcom.sun.management.jmxremote.ssl=false-XX:NewRatio=1 -XX:SurvivorRatio=2 -XX:+PrintGCApplicationStoppedTime -XX:+PrintReferenceGC -Xloggc:gc.log -XX:+PrintCommandLineFlags -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=.-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps jstat 看懂GC日志 https://blog.csdn.net/renfufei/article/details/54885190 Option Displays Ex class 用于查看类加载情况的统计 jstat -class pid:显示加载class的数量，及所占空间等信息。 compiler 查看HotSpot中即时编译器编译情况的统计 jstat -compiler pid:显示VM实时编译的数量等信息。 gc 查看JVM中堆的垃圾收集情况的统计 jstat -gc pid:可以显示gc的信息，查看gc的次数，及时间。其中最后五项，分别是young gc的次数，young gc的时间，full gc的次数，full gc的时间，gc的总时间。 gccapacity 查看新生代、老生代及持久代的存储容量情况 jstat -gccapacity:可以显示，VM内存中三代（young,old,perm）对象的使用和占用大小 gccause 查看垃圾收集的统计情况（这个和-gcutil选项一样），如果有发生垃圾收集，它还会显示最后一次及当前正在发生垃圾收集的原因。 jstat -gccause:显示gc原因 gcnew 查看新生代垃圾收集的情况 jstat -gcnew pid:new对象的信息 gcnewcapacity 用于查看新生代的存储容量情况 jstat -gcnewcapacity pid:new对象的信息及其占用量 gcold 用于查看老生代及持久代发生GC的情况 jstat -gcold pid:old对象的信息 gcoldcapacity 用于查看老生代的容量 jstat -gcoldcapacity pid:old对象的信息及其占用量 gcpermcapacity 用于查看持久代的容量 jstat -gcpermcapacity pid: perm对象的信息及其占用量 gcutil 查看新生代、老生代及持代垃圾收集的情况 jstat -util pid:统计gc信息统计 printcompilation HotSpot编译方法的统计 jstat -printcompilation pid:当前VM执行的信息 GCViewer 图形化GC日志分析利器 https://github.com/chewiebug/GCViewer 12mvn packagejava -jar target\\gcviewer-1.36-SNAPSHOT.jar gc.log summary.csv chart.png JVM调优案例 后端服务，增加Eden占比，减少Eden到Suvivor复制 123-XX:SurvivorRatio=6 增加了Eden在新生代中的比例后, 如果对象很快可以使用完成则直接被清理, 不需要被复制到Survivor区域 服务转发应用，增加新生代空间减少Eden占比，避免清理老年代 12-XX:SurvivorRatio=3减少Eden占比后，对象能保存到Survivor区域，后续不需要被复制到老年代 CMS-Remark之前强制进行年轻代的GC https://segmentfault.com/a/1190000005174819 线程死锁检查（jstack分析线程快照） https://www.jianshu.com/p/f36a1db63ad2","categories":[{"name":"JVM","slug":"JVM","permalink":"http://blog.tgyf.com/categories/JVM/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://blog.tgyf.com/tags/JVM/"}]},{"title":"JVM内存模型及垃圾回收","slug":"Java虚拟机/JVM内存模型及垃圾回收","date":"2020-03-04T12:08:31.852Z","updated":"2020-03-04T12:08:31.852Z","comments":true,"path":"2020/03/04/Java虚拟机/JVM内存模型及垃圾回收/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Java%E8%99%9A%E6%8B%9F%E6%9C%BA/JVM%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","excerpt":"","text":"内存布局 示意图 基本介绍 方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序计数器是运行时线程私有的内存区域。堆中存的是对象。栈中存的是基本数据类型和堆中对象的引用; Java堆（Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 方法区（MethodArea）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 程序计数器（ProgramCounterRegister）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。 JVM栈（JVMStacks）全称Java虚拟机栈（JavaVirtualMachineStacks）与程序计数器一样，也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（StackFrame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 本地方法栈（NativeMethodStacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。 类生命周期 类生命周期(内存中) 加载，查找并加载类的二进制数据，在Java堆中也创建一个java.lang.Class类的对象。 连接，连接又包含三块内容：验证、准备、解析。 1231）验证，文件格式、元数据、字节码、符号引用验证；2）准备，为类的静态变量分配内存，并将其初始化为默认值；3）解析，把类中的符号引用转换为直接引用； 初始化，为类的静态变量赋予正确的初始值。 使用，new出对象程序中使用。 卸载，执行垃圾回收。 堆及垃圾回收 堆及垃圾回收（空间使用方式） 整个堆大小=年轻代大小+年老代大小+持久代大小； 年轻代分三个区。一个Eden区，两个Survivor区(一般而言)。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从 第 一 个S u r v i v o r区 复 制 过 来 的 并 且 此 时 还 存 活 的 对 象，将被复制“年 老 区( Te n u r e d )”；-XX:PretenureSizeThreshold即对象的大小大于此值，就会绕过新生代，直接在老年代分配； 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象； 持久代:用于存放静态文件，如今Java类、方法等。-XX:MaxPermSize（JDK8去除了永久代，引入了元空间Metaspace，-XX:MaxMetaspaceSize=64m） 垃圾回收算法 引用计数 比较古老的回收算法。 原理是此对象有一个引用，即增加一个计数，删除一个引用则减少一个计数。垃圾回收时，只用收集计数为0的对象。此算法最致命的是无法处理循环引用的问题。 标记-清除（Mark-Sweep） 此算法执行分两阶段。 第一阶段从引用根节点开始标记所有被引用的对象 第二阶段遍历整个堆，把未标记的对象清除 此算法需要暂停整个应用，同时，会产生内存碎片。碎片太多可能引发另一次GC； 复制（Copying） 此算法把内存空间划为两个相等的区域，每次只使用其中一个区域。 垃圾回收时，遍历当前使用区域，把正在使用中的对象复制到另外一个区域中。 此方法每次只处理正在使用中的对象，因此复制成本比较小，同时复制过去以后还能进行相应的内存整理，不会出现“碎片”问题。 当然，此算法的缺点也是很明显的，就是需要两倍内存空间。 4. 标记-整理（Mark-Compact） 此算法结合了“标记-清除”和“复制”两个算法的优点。 也是分两阶段: 第一阶段从根节点开始标记所有被引用对象， 第二阶段遍历整个堆，清除未标记对象并且把存活对象“压缩”到堆的其中一块，按顺序排放。 此算法避免了“标记-清除”的碎片问题，同时也避免了“复制”算法的空间问题。 垃圾回收性能指标 吞吐量（用户时间占比） 1吞吐量（Throughput）=运行用户代码时间/（运行用户代码时间+垃圾收集时间） 系统停顿时间（用户时间延迟） 1GC造成的用户线程最长停顿时间 垃圾回收器（算法实现） Serial串行收集器 特点: 仅仅使用单线程进行内存回收； 它是独占式的内存回收； 进行内存回收时,暂停所有的工作线程(“Stop-The-World”)； 使用复制算法； 适合CPU等硬件一般的场合； 到JDK1.7为止，是JVMClient模式下默认的新生代收集器； 设置参数: 1-XX:+UseSerialGC指定使用新生代SerialGC和老年代SerialOldGC SerialOld收集器（单线程独占，标记整理算法） 特点: 同新生代Serial收集器一样，单线程、独占式的垃圾收集器； 使用“标记-整理”算法； 通常老年代内存回收比新生代内存回收要更长时间，所以可能会使应用程序停顿较长时间； 设置参数: 123-XX:+UseSerialGC新生代、老年代都使用串行GC；（分别是Serial和SerialOld）-XX:+UseParNewGC新生代使用ParNew，老年代使用SerialOld；-XX:+UseParallelGC新生代使用Parallel，老年代使用SerialOld； ParNew收集器，并行GC 特点: Serial的多线程版本； 使用复制算法； 垃圾回收时，应用程序仍会暂停，只不过由于是多线程回收，在多核CPU上，回收效率会高于串行GC。反之在单核CPU，效率会不如串行GC； 设置参数: 123-XX:+UseParNewGC新生代使用ParNew，老年代使用SerialOld；-XX:+UseConcMarkSweepGC新生代使用ParNew，老年代使用CMS；-XX:ParallelGCThreads=n指定ParNew收集器工作时的收集线程数，当CPU核数小于8时，默认开启的线程数等于CPU数量，当高于8时，可使用公式：3+((5*CPU_count)/8)。 在JVMServer模式下首选的新生代收集器，其中一个很重要的原因是：除了Serial收集器外，目前只有它能与CMS收集器(并发GC)配合工作。 ParNew收集器在单CPU环境中绝对不会有比Serial收集器更好的效果。 Parallel收集器 特点: 同ParNew回收器一样，不同的地方在于，它非常关注系统的吞吐量(通过参数控制)； 使用复制算法； 支持自适应的GC调节策略； 设置参数: 12345-XX:+UseParallelGC新生代使用Parallel，老年代使用SerialOld；-XX:+UseParallelOldGC新生代使用Parallel，老年代使用ParallelOld；-XX:MaxGCPauseMillis=n设置内存回收的最大停顿时间，单位ms；-XX:GCTimeRatio=n设置吞吐量的大小，假设值为n(在0-100之间)，那么系统将花费不超过1/(n+1)的时间用于内存回收。默认值为99，就是允许最大1%的垃圾收集时间；-XX:+UseAdaptiveSizePolicy自适应GC策略的开关参数。 ParallelOld收集器 特点: 关注吞吐量的老年代并发收集器； 使用“标记-整理”算法； 设置参数: 1-XX:+UseParallelOldGC新生代使用Parallel，老年代使用ParallelOld。这个收集器是在JDK1.6中才开始提供，在此之前，如果新生代选择了Parallel收集器，老年代除了SerialOld收集器外别无选择。 CMS收集器（Concurrent Mark Sweep） 特点: 非独占式的老年代并发收集器，大部分时候应用程序不会停止运行；–使用“标记-清除”算法，因此回收后会有内存碎片，可设置参数进行内存碎片的压缩整理； 与Parallel和ParallelOld不同，CMS主要关注系统停顿时间； 缺点: 对CPU资源敏感； 无法处理浮动垃圾（FloatingGarbage）； 内存碎片问题 设置参数: 12345678-XX:-CMSPrecleaningEnabled关闭预清理，默认在并发标记后会有一个预清理的操作；-XX:+UseConcMarkSweepGC新生代使用ParNew，老年代使用CMS-XX:ConcGCThreads=n设置并发线程数；（早期版本是-XX:ParallelCMSThreads=n）-XX:CMSInitiatingOccupancyFraction=n指定老年代回收阀值，默认值为68；-XX:+UseCMSCompactAtFullCollection开启内存碎片整理；-XX:CMSFullGCsBeforeCompation=n指定进行多少次CMS垃圾回收后再进行一次内存压缩；-XX:+CMSParallelRemarkEnabled在使用UseParNewGC参数的情况下，尽量减少mark(标记)的时间；-XX:+UseCMSInitiatingOccupancyOnly表示只有达到阀值时才进行CMS垃圾回收 G1 取代了CMS 由于G1的出现，CMS在Java9中已被废弃；http://openjdk.java.net/jeps/291 G1（GarbageFirst）是一个横跨新生代和老年代的垃圾回收器。实际上，它已经打乱了前面所说的堆结构，直接将堆分成极其多个区域。每个区域都可以充当Eden区、Survivor区或者老年代中的一个。它采用的是标记-压缩算法(标记-清除-整理)，所以不会产生内存碎片。而且和CMS一样都能够在应用程序运行过程中并发地进行垃圾回收。 G1是逻辑分代，物理不分代 除此之外不仅逻辑分代，而且物理分代 G1能够针对每个细分的区域来进行垃圾回收。在选择进行垃圾回收的区域时，它会优先回收死亡对象较多的区域。这也是G1名字的由来。 https://blog.csdn.net/baiye_xing/article/details/73743395 关于G1的细节，如果想了解得更多，可以参考如下资料： http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html https://www.zhihu.com/question/50398881/answer/120831226 ZGC “Zero”零暂停垃圾回收器暂停时间不超过10ms 参考如下资料： http://openjdk.java.net/projects/zgc/ 垃圾回收器组合 查看命令 1java -XX:+PrintCommandLineFlags -version 在网上找了张图，这样可以直观的展示出他们是怎样组合的. 上图展示了JDK1.7+后，Hotspot JVM的所有垃圾收集器以及它们适用的“代”,适合新生代的垃圾收集器有：Serial、ParNew、Parallel Scavenge、G1。适合年老代的垃圾收集器有：CMS、Serial Old、Parallel Old、G1。它们之间的组合关系如上图连线（粗线相连的是最佳组合），其中G1是JDK1.7 Update14这个版本中正式提供的商业收集器，它可以同时适用于新生代和年老代。 组合相关JVM参数，如图： 第一个组合：Serial + Serial Old Serial作为年轻代回收器和Serial Old垃圾回收器(JDK1.3版本之前)作为老年代回收器。在JDK1.3版本之前是唯一选择，现在基本不用，因为是单进程收集器，没有发挥出现在多核并行处理的优势。 第二个组合：ParNew + CMS ParNew作为年轻代回收器，CMS作为老年代回收器,一般需要手动指定，参数是： -XX:+UseParNewGC -XX:+UseConcMarkSweepGC 因为现在jdk7,8默认不是使用这个策略。而是使用的下面的Parallel Scavenge + Parallel Old。其基本收集原理和下面的Parallel Scavenge + Parallel Old没有区别，区别在于Parallel Scavenge和 Parallel Old有自适应调节策略，直接可以适应最大吞吐量。但忽略了停顿时间，不适用于要求用户体验的场景，个别请求可能等待时间较长。而ParNew + CMS主要场景是注重控制单次回收停顿时间。 第三个组合：Parallel Scavenge + Parallel Old JDK6版本之后引入，Parallel Scavenge作为年轻代回收器，Parallel Old作为老年代回收器，在JDK6之前，Parallel Scavenge只能适配Serial Old，现在是JDK7,JDK8的默认组合。特点上面说了就是适应最大吞吐量。 第四个组合：G1回收器 G1回收器，JDK7出现，JDK9之后的默认回收器，老年代和年轻代都可以回收，特点是直接对停顿时间进行设置。","categories":[{"name":"JVM","slug":"JVM","permalink":"http://blog.tgyf.com/categories/JVM/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://blog.tgyf.com/tags/JVM/"}]},{"title":"SpringCloud框架原理","slug":"Java框架/SpringCloud/SpringCloud框架原理","date":"2020-03-04T07:52:58.339Z","updated":"2020-03-04T07:52:58.339Z","comments":true,"path":"2020/03/04/Java框架/SpringCloud/SpringCloud框架原理/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Java%E6%A1%86%E6%9E%B6/SpringCloud/SpringCloud%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/","excerpt":"","text":"Spring Cloud是一个全家桶式的技术栈，包含了很多组件。本文先从其最核心的几个组件入手，来剖析一下其底层的工作原理。 核心组件 Eureka Feign Ribbon Hystrix Zuul 假设的一个业务场景 直接说他们的原理太空泛了，所以这里我假设了一个电商的业务场景.就把他最核心那个实现支付订单的功能拿出来说一下. 业务流程： 创建一个订单之后，如果用户立刻支付了这个订单，我们需要将订单状态更新为“已支付” 扣减相应的商品库存 通知仓储中心，进行发货 给用户的这次购物增加相应的积分 针对上述流程，我们需要有订单服务、库存服务、仓储服务、积分服务. 整体思路： 用户针对一个订单完成支付之后，就会去找订单服务，更新订单状态（标记为已支付） 订单服务调用库存服务，扣减库存并完成相应功能 订单服务调用仓储服务，通知仓储发货并完成相应功能 订单服务调用积分服务，为用户增加积分并完成相应功能 Spring Cloud微服务架构各组件协作与作用 核心组件：Eureka Eureka是微服务架构中的注册中心，专门负责服务的注册与发现。 说白了，就是告诉Eureka Server，自己在哪台机器上，监听着哪个端口。而Eureka Server是一个注册中心，里面有一个注册表，保存了各服务所在的机器和端口号 订单服务里也有一个Eureka Client组件，这个Eureka Client组件会找Eureka Server问一下：库存服务在哪台机器啊？监听着哪个端口啊？仓储服务呢？积分服务呢？然后就可以把这些相关信息从Eureka Server的注册表中拉取到自己本地缓存起来。 这时如果订单服务想要调用库存服务，不就可以找自己本地的Eureka Client问一下库存服务在哪台机器？监听哪个端口吗？收到响应后，紧接着就可以发送一个请求过去，调用库存服务扣减库存的那个接口！同理，如果订单服务要调用仓储服务、积分服务，也是如法炮制。 总结一下： Eureka Client：负责将这个服务的信息注册到Eureka Server中 Eureka Server：注册中心，里面有一个注册表，保存了各个服务所在的机器和端口号 核心组件：Feign Feign Client会在底层根据你的注解，跟你指定的服务建立连接、构造请求、发起靕求、获取响应、解析响应 Feign的一个关键机制就是使用了动态代理。 首先，如果你对某个接口定义了@FeignClient注解，Feign就会针对这个接口创建一个动态代理 接着你要是调用那个接口，本质就是会调用 Feign创建的动态代理，这是核心中的核心 Feign的动态代理会根据你在接口上的@RequestMapping等注解，来动态构造出你要请求的服务的地址 最后针对这个地址，发起请求、解析响应 核心组件：Ribbon Ribbon的作用是负载均衡，会帮你在每次请求时选择一台机器，均匀的把请求分发到各个机器上. Ribbon的负载均衡默认使用的最经典的Round Robin轮询算法。这是啥？简单来说，就是如果订单服务对库存服务发起10次请求，那就先让你请求第1台机器、然后是第2台机器、第3台机器、第4台机器、第5台机器，接着再来—个循环，第1台机器、第2台机器…以此类推。 此外，Ribbon是和Feign以及Eureka紧密协作，完成工作的，具体如下： 首先Ribbon会从 Eureka Client里获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口号。 然后Ribbon就可以使用默认的Round Robin算法，从中选择一台机器 Feign就会针对这台机器，构造并发起请求。 核心组件：Hystrix Hystrix相当于Springcloud中的保险，作用是隔离、熔断以及降级，避免因为某些服务的不可用导致服务雪崩问题. 业务场景为例：订单服务在一个业务流程里需要调用三个服务。现在假设订单服务自己最多只有100个线程可以处理请求，然后呢，积分服务不幸的挂了，每次订单服务调用积分服务的时候，都会卡住几秒钟，然后抛出—个超时异常。 这样会导致什么问题？ 如果系统处于高并发的场景下，大量请求涌过来的时候，订单服务的100个线程都会卡在请求积分服务这块。导致订单服务没有一个线程可以处理请求 然后就会导致别人请求订单服务的时候，发现订单服务也挂了，不响应任何请求了 这个就是微服务架构中恐怖的服务雪崩问题 服务雪崩问题如何解决？ Hystrix会搞很多个小小的线程池，比如订单服务请求库存服务是一个线程池，请求仓储服务是一个线程池，请求积分服务是一个线程池。每个线程池里的线程就仅仅用于请求那个服务。 打个比方：现在很不幸，积分服务挂了，会咋样？ 当然会导致订单服务里的那个用来调用积分服务的线程都卡死不能工作了啊！但是由于订单服务调用库存服务、仓储服务的这两个线程池都是正常工作的，所以这两个服务不会受到任何影响。 这个时候如果别人请求订单服务，订单服务还是可以正常调用库存服务扣减库存，调用仓储服务通知发货。只不过调用积分服务的时候，每次都会报错。但是如果积分服务都挂了，每次调用都要去卡住几秒钟干啥呢？有意义吗？当然没有！所以我们直接对积分服务熔断不就得了，比如在5分钟内请求积分服务直接就返回了，不要去走网络请求卡住几秒钟，这个过程，就是所谓的熔断！ 那人家又说，兄弟，积分服务挂了你就熔断，好歹你干点儿什么啊！别啥都不干就直接返回啊？没问题，咱们就来个降级：每次调用积分服务，你就在数据库里记录一条消息，说给某某用户增加了多少积分，因为积分服务挂了，导致没增加成功！这样等积分服务恢复了，你可以根据这些记录手工加一下积分。这个过程，就是所谓的降级。 核心组件：Zuul Zuul，也就是微服务网关，这个组件是负责网络路由的. 假设你后台部署了几百个服务，现在有个前端兄弟，人家请求是直接从浏览器那儿发过来的。打个比方：人家要请求一下库存服务，你难道还让人家记着这服务的名字叫做inventory-service？部署在5台机器上？就算人家肯记住这一个，你后台可有几百个服务的名称和地址呢？难不成人家请求一个，就得记住一个？你要这样玩儿，那真是友谊的小船，说翻就翻！ 上面这种情况，压根儿是不现实的。所以一般微服务架构中都必然会设计一个网关在里面，像android、ios、pc前端、微信小程序、H5等等，不用去关心后端有几百个服务，就知道有一个网关，所有请求都往网关走，网关会根据请求中的一些特征，将请求转发给后端的各个服务。 而且有一个网关之后，还有很多好处，比如可以做统一的降级、限流、认证授权、安全，等等。 总结 Eureka：各个服务启动时，Eureka Client都会将服务注册到Eureka Server，并且Eureka Client还可以反过来从Eureka Server拉取注册表，从而知道其他服务在哪里 Ribbon：服务间发起请求的时候，基于Ribbon做负载均衡，从一个服务的多台机器中选择一台 Feign：基于Feign的动态代理机制，根据注解和选择的机器，拼接请求URL地址，发起请求 Hystrix：发起请求是通过Hystrix的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题 Zuul：如果前端、移动端要调用后端系统，统一从Zuul网关进入，由Zuul网关转发请求给对应的服务","categories":[{"name":"Java框架","slug":"Java框架","permalink":"http://blog.tgyf.com/categories/Java%E6%A1%86%E6%9E%B6/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/categories/SpringCloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tgyf.com/tags/Java/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.tgyf.com/tags/SpringCloud/"}]},{"title":"MQ总结：（六）如果写一个消息队列，应该怎样考虑?","slug":"Middleware/MQ总结：（六）如果写一个消息队列，应该怎样考虑","date":"2020-03-04T06:45:47.084Z","updated":"2020-03-04T06:45:47.084Z","comments":true,"path":"2020/03/04/Middleware/MQ总结：（六）如果写一个消息队列，应该怎样考虑/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Middleware/MQ%E6%80%BB%E7%BB%93%EF%BC%9A%EF%BC%88%E5%85%AD%EF%BC%89%E5%A6%82%E6%9E%9C%E5%86%99%E4%B8%80%E4%B8%AA%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%8C%E5%BA%94%E8%AF%A5%E6%80%8E%E6%A0%B7%E8%80%83%E8%99%91/","excerpt":"","text":"很多时候我们都没考虑过这样的问题，因为平时的工作中除非是框架部门的，不然大多数时候我们都是用消息队列，而不是去搞消息队列中间件的开发. 之所以有这篇思考性的文章，主要是之前由于之前去面试一家公司被问到怎样去设计一个mybatis的框架.其实这类问题都是可以发散的，比如还可以思考怎样去设计spring框架、dubbo框架、netty框架、springcloud框架等等. 其实，说白了就是对比几个同类型产品，技术的基本原理，核心组成部分，基本架构构成，然后参照他们拿出一个系统设计出来的思路. 设计考虑的几个方向 可伸缩性 就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下kafka的设计理念，broker -&gt; topic -&gt; partition，每个partition放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给topic增加partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？ 数据落盘 落磁盘，才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是kafka的思路。 可用性 参考一下kafka的高可用保障机制。多副本 -&gt; leader &amp; follower -&gt; broker挂了重新选举leader即可对外服务。 数据零丢失 参考kafka数据零丢失方案","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"}]},{"title":"MQ总结：（五）消息队列里产生积压怎么解决？","slug":"Middleware/MQ总结：（五）消息队列里产生积压怎么解决","date":"2020-03-04T06:18:17.269Z","updated":"2020-03-04T06:18:17.269Z","comments":true,"path":"2020/03/04/Middleware/MQ总结：（五）消息队列里产生积压怎么解决/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Middleware/MQ%E6%80%BB%E7%BB%93%EF%BC%9A%EF%BC%88%E4%BA%94%EF%BC%89%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%87%8C%E4%BA%A7%E7%94%9F%E7%A7%AF%E5%8E%8B%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3/","excerpt":"","text":"消息积压常见的问题 如何解决消息队列的延时以及过期失效问题？ 消息队列满了以后该怎么处理？ 有几百万消息持续积压几小时，怎么解决？ 消息积压情景再现 假设一个场景，我们现在消费端出故障了，然后大量消息在mq里积压，出现生产事故了. 场景一:大量消息在mq里积压了几个小时了还没解决 几千万条数据在MQ里积压了七八个小时，从下午4点多，积压到了晚上很晚，10点多，11点多 这个时候要不然就是修复consumer的问题，让他恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。 一个消费者一秒是1000条，一秒3个消费者是3000条，一分钟是18万条，1000多万条 所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概1小时的时间才能恢复过来 一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下： 先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉 新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量 然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue 接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据 这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据 等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息 场景二:超过rabbitmq设置过期时间（TTL）数据直接被清理掉 这个情况下，就不是说要增加consumer消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。 这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入mq里面去，把白天丢的数据给他补回来。也只能是这样了。 假设1万个订单积压在mq里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单给查出来，手动发到mq里去再补一次 场景三:大量消息在mq里积压长时间都没处理掉，导致mq快写满了 临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个场景的方案，到了晚上再补数据吧。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"}]},{"title":"MQ总结：（四）从消息队列里拿到的数据按顺序执行怎么保证？","slug":"Middleware/MQ总结：（四）从消息队列里拿到的数据按顺序执行怎么保证","date":"2020-03-04T05:57:17.413Z","updated":"2020-03-04T05:57:17.414Z","comments":true,"path":"2020/03/04/Middleware/MQ总结：（四）从消息队列里拿到的数据按顺序执行怎么保证/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Middleware/MQ%E6%80%BB%E7%BB%93%EF%BC%9A%EF%BC%88%E5%9B%9B%EF%BC%89%E4%BB%8E%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%87%8C%E6%8B%BF%E5%88%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8C%89%E9%A1%BA%E5%BA%8F%E6%89%A7%E8%A1%8C%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81/","excerpt":"","text":"假设的一个消息的顺序性情景 要做一个mysql binlog同步的系统，在mysql里增删改一条数据，对应出来了增删改3条binlog，接着这三条binlog发送到MQ里面，到消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么. rabbitmq消息的顺序性 rabbitmq错乱的场景 一个queue，多个consumer. rabbitmq如何保证消息的顺序性？ 拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦点；或者就一个queue但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理. kafka消息的顺序性 kafka错乱的场景 一个topic，一个partition，一个consumer，内部多线程. kafka如何保证消息的顺序性？ 一个topic，一个partition，一个consumer，内部单线程消费，写N个内存queue，然后N个线程分别消费一个内存queue即可","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"}]},{"title":"MQ总结：（三）发到消息队列里面的数据不见了怎么办?","slug":"Middleware/MQ总结：（三）发到消息队列里面的数据不见了怎么办","date":"2020-03-04T05:45:36.942Z","updated":"2020-03-04T05:45:36.942Z","comments":true,"path":"2020/03/04/Middleware/MQ总结：（三）发到消息队列里面的数据不见了怎么办/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Middleware/MQ%E6%80%BB%E7%BB%93%EF%BC%9A%EF%BC%88%E4%B8%89%EF%BC%89%E5%8F%91%E5%88%B0%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%87%8C%E9%9D%A2%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E8%A7%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/","excerpt":"","text":"用mq有个基本原则：数据不能多一条，也不能少一条. 不能多，就是重复消费和幂等性问题. 不能少，就是说这数据别搞丢了. 丢数据的情景 丢数据，mq大体一般分为三种： 要么是我们把消息送达mq的时候弄丢了 要么是mq自己弄丢了 要么是我们消费mq的时候弄丢了 rabbitmq丢数据的三种情景 生产者弄丢了数据 生产者将数据发送到rabbitmq的时候，可能数据就在半路给搞丢了，因为网络啥的问题，都有可能。 此时可以选择用rabbitmq提供的事务功能，就是生产者发送数据之前开启rabbitmq事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，rabbitmq事务机制一搞，基本上吞吐量会下来，因为太耗性能。 所以一般来说，如果你要确保说写rabbitmq的消息别丢，可以开启confirm模式，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给你回传一个ack消息，告诉你说这个消息ok了。如果rabbitmq没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和cnofirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用confirm机制的。 rabbitmq弄丢了数据 就是rabbitmq自己弄丢了数据，这个你必须开启rabbitmq的持久化，就是消息写入之后会持久化到磁盘，哪怕是rabbitmq自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，rabbitmq还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小。 设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。 而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。 哪怕是你给rabbitmq开启了持久化机制，也有一种可能，就是这个消息写到了rabbitmq中，但是还没来得及持久化到磁盘上，结果不巧，此时rabbitmq挂了，就会导致内存里的一点点数据会丢失。 消费端弄丢了数据 rabbitmq如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，rabbitmq认为你都消费了，这数据就丢了。 这个时候得用rabbitmq提供的ack机制，简单来说，就是你关闭rabbitmq自动ack，可以通过一个api来调用就行，然后每次你自己代码里确保处理完的时候，再程序里ack一把。这样的话，如果你还没处理完，不就没有ack？那rabbitmq就认为你还没处理完，这个时候rabbitmq会把这个消费分配给别的consumer去处理，消息是不会丢的。 kafka丢数据的情景 消费端弄丢了数据 唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了offset，让kafka以为你已经消费好了这个消息，其实你刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是一样么，大家都知道kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。但是此时确实还是会重复消费，比如你刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 生产环境碰到的一个问题，就是说我们的kafka消费者消费到了数据之后是写到一个内存的queue里先缓冲一下，结果有的时候，你刚把消息写入内存queue，然后消费者会自动提交offset。 然后此时我们重启了系统，就会导致内存queue里还没来得及处理的数据就丢失了 kafka弄丢了数据 这块比较常见的一个场景，就是kafka某个broker宕机，然后重新选举partiton的leader时。大家想想，要是此时其他的follower刚好还有些数据没有同步，结果此时leader挂了，然后选举某个follower成leader之后，他不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前kafka的leader机器宕机了，将follower切换为leader之后，就会发现说这个数据就丢了 所以此时一般是要求起码设置如下4个参数： 给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本 在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧 在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了 在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了 我们生产环境就是按照上述要求配置的，这样配置之后，至少在kafka broker端就可以保证在leader所在broker发生故障，进行leader切换时，数据不会丢失 生产者会不会弄丢数据 如果按照上述的思路设置了ack=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"}]},{"title":"MQ总结：（一）如果MQ挂了怎么办?","slug":"Middleware/MQ总结：（一）如果MQ挂了怎么办","date":"2020-03-04T05:08:46.039Z","updated":"2020-03-04T05:08:46.039Z","comments":true,"path":"2020/03/04/Middleware/MQ总结：（一）如果MQ挂了怎么办/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Middleware/MQ%E6%80%BB%E7%BB%93%EF%BC%9A%EF%BC%88%E4%B8%80%EF%BC%89%E5%A6%82%E6%9E%9CMQ%E6%8C%82%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/","excerpt":"","text":"如何保证消息队列的高可用？ 如果MQ挂了，导致几个小时系统不可用，公司损失几千万，Team背锅，你闹的祸，你老大帮你一起背锅. 所以说，在非常核心的系统里，一定要考虑引入MQ所导致系统可用性降低的问题. RabbitMQ的高可用性 Rabbitmq有三种模式：单机模式，普通集群模式，镜像集群模式. RabbitMQ – 单机模式 就是demo级别的，一般就是你本地启动了玩玩儿的，用于本地开发环境，没人在生产环境用单机模式. RabbitMQ – 普通集群模式 就是在多台机器上启动多个rabbitmq实例，每个机器启动一个.但是你创建的queue，只会放在一个rabbtimq实例上，但是每个实例都同步queue的元数据.完了你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从queue所在实例上拉取数据过来. 缺点： 这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群.因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个queue所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈. 如果那个放queue的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让rabbitmq落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个queue拉取数据. 总结： 所以综上所述这就没有什么所谓的高可用性可言了，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个queue的读写操作. RabbitMQ – 镜像集群模式 这种模式，才是所谓的rabbitmq的高可用模式，跟普通集群模式不一样的是，你创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，然后每次你写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步. 优点： 你任何一个机器宕机了，没事儿，别的机器都可以用. 缺点： 第一 这个性能开销太大了，消息同步所有机器，导致网络带宽压力和消耗很重！ 第二 这么玩，就没有扩展性可言了，如果某个queue负载很重，你加机器，新增的机器也包含了这个queue的所有数据，并没有办法线性扩展你的queue. 怎么开启镜像集群模式? rabbitmq有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候可以要求数据同步到所有节点的，也可以要求就同步到指定数量的节点，然后你再次创建queue的时候，应用这个策略，就会自动将数据同步到其他的节点上去了. kafka的高可用性 kafka最基本的架构认识 多个broker组成，每个broker是一个节点；你创建一个topic，这个topic可以划分为多个partition，每个partition可以存在于不同的broker上，每个partition就放一部分数据. 这就是天然的分布式消息队列，就是说一个topic的数据，是分散放在多个机器上的，每个机器就放一部分数据. 实际上rabbitmq之类的，并不是分布式消息队列，他就是传统的消息队列，只不过提供了一些集群、HA的机制而已，因为无论怎么玩儿，rabbitmq一个queue的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个queue的完整数据. kafka的HA机制 kafka 0.8以前，是没有HA机制的，就是任何一个broker宕机了，那个broker上的partition就废了，没法写也没法读，没有什么高可用性可言. kafka 0.8以后，提供了HA机制，就是replica副本机制.每个partition的数据都会同步到吉他机器上，形成自己的多个replica副本.然后所有replica会选举一个leader出来，那么生产和消费都跟这个leader打交道，然后其他replica就是follower.写的时候，leader会负责把数据同步到所有follower上去，读的时候就直接读leader上数据即可.只能读写leader？很简单，要是你可以随意读写每个follower，那么就要care数据一致性的问题，系统复杂度太高，很容易出问题.kafka会均匀的将一个partition的所有replica分布在不同的机器上，这样才可以提高容错性. 就有所谓的高可用性了，因为如果某个broker宕机了，没事儿，那个broker上面的partition在其他机器上都有副本的，如果这上面有某个partition的leader，那么此时会重新选举一个新的leader出来，大家继续读写那个新的leader即可.这就有所谓的高可用性了. 写数据的时候，生产者就写leader，然后leader将数据落地写本地磁盘，接着其他follower自己主动从leader来pull数据.一旦所有follower同步好数据了，就会发送ack给leader，leader收到所有follower的ack之后，就会返回写成功的消息给生产者.（当然，这只是其中一种模式，还可以适当调整这个行为） 消费的时候，只会从leader去读，但是只有一个消息已经被所有follower都同步成功返回ack的时候，这个消息才会被消费者读到.","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"}]},{"title":"关于中间件的一些思考","slug":"Middleware/ThinkInMiddleware","date":"2020-03-04T01:57:25.884Z","updated":"2020-03-04T01:57:25.885Z","comments":true,"path":"2020/03/04/Middleware/ThinkInMiddleware/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Middleware/ThinkInMiddleware/","excerpt":"","text":"为什么有这篇文章？ 在项目团队中往往有这样一些人，并不知道自己为什么要在项目用中间件这个东西.其实说白了，就是为了用而用.或者是别人设计的架构，从头到尾没有思考过.这些人给我的映像就是木头木脑的干呆活儿，一本正经的挖坑.为了避免以后团队中出现这类型人，所以有了这篇思维导图型的文章. 1.为什么使用这个类型中间件？ 其实就是这个中间件都有哪些使用场景，然后在项目里具体是什么场景，这个业务场景有个什么技术挑战，如果不用可能会很麻烦，但是你现在用了之后带给了你很多的好处. 2.有什么优点和缺点？原理是怎样的？ 引入中间件之后会不会有什么坏处？要是没考虑过这个，那盲目弄个中间件进系统里，后面出了问题是不是当事人就溜了，这就是给公司后来接盘的人留坑.要是没考虑过引入一个技术可能存在的弊端和风险，这类哥们，基本可能就是挖坑型选手. 3.同类型产品调研对比，分别适合哪些场景？ 中间件没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势. 如果去设计个什么系统，在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑.","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"}]},{"title":"MQ总结：（二）MQ里消费到重复数据怎么办？","slug":"Middleware/MQ总结：（二）MQ里消费到重复数据怎么办","date":"2020-03-04T01:56:02.060Z","updated":"2020-03-04T01:56:02.060Z","comments":true,"path":"2020/03/04/Middleware/MQ总结：（二）MQ里消费到重复数据怎么办/","link":"","permalink":"http://blog.tgyf.com/2020/03/04/Middleware/MQ%E6%80%BB%E7%BB%93%EF%BC%9A%EF%BC%88%E4%BA%8C%EF%BC%89MQ%E9%87%8C%E6%B6%88%E8%B4%B9%E5%88%B0%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE%E6%80%8E%E4%B9%88%E5%8A%9E/","excerpt":"","text":"如何保证消息消费时的幂等性？（或：如何保证消息不被重复消费？） 既然是消费消息，那肯定要考虑考虑这三点： 会不会重复消费？ 能不能避免重复消费？ 或者重复消费了也别造成系统异常可以吗？ 大概可能会有哪些重复消费 如rabbitmq、rocketmq、kafka，都有可能会出现消费重复消费的问题，正常.因为这问题通常不是mq自己保证的，是给你保证的.然后我们挑一个kafka来举个例子，说说怎么重复消费吧. kafka实际上有个offset的概念，就是每个消息写进去，都有一个offset，代表他的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息的offset提交一下，代表我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的offset来继续消费吧. 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接kill进程了，再重启.这会导致consumer有些消息处理了，但是没来得及提交offset，尴尬了.重启之后，少数消息会再次消费一次. 其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性. 怎么保证消息队列消费的幂等性 其实还是得结合业务来思考，这里给几个思路： 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update一下好吧 比如你是写redis，那没问题了，反正每次都是set，天然幂等性 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的id，类似订单id之类的东西，然后你这里消费到了之后，先根据这个id去比如redis里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个id写redis.如果消费过了，那你就别处理了，保证别重复处理相同的消息即可. 还有比如基于数据库的唯一键来保证重复数据不会重复插入多条，我们之前线上系统就有这个问题，就是拿到数据的时候，每次重启可能会有重复，因为kafka消费者还没来得及提交offset，重复数据拿到了以后我们插入的时候，因为有唯一键约束了，所以重复数据只会插入报错，不会导致数据库中出现脏数据 如何保证MQ的消费是幂等性的，需要结合具体的业务来看","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"}],"tags":[{"name":"一些思考","slug":"一些思考","permalink":"http://blog.tgyf.com/tags/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"}]},{"title":"RabbitMQ（四） - 优先级队列(PriorityQueue)","slug":"Middleware/RabbitMQ/RabbitMQ-PriorityQueue","date":"2020-02-24T15:11:23.490Z","updated":"2020-02-24T15:11:23.490Z","comments":true,"path":"2020/02/24/Middleware/RabbitMQ/RabbitMQ-PriorityQueue/","link":"","permalink":"http://blog.tgyf.com/2020/02/24/Middleware/RabbitMQ/RabbitMQ-PriorityQueue/","excerpt":"","text":"RabbitMQ - 优先级队列(PriorityQueue) 在RabbitMQ中使用优先级特性需要的版本为3.5+。 使用优先级特性只需做两件事情： 1. 将队列声明为优先级队列，即在创建队列的时候添加参数 x-max-priority 以指定最大的优先级，值为0-255（整数）。 2. 为优先级消息添加优先级。 注意:没有指定优先级的消息会将优先级以0对待。 对于超过优先级队列所定最大优先级的消息，优先级以最大优先级对待。对于相同优先级的消息，后进的排在前面。 核心代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.tgyf.rabbit.config;import lombok.extern.slf4j.Slf4j;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.DirectExchange;import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import java.util.HashMap;import java.util.Map;/** * direct exchange -- 直接交换器 * 发送到该交换器的消息都会被路由到与 routing key 匹配的队列中 * @author 韬光养月巴 * @modify * @createDate 2019/8/24 1:29 PM * @remark */@Configuration@Slf4jpublic class DirectExchangeConf &#123; public static final String QUEUE = \"direct-queue-priority\"; public static final String EXCHANGE = \"exchange-direct\"; public static final String ROUTING_KEY = \"direct.queue.priority\"; @Bean Queue directQueuePriority() &#123; //创建队列的时候添加参数 x-max-priority 以指定最大的优先级，值为0-255 Map&lt;String, Object&gt; args= new HashMap&lt;&gt;(); args.put(\"x-max-priority\", 100); return new Queue(QUEUE, false, false, false, args); &#125; @Bean DirectExchange directExchange() &#123; return new DirectExchange(EXCHANGE); &#125; @Bean Binding directQueuePriorityBinding(Queue directQueuePriority, DirectExchange directExchange) &#123; return BindingBuilder.bind(directQueuePriority).to(directExchange).with(ROUTING_KEY); &#125;&#125; 测试 1.测试优先级队列 发送优先级低的消息 100 条到 RabbitMQ curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-direct&quot;, &quot;routingKey&quot;: &quot;direct.queue.priority&quot;, &quot;priority&quot;: 1, &quot;content&quot;:&quot; hello priority queue! &quot;, &quot;count&quot;: 100 }' 发送优先级高的消息 5 条到 RabbitMQ curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-direct&quot;, &quot;routingKey&quot;: &quot;direct.queue.priority&quot;, &quot;priority&quot;: 10, &quot;content&quot;:&quot; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; hello priority queue! &quot;, &quot;count&quot;: 5 }'","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.tgyf.com/categories/RabbitMQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.tgyf.com/tags/RabbitMQ/"}]},{"title":"RabbitMQ（二） - 交换器（Exchange）","slug":"Middleware/RabbitMQ/RabbitMQ-Exchang","date":"2020-02-24T15:11:16.763Z","updated":"2020-02-24T15:11:16.763Z","comments":true,"path":"2020/02/24/Middleware/RabbitMQ/RabbitMQ-Exchang/","link":"","permalink":"http://blog.tgyf.com/2020/02/24/Middleware/RabbitMQ/RabbitMQ-Exchang/","excerpt":"","text":"RabbitMQ - 交换器（Exchange） 交换器名称 作用 fanout exchange 发送到该交换器的所有消息，会被路由到其绑定的所有队列 direct exchange 发送到该交换器的消息，会通过路由键完全匹配，匹配成功就会路由到指定队列 topic exchange 发送到该交换器的消息，会通过路由键模糊匹配，匹配成功就会路由到指定队列 header exchange 发送到该交换器的消息，会通过消息的 header 信息匹配，匹配成功就会路由到指定队列 核心代码 pom.xml 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.tgyf.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;exchange&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; direct exchange – 直接交换器 发送到该交换器的消息都会被路由到与 routing key 匹配的队列中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.tgyf.rabbit.config;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.DirectExchange;import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * direct exchange -- 直接交换器 * 发送到该交换器的消息都会被路由到与 routing key 匹配的队列中 * @author 韬光养月巴 * @modify * @createDate 2019/8/24 12:07 PM * @remark */@Configurationpublic class DirectExchangeConf &#123; public static final String QUEUE_1 = \"direct-queue-1\"; public static final String QUEUE_2 = \"direct-queue-2\"; private static final String EXCHANGE = \"exchange-direct\"; private static final String ROUTING_KEY_TO_QUEUE1 = \"queue.direct.key1\"; private static final String ROUTING_KEY_TO_QUEUE2 = \"queue.direct.key2\"; @Bean Queue directQueue1() &#123; return new Queue(QUEUE_1, false); &#125; @Bean Queue directQueue2() &#123; return new Queue(QUEUE_2, false); &#125; @Bean DirectExchange directExchange() &#123; return new DirectExchange(EXCHANGE); &#125; @Bean Binding bindingDirectQueue1(Queue directQueue1, DirectExchange directExchange) &#123; return BindingBuilder.bind(directQueue1).to(directExchange).with(ROUTING_KEY_TO_QUEUE1); &#125; @Bean Binding bindingDirectQueue2(Queue directQueue2, DirectExchange directExchange) &#123; return BindingBuilder.bind(directQueue2).to(directExchange).with(ROUTING_KEY_TO_QUEUE2); &#125;&#125; fanout exchange – 扇出交换器 所有发送到该交换器的消息都会被路由到所有与该交换器绑定的队列中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.tgyf.rabbit.config;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.FanoutExchange;import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * fanout exchange -- 扇出交换器 * 所有发送到该交换器的消息都会被路由到所有与该交换器绑定的队列中 * @author 韬光养月巴 * @modify * @createDate 2019/8/24 12:04 PM * @remark */@Configurationpublic class FanoutExchangeConf &#123; public static final String QUEUE_1 = \"fanout-queue-1\"; public static final String QUEUE_2 = \"fanout-queue-2\"; private static final String EXCHANGE = \"exchange-fanout\"; @Bean Queue fanoutQueue1() &#123; return new Queue(QUEUE_1, false); &#125; @Bean Queue fanoutQueue2() &#123; return new Queue(QUEUE_2, false); &#125; @Bean FanoutExchange fanoutExchange() &#123; return new FanoutExchange(EXCHANGE); &#125; @Bean Binding bindingFanoutQueue1(Queue fanoutQueue1, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange); &#125; @Bean Binding bindingFanoutQueue2(Queue fanoutQueue2, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange); &#125;&#125; headers exchange – headers交换器 发送到该交换器的消息会根据消息的 header 信息路由到对应的队列 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.tgyf.rabbit.config;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.HeadersExchange;import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;/** * headers exchange -- headers交换器 * 发送到该交换器的消息会根据消息的 header 信息路由到对应的队列 * 说明： * where 匹配单个 header * whereAll 同时匹配多个 header * whereAny 匹配一个或多个 header * * @author 韬光养月巴 * @modify * @createDate 2019/8/24 12:12 PM * @remark */public class HeadersExchangeConf &#123; public static final String QUEUE_1 = \"headers-queue-1\"; public static final String QUEUE_2 = \"headers-queue-2\"; public static final String QUEUE_3 = \"headers-queue-3\"; private static final String EXCHANGE = \"exchange-headers\"; @Bean Queue headersQueue1() &#123; return new Queue(QUEUE_1, false); &#125; @Bean Queue headersQueue2() &#123; return new Queue(QUEUE_2, false); &#125; @Bean Queue headersQueue3() &#123; return new Queue(QUEUE_3, false); &#125; @Bean HeadersExchange headersExchange() &#123; return new HeadersExchange(EXCHANGE); &#125; @Bean Binding bindingHeadersQueue1(Queue headersQueue1, HeadersExchange headersExchange) &#123; return BindingBuilder.bind(headersQueue1).to(headersExchange).where(\"one\").exists(); &#125; @Bean Binding bindingHeadersQueue2(Queue headersQueue1, HeadersExchange headersExchange) &#123; return BindingBuilder.bind(headersQueue1).to(headersExchange).whereAll(\"all1\", \"all2\").exist(); &#125; @Bean Binding bindingHeadersQueue3(Queue headersQueue3, HeadersExchange headersExchange) &#123; return BindingBuilder.bind(headersQueue3).to(headersExchange).whereAny(\"any1\", \"any2\").exist(); &#125;&#125; topic exchange – 主题交换器 发送到该交换器的消息都会被路由到与 routing key 匹配的队列中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.tgyf.rabbit.config;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.Queue;import org.springframework.amqp.core.TopicExchange;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * topic exchange -- 主题交换器 * 发送到该交换器的消息都会被路由到与 routing key 匹配的队列中 * 说明： * routing key 以 '.' 分隔为多个单词 * routing key 以 '*' 匹配一个单词 * routing key 以 '#' 匹配零个或多个单词 * 例如： * queue.topic.key -&gt; QUEUE_1 + QUEUE_2 * test.topic.key -&gt; QUEUE_1 * queue -&gt; QUEUE_2 * queue.topic -&gt; QUEUE_2 * * @author 韬光养月巴 * @modify * @createDate 2019/8/24 12:09 PM * @remark */@Configurationpublic class TopicExchangeConf &#123; public static final String QUEUE_1 = \"topic-queue-1\"; public static final String QUEUE_2 = \"topic-queue-2\"; private static final String EXCHANGE = \"exchange-topic\"; private static final String ROUTING_KEY_TO_QUEUE1 = \"*.topic.*\"; private static final String ROUTING_KEY_TO_QUEUE2 = \"queue.#\"; @Bean Queue topicQueue1() &#123; return new Queue(QUEUE_1, false); &#125; @Bean Queue topicQueue2() &#123; return new Queue(QUEUE_2, false); &#125; @Bean TopicExchange topicExchange() &#123; return new TopicExchange(EXCHANGE); &#125; @Bean Binding bindingTopicQueue1(Queue topicQueue1, TopicExchange topicExchange) &#123; return BindingBuilder.bind(topicQueue1).to(topicExchange).with(ROUTING_KEY_TO_QUEUE1); &#125; @Bean Binding bindingTopicQueue2(Queue topicQueue2, TopicExchange topicExchange) &#123; return BindingBuilder.bind(topicQueue2).to(topicExchange).with(ROUTING_KEY_TO_QUEUE2); &#125;&#125; 测试 1.测试fanout exchange curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-fanout&quot;, &quot;routingKey&quot;: &quot;default&quot;, &quot;content&quot;:&quot; hello fanout!&quot;, &quot;count&quot;: 1 }' 2.测试 direct exchange curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-direct&quot;, &quot;routingKey&quot;: &quot;queue.direct.key1&quot;, &quot;content&quot;:&quot; hello direct! &quot;, &quot;count&quot;: 1 }' curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-direct&quot;, &quot;routingKey&quot;: &quot;queue.direct.key2&quot;, &quot;content&quot;:&quot; hello direct! &quot;, &quot;count&quot;: 1 }' 3.测试 topic exchange curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-topic&quot;, &quot;routingKey&quot;: &quot;queue.topic.key1&quot;, &quot;content&quot;:&quot; hello topic! &quot;, &quot;count&quot;: 1 }' curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-topic&quot;, &quot;routingKey&quot;: &quot;test.topic.key2&quot;, &quot;content&quot;:&quot; hello topic! &quot;, &quot;count&quot;: 1 }' curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-topic&quot;, &quot;routingKey&quot;: &quot;queue.hello&quot;, &quot;content&quot;:&quot; hello topic! &quot;, &quot;count&quot;: 1 }' 4.测试 headers exchange curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-headers&quot;, &quot;content&quot;:&quot; hello headers! &quot;, &quot;count&quot;: 1, &quot;headers&quot;:{ &quot;one&quot;:&quot;value&quot; } }' curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-headers&quot;, &quot;content&quot;:&quot; hello headers! &quot;, &quot;count&quot;: 1, &quot;headers&quot;:{ &quot;all1&quot;:&quot;value&quot;, &quot;all2&quot;:&quot;value&quot; } }' curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-headers&quot;, &quot;content&quot;:&quot; hello headers! &quot;, &quot;count&quot;: 1, &quot;headers&quot;:{ &quot;any2&quot;:&quot;value&quot;, } }'","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.tgyf.com/categories/RabbitMQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.tgyf.com/tags/RabbitMQ/"}]},{"title":"RabbitMQ（三） - 死信队列(DeadLetterQueue)","slug":"Middleware/RabbitMQ/RabbitMQ-DeadletterQueue","date":"2020-02-24T15:11:10.610Z","updated":"2020-02-24T15:11:10.611Z","comments":true,"path":"2020/02/24/Middleware/RabbitMQ/RabbitMQ-DeadletterQueue/","link":"","permalink":"http://blog.tgyf.com/2020/02/24/Middleware/RabbitMQ/RabbitMQ-DeadletterQueue/","excerpt":"","text":"RabbitMQ - 死信队列(DeadLetterQueue) 什么是死信 “死信”是RabbitMQ中的一种消息机制，当你在消费消息时，如果队列里的消息出现以下情况： 1.消息被否定确认，使用 channel.basicNack 或 channel.basicReject ，并且此时requeue 属性被设置为false。 2.消息在队列的存活时间超过设置的TTL时间。 3.消息队列的消息数量已经超过最大队列长度。 那么该消息将成为“死信”。 “死信”消息会被RabbitMQ进行特殊处理，如果配置了死信队列信息，那么该消息将会被丢进死信队列中，如果没有配置，则该消息将会被丢弃。 死信生命周期 死信队列只是一个绑定在死信交换机上的普通队列，而死信交换机也只是一个普通的交换机，不过是用来专门处理死信的交换机。 死信的生命周期： 业务消息被投入业务队列 消费者消费业务队列的消息，由于处理过程中发生异常，于是进行了nck或者reject操作 被nck或reject的消息由RabbitMQ投递到死信交换机中 死信交换机将消息投入相应的死信队列 死信队列的消费者消费死信消息 死信消息是RabbitMQ为我们做的一层保证，其实我们也可以不使用死信队列，而是在消息消费异常时，将消息主动投递到另一个交换机中，关键在于这些Exchange和Queue怎么配合。比如从死信队列拉取消息，然后发送邮件、短信、钉钉通知来通知开发人员关注。或者将消息重新投递到一个队列然后设置过期时间，来进行延时消费。 死信消息的Header 字段名 含义 x-first-death-exchange 第一次被抛入的死信交换机的名称 x-first-death-reason 第一次成为死信的原因，rejected：消息在重新进入队列时被队列拒绝，由于default-requeue-rejected 参数被设置为false。expired ：消息过期。maxlen ： 队列内消息数量超过队列最大容量 x-first-death-queue 第一次成为死信前所在队列名称 x-death 历次被投入死信交换机的信息列表，同一个消息每次进入一个死信交换机，这个数组的信息就会被更新 死信队列应用场景 一般用在较为重要的业务队列中，确保未被正确消费的消息不被丢弃，一般发生消费异常可能原因主要有由于消息信息本身存在错误导致处理异常，处理过程中参数校验异常，或者因网络波动导致的查询异常等等，当发生异常时，当然不能每次通过日志来获取原消息，然后让运维帮忙重新投递消息（没错，以前就是这么干的= =）。通过配置死信队列，可以让未正确处理的消息暂存到另一个队列中，待后续排查清楚问题后，编写相应的处理代码来处理死信消息，这样比手工恢复数据要好太多了。 核心代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.tgyf.rabbit.config;import lombok.extern.slf4j.Slf4j;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.DirectExchange;import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * 死信交换器 * * @author 韬光养月巴 * @modify * @createDate 2019/8/24 2:18 PM * @remark */@Configuration@Slf4jpublic class DeadLetterExchangeConf &#123; public static final String QUEUE_BY_MAX_LENGTH = \"direct-queue-dead-by-max-length\"; public static final String QUEUE_BY_TTL = \"direct-queue-dead-by-ttl\"; public static final String QUEUE_BY_REJECT = \"direct-queue-dead-by-reject\"; public static final String EXCHANGE = \"exchange-direct-dead\"; public static final String ROUTING_KEY_BY_MAX_LENGTH = \"direct.queue.dead.max.length\"; public static final String ROUTING_KEY_BY_TTL = \"direct.queue.dead.ttl\"; public static final String ROUTING_KEY_BY_REJECT = \"direct.queue.dead.reject\"; @Bean Queue deadByMaxLengthQueue() &#123; return new Queue(QUEUE_BY_MAX_LENGTH, false); &#125; @Bean Queue deadByTTLQueue() &#123; return new Queue(QUEUE_BY_TTL, false); &#125; @Bean Queue deadByRejectQueue() &#123; return new Queue(QUEUE_BY_REJECT, false); &#125; @Bean DirectExchange deadDirectExchange() &#123; return new DirectExchange(EXCHANGE); &#125; @Bean Binding deadByMaxLengthQueueBinding(Queue deadByMaxLengthQueue, DirectExchange deadDirectExchange) &#123; return BindingBuilder.bind(deadByMaxLengthQueue).to(deadDirectExchange).with(ROUTING_KEY_BY_MAX_LENGTH); &#125; @Bean Binding deadByTTLQueueBinding(Queue deadByTTLQueue, DirectExchange deadDirectExchange) &#123; return BindingBuilder.bind(deadByTTLQueue).to(deadDirectExchange).with(ROUTING_KEY_BY_TTL); &#125; @Bean Binding deadByRejectQueueBinding(Queue deadByRejectQueue, DirectExchange deadDirectExchange) &#123; return BindingBuilder.bind(deadByRejectQueue).to(deadDirectExchange).with(ROUTING_KEY_BY_REJECT); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.tgyf.rabbit.config;import lombok.extern.slf4j.Slf4j;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.DirectExchange;import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import java.util.HashMap;import java.util.Map;/** * 普通交换器 * * @author 韬光养月巴 * @modify * @createDate 2019/8/24 2:18 PM * @remark */@Configuration@Slf4jpublic class DirectExchangeConf &#123; public static final String QUEUE_MAX_LENGTH = \"direct-queue-max-length\"; public static final String QUEUE_TTL = \"direct-queue-ttl\"; public static final String QUEUE_REJECT = \"direct-queue-reject\"; public static final String EXCHANGE = \"exchange-direct\"; public static final String ROUTING_KEY_MAX_LENGTH = \"direct.queue.max.length\"; public static final String ROUTING_KEY_TTL = \"direct.queue.ttl\"; public static final String ROUTING_KEY_REJECT = \"direct.queue.reject\"; @Bean Queue maxLengthQueue() &#123; Map&lt;String, Object&gt; args= new HashMap&lt;&gt;(); // 设置队列最大长度 args.put(\"x-max-length\", 10); // 设置死信转发的 exchange 和 routing key args.put(\"x-dead-letter-exchange\", DeadLetterExchangeConf.EXCHANGE); args.put(\"x-dead-letter-routing-key\", DeadLetterExchangeConf.ROUTING_KEY_BY_MAX_LENGTH); return new Queue(QUEUE_MAX_LENGTH, false, false, false, args); &#125; @Bean Queue ttlQueue() &#123; Map&lt;String, Object&gt; args= new HashMap&lt;&gt;(); // 设置消息存活时间 10s args.put(\"x-message-ttl\", 10000); // 设置死信转发的 exchange 和 routing key args.put(\"x-dead-letter-exchange\", DeadLetterExchangeConf.EXCHANGE); args.put(\"x-dead-letter-routing-key\", DeadLetterExchangeConf.ROUTING_KEY_BY_TTL); return new Queue(QUEUE_TTL, false, false, false, args); &#125; @Bean Queue rejectQueue() &#123; Map&lt;String, Object&gt; args= new HashMap&lt;&gt;(); // 设置死信转发的 exchange 和 routing key args.put(\"x-dead-letter-exchange\", DeadLetterExchangeConf.EXCHANGE); args.put(\"x-dead-letter-routing-key\", DeadLetterExchangeConf.ROUTING_KEY_BY_REJECT); return new Queue(QUEUE_REJECT, false, false, false, args); &#125; @Bean DirectExchange directExchange() &#123; return new DirectExchange(EXCHANGE); &#125; @Bean Binding maxLengthQueueBinding(Queue maxLengthQueue, DirectExchange directExchange) &#123; return BindingBuilder.bind(maxLengthQueue).to(directExchange).with(ROUTING_KEY_MAX_LENGTH); &#125; @Bean Binding ttlQueueBinding(Queue ttlQueue, DirectExchange directExchange) &#123; return BindingBuilder.bind(ttlQueue).to(directExchange).with(ROUTING_KEY_TTL); &#125; @Bean Binding rejectQueueBinding(Queue rejectQueue, DirectExchange directExchange) &#123; return BindingBuilder.bind(rejectQueue).to(directExchange).with(ROUTING_KEY_REJECT); &#125;&#125; 测试 1.测试消费者否认消息 curl -X POST \\ http://127.0.0.1:8080/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-direct&quot;, &quot;routingKey&quot;: &quot;direct.queue.reject&quot;, &quot;content&quot;:&quot; hello reject queue! &quot;, &quot;count&quot;: 1 }' 2.测试消息超出队列最大长度 curl -X POST \\ http://127.0.0.1:8080/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-direct&quot;, &quot;routingKey&quot;: &quot;direct.queue.max.length&quot;, &quot;content&quot;:&quot; hello max length queue! &quot;, &quot;count&quot;: 30 }' 提示：消息队列遵循先进先出的策略，假设队列最大长度设置为 10，发送 30 条消息到该队列，若无消费者，前 20 条消息会被转发到指定的其他队列，后 10 条会保存在该队列中，除非有新的消息入队，这 10 条消息才会被转发 3.测试消息超时 curl -X POST \\ http://127.0.0.1:8080/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-direct&quot;, &quot;routingKey&quot;: &quot;direct.queue.ttl&quot;, &quot;content&quot;:&quot; hello ttl queue! &quot;, &quot;count&quot;: 10 }' 4.测试延迟队列 curl -X POST \\ http://127.0.0.1/send \\ -H 'Content-Type: application/json' \\ -d '{ &quot;exchange&quot;:&quot;exchange-direct&quot;, &quot;routingKey&quot;: &quot;direct.queue.delay&quot;, &quot;content&quot;:&quot; hello delay delay! &quot;, &quot;count&quot;: 1, &quot;delayTime&quot;: &quot;10000&quot; }'","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.tgyf.com/categories/RabbitMQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.tgyf.com/tags/RabbitMQ/"}]},{"title":"RabbitMQ（一） - 配置详解","slug":"Middleware/RabbitMQ/RabbitMQ","date":"2020-02-24T15:10:50.667Z","updated":"2020-02-24T15:10:50.667Z","comments":true,"path":"2020/02/24/Middleware/RabbitMQ/RabbitMQ/","link":"","permalink":"http://blog.tgyf.com/2020/02/24/Middleware/RabbitMQ/RabbitMQ/","excerpt":"","text":"启动 RabbitMQ docker run -d --name rabbitmq -p 5672:5672 -p 15672:15672 -v `pwd`/data:/var/lib/rabbitmq --hostname rabbit -e RABBITMQ_DEFAULT_VHOST=my_vhost -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin rabbitmq:3.7-management-plugins 构建RabbitMQ镜像参考RabbitMQ 镜像dockerfile 内容结构 序号 内容名称 内容说明 1 exchange 交换器 2 priorityqueue 优先级队列 3 deadletterqueue 死信队列 Spring Boot RabbitMQ 参数配置详解 连接配置 spring.rabbitmq.host=localhost # RabbitMQ 地址 spring.rabbitmq.port=5672 # RabbitMQ 端口 spring.rabbitmq.username=guest # RabbitMQ 用户名 spring.rabbitmq.password=guest # RabbitMQ 密码 spring.rabbitmq.addresses= # 设置 RabbitMQ 集群，多个地址使用 &quot;,&quot; 分隔，例如：192.168.0.100:5672,192.168.0.101:5672 spring.rabbitmq.virtual-host= # 设置 Virtual Host spring.rabbitmq.ssl.algorithm= # SSL 算法，默认情况下，由 Rabbit 客户端配置 spring.rabbitmq.ssl.enabled=false # 是否启用 SSL 支持 spring.rabbitmq.ssl.key-store= # key 存储路径 spring.rabbitmq.ssl.key-store-password= # 用于访问 key 的密码 spring.rabbitmq.ssl.key-store-type=PKCS12 # Key 存储类型 spring.rabbitmq.ssl.trust-store= # Trust 存储路径 spring.rabbitmq.ssl.trust-store-password= # 用于访问 Trust 的密码 spring.rabbitmq.ssl.trust-store-type=JKS # Trust 存储类型 spring.rabbitmq.ssl.validate-server-certificate=true # 是否启用服务端证书验证 spring.rabbitmq.ssl.verify-hostname=true # 是否启用 hostname 验证 Publisher 配置 spring.rabbitmq.publisher-confirms=false # 是否启用 publisher 确认 spring.rabbitmq.publisher-returns=false # 是否启用 publisher 返回 spring.rabbitmq.template.default-receive-queue= # 没有没确定指定队列时的默认队列 spring.rabbitmq.template.exchange= # 发送消息默认的 exchange spring.rabbitmq.template.mandatory= # 是否启用 mandatory 消息 spring.rabbitmq.template.receive-timeout= # `receive()` 操作的超时时间 spring.rabbitmq.template.reply-timeout= # `sendAndReceive()` 操作的超时时间 spring.rabbitmq.template.retry.enabled=false # 是否启用重试 spring.rabbitmq.template.retry.initial-interval=1000ms # 两次重试间的时间间隔 spring.rabbitmq.template.retry.max-attempts=3 # 最大重试次数 spring.rabbitmq.template.retry.max-interval=10000ms # 最长重试时间 spring.rabbitmq.template.retry.multiplier=1 # Multiplier to apply to the previous retry interval. spring.rabbitmq.template.routing-key= # 发送消息默认的 routing key Consumer 设置 spring.rabbitmq.listener.direct.acknowledge-mode= # 确认模式：auto / manual / none spring.rabbitmq.listener.direct.auto-startup=true # 是否在应用启动时自动启动容器 spring.rabbitmq.listener.direct.consumers-per-queue= # 每个队列的消费者数量 spring.rabbitmq.listener.direct.default-requeue-rejected= # 默认情况下，拒收的消息是否重新排队 spring.rabbitmq.listener.direct.idle-event-interval= # 空闲容器事件发布的频率 spring.rabbitmq.listener.direct.missing-queues-fatal=false # 如果容器声明的队列在 broker 上不可用，是否失败 spring.rabbitmq.listener.direct.prefetch= # 预加载的消息数量 spring.rabbitmq.listener.direct.retry.enabled=false # 是否启用发布重试 spring.rabbitmq.listener.direct.retry.initial-interval=1000ms # 两次重试时间间隔 spring.rabbitmq.listener.direct.retry.max-attempts=3 # 最大重试次数 spring.rabbitmq.listener.direct.retry.max-interval=10000ms # 最长重试时间 spring.rabbitmq.listener.direct.retry.multiplier=1 # 上次重试间隔的倍数 spring.rabbitmq.listener.direct.retry.stateless=true # 重试是否有状态 spring.rabbitmq.listener.simple.acknowledge-mode= # 确认模式：auto / manual / none spring.rabbitmq.listener.simple.auto-startup=true # 是否在应用启动时自动启动容器 spring.rabbitmq.listener.simple.concurrency= # 监听器最小线程数 spring.rabbitmq.listener.simple.default-requeue-rejected= # 默认情况下，拒收的消息是否重新排队 spring.rabbitmq.listener.simple.idle-event-interval= # 空闲容器事件发布的频率 spring.rabbitmq.listener.simple.max-concurrency= # 监听器最大线程数 spring.rabbitmq.listener.simple.missing-queues-fatal=true # 如果容器声明的队列在 broker 上不可用，是否失败； 如果在运行时删除队列，容器是否停止 spring.rabbitmq.listener.simple.prefetch= # 预加载的消息数量 spring.rabbitmq.listener.simple.retry.enabled=false # 是否启用发布重试 spring.rabbitmq.listener.simple.retry.initial-interval=1000ms # 两次重试时间间隔 spring.rabbitmq.listener.simple.retry.max-attempts=3 # 最大重试次数 spring.rabbitmq.listener.simple.retry.max-interval=10000ms # 最长重试时间 spring.rabbitmq.listener.simple.retry.multiplier=1 # 上次重试间隔的倍数 spring.rabbitmq.listener.simple.retry.stateless=true # 重试是否有状态 spring.rabbitmq.listener.simple.transaction-size= # 确认模式为 auto 时，在 acks 之间处理的消息数. 如果大于预加载的数量，则预加载的数量增加到此值 rabbitmq listener 类型有两种：simple 和 direct，二者有什么区别呢？ DirectMessageListenerContainer 注释如下： The {@code SimpleMessageListenerContainer} is not so simple. Recent changes to the rabbitmq java client has facilitated a much simpler listener container that invokes the listener directly on the rabbit client consumer thread. There is no txSize property - each message is acked (or nacked) individually. 其他设置 spring.rabbitmq.dynamic=true # 是否创建 AmqpAdmin bean spring.rabbitmq.requested-heartbeat= # 请求心跳超时时间. 设置为 0 代表没有，如果未指定时间后缀，则默认使用秒 Spring Boot RabbitMQ 队列属性详解 属性名称 属性说明 Durable 代表该队列是否持久化至硬盘（若要使队列中消息不丢失，同时也需要将消息声明为持久化 Exclusive 是否声明该队列是否为连接独占，若为独占，连接关闭后队列即被删除 Auto-delete 若没有消费者订阅该队列，队列将被删除 Arguments 可选map类型参数，可以指定队列长度，消息生存时间，镜相设置等 RabbitMQ规定，队列的名字最长不超过UTF-8编码的255字节 RabbitMQ内部的Queue命名规则采用 &quot;amq.&quot;形式，注意不要与此规则冲突 常见问题 1. 声明了一个已经存在的队列？ 如果队列已经存在，再次声明将不会起作用。若原始队列参数和该次声明时不同则会报异常。 2. 队列中消息顺序？ 默认情况下是FIFO，即先进先出，同时也支持发送消息时指定消息的优先级。 3. 队列消息存放位置？ 对于临时消息，RabbitMQ尽量将其存放在内存，当出现内存告警时，MQ会将消息持久化至硬盘。对于持久化消息与Lazy-queues，MQ会先将消息存入硬盘，消费时再取出。 4. 队列中消息的消费？ 默认情况下，MQ会设置消费者的消费确认模式为自动。对于一些重要消息的处理，推荐确认模式改为手动。（nack和reject区别？nack可以一次拒绝多条消息） 5. 队列中消息的消费速度？ 通过Prefetch（通道上最大未确认投递数量）设置消费者每次消费的条数，一般将该值设为1，但他会降低吞吐量。RabbitMQ官网建议的是100-300.（更建议反复试验得到一个表现符合期望的值） 6. 队列中消息状态？ 队列中的消息共有俩种状态，一是准备投递，二是已投递但未确认。队列最大长度？ 声明队列时可以指定最大长度，需要注意的是只限制状态为准备投递的数量，未确认的消息不计算在内。当队列长度超过限制，MQ会根据策略选择丢弃（默认）或者将消息投递进死信队列。 7. 关于死信队列？ 其实更准确的说法是死信交换机，提前声明一个交换机，在声明队列时使用“x-dead-letter-exchange”参数（可指定routKey）将队列绑定到该死信交换机。消息有以下情况之一会成为死信：被reject或者nack，消息超过生存时间，队列长度超过限制。 8. 关于不能路由到队列的消息？ 这个和上面一样，其实不算Queue系列而是Exchange。针对消息无法路由到队列的情况MQ提供了Alternate Exchange处理。声明Exchange时添加args.put(“alternate-exchange”,“my-ae”)参数。即当该交换机存在无法路由的消息时，它将消息发布到AE上，AE把消息路由到绑定在他上面的消息。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://blog.tgyf.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/categories/MQ/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.tgyf.com/categories/RabbitMQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.tgyf.com/tags/MQ/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.tgyf.com/tags/RabbitMQ/"}]},{"title":"Hexo主题pure使用指南","slug":"Blog/Hexo主题pure使用指南","date":"2020-02-23T15:12:34.123Z","updated":"2020-02-23T15:12:34.123Z","comments":true,"path":"2020/02/23/Blog/Hexo主题pure使用指南/","link":"","permalink":"http://blog.tgyf.com/2020/02/23/Blog/Hexo%E4%B8%BB%E9%A2%98pure%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"","text":"123预览地址: https://blog.cofess.com项目地址: https://github.com/cofess/hexo-theme-pure中文使用文档: https://github.com/cofess/hexo-theme-pure/blob/master/README.cn.md 使用前请操作 使用该主题前, 请先复制 theme/pure/_source/ 目录下的所有内容到 blog path/source/ 目录下 原因在于该目录下有建好的菜单 categories(分类)、tags(标签)、repository(项目)、books(书单)、links(友链)、about(关于)页面 当你使用自动生成分类、标签，展示github项目时 文章目录索引 在文章详情页, 展示一个文章目录 主题配置文件中开启配置: 12config toc: true # 是否开启文章章节目录导航 在文章顶部将该文章开启索引, 如: 12345678910111213---title: Hexo主题pure使用指南date: 2019-11-05 14:34:15tags: - hexo主题categories:- hexotoc: true # 是否启用内容索引sidebar: none # 是否启用sidebar侧边栏，none：不启用--- 侧边栏 主题配置项中, 侧边栏可以如下配置: 123456789101112131415# 侧边栏sidebar: right# 侧边栏启用哪些模块widgets: - board # 公告 - category # 分类 - tag # 标签 - tagcloud # 标签云 - archive # 归档 - recent_posts # 最近文章# 归档列表的展示方式archive_type: 'monthly' # 归档方式: yearly | monthlyshow_count: true # 显示每个归档的文章总数 图集 在文章详情页中, 涉及的图片可以使用图集功能, 在点击一张图片时, 放大图片. 主题的图册公告是使用fancybox实现, 可以参照github 1234# Fancybox# 图集功能fancybox: true 展示github项目 在左侧菜单项目中, 点击展示自己的github项目 在主题配置文件中 _config.yml 修改, 请配置自己github用户名 123github: username: caoruiy # github用户名 新建repository页面: 12&gt; hexo new repository 你也可以直接复制 theme/pure/_source/ 目录下 repository文件夹 到 博客根目录/source/ 目录下 将文件内容修改为: 1234567---title: Repositorieslayout: repositorycomments: falsesidebar: none--- 关键内容为 layout: repository, 包含该属性才可以展示github项目 评论功能 主题集成了disqus、友言、来必力、gitment、gitalk评论系统，选择其中一种即可 你可以在主题配置文件中修改评论工具 123comment: type: valine # 启用哪种评论系统 Valine 一个无后端的评论框工具, 其依赖于 Leancloud 开发, 所以使用前需要先注册 Leancloud 账号 如何开始? 你可以从 Valine-快速开始 教程开始, 教程包含了一步一步的指引教程. Valine配置项 主题valine评论框提供了以下配置项 1234567891011121314valine: # Valine官方地址: https://valine.js.org appid: # 你的 leancloud 应用 appid appkey: # 你的 leancloud 应用 appkey notify: true # 是否开始评论邮件提醒, 教程: https://github.com/xCss/Valine/wiki verify: false # 是否开始验证码功能, 开始邮件提醒会自动开启验证码功能 placeholder: 说点什么... # 输入框默认内容 avatar: mm # 头像展示方式, 具体设置项教程: https://valine.js.org/configuration.html#avatar meta: nick,mail,link # 自定义评论信息 pageSize: 10 # 评论列表分页 lang: zh-cn, # 多语言支持 zh-cn | en visitor: true # 文章阅读量统计: https://valine.js.org/visitor.html highlight: true # 代码高亮 recordIP: true # 记录评论者的IP 关于邮件提醒: 只有在回复评论时, 并且填写了邮箱的评论才会收到回复提醒 关于文章阅读量统计: 开启阅读量统计, 会在详情页标题下展示阅读量数据 搜索功能 主题提供内置的搜索功能和百度搜索, 百度搜索就是使用百度的SEO搜索, 个人觉得不是很实用, 不建议开启. 在主题配置文件 _config.yml 中配置: 12345# Searchsearch: insight: true # 在使用搜索功能前, 你需要安装 `hexo-generator-json-content` baidu: false # 使用百度搜索前, 你必须禁用其他所有的搜索功能 内置搜索 使用搜索功能前需要先安装: 12npm i -S hexo-generator-json-content 项目地址: https://github.com/alexbruno/hexo-generator-json-content 在你运行 hexo g 或者 hexo s 时生效，在 hexo g 生成站点时, 会在根目录下生成 content.json 该文件内容即为搜索内容。 你可以对搜索内容进行自定义的配置， 只要在 _config.yml 中配置 jsonContent即可: 1234567891011121314151617181920# 示例: 隐藏分类和标签的搜索jsonContent: dateFormat: DD/MM/YYYY posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: false author: false 文章阅读数量统计 主题提供 不蒜子 和 基于 leancloud 的统计 但是经过验证, 发现基于leancloud的统计不生效, 不知原因, 实现等效的方法就是: 评论框使用valine评论框(主题已经内置), 同时开启 visitor: true 配置项项即可 字数统计&amp;阅读时长 主题内置了该功能, 使用前需要先安装插件: 12npm i -S hexo-wordcount 主题配置文件中, 开启设置即可: 123456# wordcountpostCount: enable: true wordcount: true # 文章字数统计 min2read: true # 阅读时长预计 友情链接 复制 theme/pure/_source/ 目录下 links文件夹 到 blog path/source/ 目录下 在 hexo 目录下的 source 文件夹内创建一个名为 _data（禁止改名）的文件夹。 然后在文件内创建一个名为 links.yml 的文件,在其中添加相关数据即可。 单个友情链接的格式为： 12345Name: link: http://example.com avatar: http://example.com/avatar.png desc: \"这是一个描述\" 添加多个友情链接，我们只需要根据上面的格式重复填写即可。 将 Name 改为友情链接的名字，例如 Cofess。 http://example.com 为友情链接的地址。 http://example.com/avatar.png 为友情链接的头像。 这是一个描述 为友情链接描述。","categories":[{"name":"应用部署","slug":"应用部署","permalink":"http://blog.tgyf.com/categories/%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://blog.tgyf.com/tags/%E5%8D%9A%E5%AE%A2/"}]},{"title":"搭建个人博客hexo详细步骤","slug":"Blog/搭建个人博客hexo详细步骤","date":"2020-02-23T14:44:47.683Z","updated":"2020-02-23T14:44:47.683Z","comments":true,"path":"2020/02/23/Blog/搭建个人博客hexo详细步骤/","link":"","permalink":"http://blog.tgyf.com/2020/02/23/Blog/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2hexo%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start 第一步：下载安装Git与node.js Git下载地址 node.js下载地址 第二步：安装hexo 1$ npm install -g hexo 这里如果地址被“墙”，可以参考这篇文章的“安装Hexo”部分 第三步：初始化Hexo 创建文件夹（我的是在F盘创建的Hexo） 在Hexo文件夹下，右键运行Git Bash，输入命令：hexo init 在_config.yml,进行基础配置 第四步：本地部署博客 分别输入 如下命令： 12hexo ghexo s 更多hexo常用命令","categories":[{"name":"应用部署","slug":"应用部署","permalink":"http://blog.tgyf.com/categories/%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://blog.tgyf.com/tags/%E5%8D%9A%E5%AE%A2/"}]}]}